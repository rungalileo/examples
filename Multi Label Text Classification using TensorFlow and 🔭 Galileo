{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1IQ99tSgicpVpCrkhjTp-Dp_f2ucBhvy3","timestamp":1659469611341}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Multi Label Text Classification using TensorFlow and üî≠ Galileo\n","\n","In this tutorial, we'll train a model with Tensorflow and explore the results in Galileo.\n","\n","**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"],"metadata":{"id":"j5JVjGAAJOTn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfMcPbg19uz1","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb1053e5-0a09-45de-f8bc-83251debfd05"},"outputs":[{"output_type":"stream","name":"stdout","text":["üëã Installed necessary libraries and restarting runtime! This should only need to happen once.\n","üôè Continue with the rest of the notebook or hit \"Run All\" again!\n"]}],"source":["#@title Install `dataquality`\n","try:\n","    import dataquality as dq\n","except ImportError:\n","    # Upgrade pip\n","    !pip install -U pip &> /dev/null\n","\n","    # Install HF datasets for downloading the example datasets\n","    !pip install -U dataquality==0.5.3a0 datasets transformers &> /dev/null\n","    \n","    print('üëã Installed necessary libraries and restarting runtime! This should only need to happen once.')\n","    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n","\n","    # Restart the runtime\n","    import os, time\n","    time.sleep(1) # gives the print statements time to flush\n","    os._exit(0) # exits without allowing the next cell to run"]},{"cell_type":"markdown","source":["# 1. Login to Galileo"],"metadata":{"id":"9QyHXYMZJw3H"}},{"cell_type":"code","source":["import dataquality as dq\n","\n","dq.login()"],"metadata":{"id":"pO6DdYFob5UV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663274442017,"user_tz":240,"elapsed":16435,"user":{"displayName":"Bogdan Gheorghe","userId":"05755231234703527040"}},"outputId":"2120e107-c75f-4a60-bd6e-631e6b5a1e34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Welcome to Galileo Cloud v0.5.3a0!\n","üì° https://console.cloud.rungalileo.io\n","üî≠ Logging you into Galileo\n","\n","Go to https://console.cloud.rungalileo.io to generate a new API Key\n","üîê Enter your API Key:eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJib2dkYW5AcnVuZ2FsaWxlby5pbyIsImV4cCI6MTY2MzQzNjY0NH0.1Gpz1LJ1NaSkPv5MF0aZry-v2oVzPx-oy_32M0jHnzM\n","üöÄ You're logged in to Galileo as bogdan@rungalileo.io!\n"]}]},{"cell_type":"markdown","source":["# 2. Load Data"],"metadata":{"id":"dOKPfMhYvgeL"}},{"cell_type":"code","source":["#@title ü§ó HuggingFace Dataset\n","#@markdown You can find more datasets [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:text-classification&task_ids=task_ids:multi-label-classification&sort=downloads).\n","\n","dataset_name = 'go_emotions' #@param [\"go_emotions\", \"lex_glue\"] {allow-input: true}\n","print(f\"You selected the {dataset_name} dataset\")\n","\n","from IPython.utils import io\n","from datasets import load_dataset, get_dataset_config_names\n","\n","# Try to load the data. If a config (subset) is needed, pick one\n","try:\n","  with io.capture_output() as captured:\n","    data = load_dataset(dataset_name)\n","except ValueError as e:\n","  if \"Config name is missing\" not in repr(e):\n","    raise e\n","\n","  configs = get_dataset_config_names(dataset_name)\n","  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n","  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n","  if config:\n","    assert config in configs, f\"{config} is not a valid subset\"\n","  else:\n","    config = configs[0]\n","  with io.capture_output() as captured:\n","    data = load_dataset(dataset_name, name=config)\n","\n","# Check that the dataset has at least train and either of validation/test\n","assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n","f\"üíæ The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n","\n","print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")"],"metadata":{"id":"Wru0QtcoQmY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Convert HF dataset to Pandas dataframes \n","import pandas as pd\n","import numpy as np\n","\n","def load_pandas_df(data):\n","  # Find the name of the ground truth column\n","  good_col_names = [name for name in list(data['train'].features) if \"label\" in name]\n","  if len(good_col_names) == 1:\n","    label_col = good_col_names[0]\n","  else:\n","    col_names = list(data['train'].features)\n","    print(f\"The name of the columns are {col_names}.\")\n","    label_col = input(f\"üèÖ Please enter the name of the column containing the labels: \")\n","    assert label_col in col_names, f\"{label_col} is not an existing column\"\n","\n","  # Load the labels in a dictionary\n","  num_classes = len(data['train'].features[label_col].feature.names)\n","  labels_cols = data['train'].features[label_col].feature.int2str(range(0, num_classes))\n","\n","  def binarize_label_indices(label_idxs):\n","    a = np.zeros(len(labels_cols), dtype=int)\n","    a[label_idxs] = 1\n","    return a\n","\n","  # Load the train data into a frame\n","  train_data = data[\"train\"]\n","  train_df = pd.DataFrame.from_dict(train_data)\n","  train_labels = list(map(binarize_label_indices, data['train'][label_col]))\n","  _train_df_labels = pd.DataFrame(train_labels, columns=labels_cols)\n","  train_df = pd.concat([train_df, _train_df_labels], axis=1)\n","  train_df['id'] = train_df.index\n","\n","  # Load the test data into a frame\n","  test_split_name = \"validation\" if \"validation\" in data else \"test\"\n","  test_data = data[test_split_name]\n","  test_df = pd.DataFrame.from_dict(test_data)\n","  test_labels = list(map(binarize_label_indices, data[test_split_name][label_col]))\n","  _test_df_labels = pd.DataFrame(test_labels, columns=labels_cols)\n","  test_df = pd.concat([test_df, _test_df_labels], axis=1)\n","  test_df['id'] = test_df.index\n","  \n","  return train_df, test_df, labels_cols\n","\n","train_df, test_df, labels_cols = load_pandas_df(data)\n"],"metadata":{"id":"u3yKR4e9d3xa","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Initialize Galileo"],"metadata":{"id":"cicBhcwoQvjl"}},{"cell_type":"code","source":["# üî≠üåï Galileo logging\n","dq.init(task_type=\"text_multi_label\", \n","        project_name=\"multi_label_text_classification_tensorflow\", \n","        run_name=f\"example_run_{dataset_name.replace('/', '-')}\")"],"metadata":{"id":"JyDJNcDRQis6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Log input data with Galileo\n","Input data can be logged via `log_data_samples` (or `log_dataset` for logging iterables). This step will log input samples, gold labels, data split, and list of all labels. You can achieve this by adding 1 line of code to the standard PyTorch Dataset Class."],"metadata":{"id":"6zJHmvxTJ2pN"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","import tensorflow as tf\n","\n","# üî≠üåï Galileo logging\n","dq.set_tasks_for_run(labels_cols, binary=True)\n","dq.log_data_samples(texts=train_df[\"text\"], \n","                    task_labels=train_df[labels_cols].apply(lambda row: list(row[row == 1].index.values), axis=1),\n","                    ids=train_df[\"id\"],\n","                    split=\"training\")\n","dq.log_data_samples(texts=test_df[\"text\"], \n","                    task_labels=test_df[labels_cols].apply(lambda row: list(row[row == 1].index.values), axis=1),\n","                    ids=test_df[\"id\"],\n","                    split=\"test\")\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","BATCH_SIZE = 32\n","# Train and test datasets\n","datasets = []\n","for df in [train_df, test_df]:\n","  inputs = tokenizer(df.text.to_list(), truncation=True, padding=True)\n","  inputs[\"label\"] = df[labels_cols]\n","  inputs[\"id\"] = df[\"id\"]\n","  dataset = tf.data.Dataset.from_tensor_slices(dict(inputs)).batch(BATCH_SIZE)\n","  datasets.append(dataset)\n","\n","train_ds, test_ds = datasets"],"metadata":{"id":"FX8KsPk_m6Au"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Log model data with Galileo\n","\n","Model data can be logged via `log_model_outputs`. This step will log the model logits and embeddings. You can achieve this by adding 1 line of code to the standard pytorch model. \n","\n","We log [CLS]-token embedding from final layer, but you can log any custom layer for embeddings. "],"metadata":{"id":"nMV9XnJaSg9j"}},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import TFAutoModel\n","\n","class TextMultiLabelClassificationModel(tf.keras.Model):\n","  def __init__(self, num_tasks):\n","    super().__init__()\n","    self.feature_extractor = TFAutoModel.from_pretrained(\"distilbert-base-uncased\")\n","    self.classifier = tf.keras.layers.Dense(num_tasks)\n","\n","  def call(self, x, ids):\n","    input_ids, attention_mask = x\n","    \"\"\"Model forward function.\"\"\"\n","    encoded_layers = self.feature_extractor(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n","    classification_embedding = encoded_layers[:, 0] # Extract [CLS]-token\n","    emb = tf.nn.dropout(tf.nn.relu(classification_embedding), rate=0.1)\n","    logits = self.classifier(emb)\n","\n","    # üî≠üåï Galileo logging\n","    dq.log_model_outputs(\n","        embs=classification_embedding, logits=logits, ids=ids\n","    )\n","\n","    return logits"],"metadata":{"id":"7kvfEg4cnzTZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Putting into Action: Training a Model\n","\n","We complete the training pipeline by using a standard PyTorch training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."],"metadata":{"id":"PnEeJN1IyMl0"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","\n","NUM_EPOCHS = 2\n","\n","model = TextMultiLabelClassificationModel(len(labels_cols))\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","\n","# Sparse means label can be provided as class idx, not one hot\n","loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","# Iterate over epochs.\n","for epoch in range(NUM_EPOCHS):\n","  print(\"Start of epoch %d\" % (epoch,))\n","  dq.set_epoch(epoch) # üî≠üåï Galileo logging\n","\n","  # Iterate over the batches of the train dataset.\n","  dq.set_split(\"training\") # üî≠üåï Galileo logging\n","  for train_batch in tqdm(train_ds):\n","      y = train_batch[\"label\"]\n","      x = (train_batch[\"input_ids\"], train_batch[\"attention_mask\"])\n","\n","      with tf.GradientTape() as tape:\n","          logits = model(\n","              x,\n","              ids=train_batch[\"id\"],\n","          )\n","          loss = loss_fn(y, logits)\n","\n","      grads = tape.gradient(loss, model.trainable_weights)\n","      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","\n","  # Iterate over the batches of the test dataset.\n","  dq.set_split(\"test\") # üî≠üåï Galileo logging\n","  for test_batch in tqdm(test_ds):\n","      y = test_batch[\"label\"]\n","      x = (test_batch[\"input_ids\"], test_batch[\"attention_mask\"])\n","\n","      logits = model(x, ids=test_batch[\"id\"])\n","      loss = loss_fn(y, logits)\n","\n","dq.finish() # üî≠üåï Galileo logging"],"metadata":{"id":"-lRIh9U7oL2V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# General Help and Docs\n","- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n","- To see more general data and model logging docs, run `dq.docs()`"],"metadata":{"id":"QkUoYnK0oFK_"}},{"cell_type":"code","source":["dq.get_data_logger().doc()\n","help(dq.log_dataset)"],"metadata":{"id":"V9o7ZwZVZIiM","collapsed":true},"execution_count":null,"outputs":[]}]}