{"cells":[{"cell_type":"markdown","metadata":{"id":"j5JVjGAAJOTn"},"source":["# Named Entity Recognition with SpaCy and üî≠ Galileo\n","\n","In this tutorial, we'll train a model with SpaCy and explore the results in Galileo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfMcPbg19uz1","cellView":"form"},"outputs":[],"source":["#@title Install `dataquality`\n","try:\n","    import dataquality as dq\n","except ImportError:\n","    # Upgrade pip\n","    !pip install -U pip &> /dev/null\n","    # A higher version of spacy comes preinstalled on colab\n","    !pip uninstall -U en-core-web-sm &> /dev/null\n","    # Install HF datasets for downloading the example datasets\n","    !pip install -U dataquality==0.5.3a0 datasets \"spacy==3.2.1\" &> /dev/null\n","    \n","    print('üëã Installed necessary libraries and restarting runtime! This should only need to happen once.')\n","    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n","\n","    # Restart the runtime\n","    import os, time\n","    time.sleep(1) # gives the print statements time to flush\n","    os._exit(0) # exits without allowing the next cell to run"]},{"cell_type":"markdown","metadata":{"id":"9QyHXYMZJw3H"},"source":["# Login to Galileo"]},{"cell_type":"code","source":["import dataquality as dq\n","\n","dq.login()"],"metadata":{"id":"1rFe0jpme1fR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"atuhn2xgB_ct"},"source":["# SpaCy Data Preparation \n","We load NER datasets from HuggingFaceü§ó registry, which provide word indexed NER spans. The data is formatted to be compatible with SpaCy pipelines by converting the spans to be character indexed. "]},{"cell_type":"code","source":["#@title ü§ó HuggingFace Dataset\n","#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:token-classification&task_ids=task_ids:named-entity-recognition&sort=downloads).\n","\n","dataset_name = 'wikiann' #@param [\"wnut_17\", \"conllpp\", \"wikiann\"] {allow-input: true}\n","print(f\"You selected the {dataset_name} dataset\")\n","\n","from IPython.utils import io\n","from datasets import load_dataset, get_dataset_config_names\n","\n","# Try to load the data. If a config (subset) is needed, pick one\n","try:\n","  with io.capture_output() as captured:\n","    data = load_dataset(dataset_name)\n","except ValueError as e:\n","  if \"Config name is missing\" not in repr(e):\n","    raise e\n","\n","  configs = get_dataset_config_names(dataset_name)\n","  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n","  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n","  if config:\n","    assert config in configs, f\"{config} is not a valid subset\"\n","  else:\n","    config = configs[0]\n","  with io.capture_output() as captured:\n","    data = load_dataset(dataset_name, name=config)\n","\n","# Check that the dataset has at least train and either of validation/test\n","assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n","f\"üíæ The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n","\n","print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")"],"metadata":{"id":"xWBoiMrseLKY","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3yKR4e9d3xa","cellView":"form"},"outputs":[],"source":["#@title\n","#@markdown Converting HF-formatted NER datasets to the Spacy format\n","\n","import pandas as pd\n","\n","class NerDataset:\n","    \"\"\"\n","    Helper class to prepare the HF files for model input and output.\n","    \"\"\"\n","    # text and gold_spans \n","    def __init__(self, split, labels, label_col=\"ner_tags\"):\n","      self.idx2label = {k:v for (k,v) in enumerate(labels)}\n","      self.list_of_labels = labels\n","      self.ner_token_tags = data[split][label_col]\n","      self.tokens = data[split]['tokens']\n","      self.split = split\n","      self.text_inputs = [' '.join(_tokens) for _tokens in self.tokens]\n","      self.gold_spans = [self.extract_spans(sample_tokens, sample_ner_token_tags) for (sample_tokens, sample_ner_token_tags) in zip(self.tokens, self.ner_token_tags)]\n","\n","    def extract_spans(self, sample_tokens, sample_ner_token_tags):\n","      \"\"\"\n","      HF uses word indexed spans. \n","      Extract character indexed spans for SpaCy. Compatible with BIOES, BILOU, BIO schema\n","      \"\"\"\n","      gold_tokens_len = [0] # n position value tracks the character length of \n","      # a sentence uptill token n\n","      count = 0\n","      for _token in sample_tokens:\n","        count+=len(_token)+1\n","        gold_tokens_len.append(count)\n","      gold_sequence = [self.idx2label[ner_token_tag] for ner_token_tag in sample_ner_token_tags]\n","      \n","      gold_spans = []\n","      total_b_count = 0\n","      idx = 0\n","      while idx < len(gold_sequence):\n","          ner_label = gold_sequence[idx]\n","          next_idx = idx + 1\n","          if ner_label not in self.list_of_labels:\n","              raise Exception\n","\n","          if ner_label.startswith(\"U\") or ner_label.startswith(\"S\"):\n","              ner_tag, ner_class = ner_label.split(\"-\", 1)\n","              total_b_count += 1\n","              gold_spans.append(\n","                  {\n","                      \"start\": gold_tokens_len[idx],\n","                      \"end\": gold_tokens_len[idx + 1],\n","                      \"label\": ner_class,\n","                  }\n","              )\n","              idx += 1\n","              continue\n","\n","          if not ner_label.startswith(\"B\"):\n","              idx += 1\n","              continue\n","\n","          total_b_count += 1\n","          ner_tag, ner_class = ner_label.split(\"-\", 1)\n","          for next_tok in gold_sequence[idx + 1 :]:\n","              if next_tok not in self.list_of_labels:\n","                  raise Exception\n","              if next_tok.startswith(\"I\") and next_tok.split(\"-\", 1)[1] == ner_class:\n","                  next_idx += 1\n","              elif (next_tok.startswith(\"L\") and next_tok.split(\"-\", 1)[1] == ner_class) or (next_tok.startswith(\"E\") and next_tok.split(\"-\", 1)[1] == ner_class):\n","                  next_idx += 1\n","                  break\n","              else:\n","                  break\n","          gold_spans.append(\n","              {\n","                  \"start\": gold_tokens_len[idx],\n","                  \"end\": gold_tokens_len[next_idx] - 1,\n","                  \"label\": ner_class,\n","              }\n","          )\n","          idx = next_idx\n","\n","      assert total_b_count == len(gold_spans)\n","      return gold_spans\n","\n","# Find the name of the ground truth column\n","good_col_names = [name for name in list(data['train'].features) if \"tags\" in name]\n","if len(good_col_names) == 1:\n","  label_col = good_col_names[0]\n","elif \"ner_tags\" in good_col_names:\n","  label_col = \"ner_tags\"\n","else:\n","  col_names = list(data['train'].features)\n","  print(f\"The name of the columns are {col_names}.\")\n","  label_col = input(f\"üèÖ Please enter the name of the column containing the ner tags: \")\n","  assert label_col in col_names, f\"{label_col} is not an existing column\"\n","\n","labels = data[\"train\"].features[label_col].feature.names\n","train_data = NerDataset(split=\"train\", labels=labels)\n","test_split_name = \"validation\" if \"validation\" in data else \"test\"\n","test_data = NerDataset(split=test_split_name, labels=labels, label_col=label_col)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FX8KsPk_m6Au","cellView":"form"},"outputs":[],"source":["#@markdown Convert datasets to Spacy examples\n","import spacy\n","from typing import List\n","from spacy.training import Example\n","\n","def generate_examples(texts, samples_annotations, nlp):\n","  examples = []\n","  for text, annotations in zip(texts, samples_annotations):\n","    # spacy requires annotations in this format\n","    annotations = {\n","        \"entities\": [\n","            (annotation[\"start\"], annotation[\"end\"], annotation[\"label\"])\n","            for annotation in annotations\n","        ]\n","    }\n","\n","    # Generating the docs/examples\n","    doc = nlp.make_doc(text)\n","    examples.append(Example.from_dict(doc, annotations))\n","  return examples\n","\n","# Create Spacy NER model\n","nlp = spacy.blank(\"en\")\n","nlp.add_pipe(\"ner\", last=True)\n","\n","# Create train and test examples\n","train_examples = generate_examples(train_data.text_inputs, train_data.gold_spans, nlp)\n","test_examples = generate_examples(test_data.text_inputs, test_data.gold_spans, nlp)"]},{"cell_type":"markdown","metadata":{"id":"nMV9XnJaSg9j"},"source":["# Training with Galileo\n","Input samples are logged to Galileo using `log_input_examples`. Model data is logged by wrapping the `nlp` object using `watch`. This automatically logs the logits and embeddings from your model to Galileo with just 1 line of code. \n","\n","We complete the training pipeline by using a standard SpaCy training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kvfEg4cnzTZ"},"outputs":[],"source":["import random\n","import spacy\n","from spacy.util import minibatch\n","from dataquality.integrations.spacy import log_input_examples, watch\n","\n","num_epochs = 5\n","batch_size = 64\n","\n","# üî≠üåï Galileo logging\n","dq.init(task_type=\"text_ner\", \n","        project_name=\"named_entity_recognition_spacy\", \n","        run_name=f\"example_run_{dataset_name.replace('/', '-')}\"))\n","\n","optimizer = nlp.initialize(lambda: train_examples+test_examples)\n","\n","watch(nlp) # üî≠üåï Galileo logging\n","log_input_examples(train_examples, \"training\") # üî≠üåï Galileo logging\n","log_input_examples(test_examples, \"test\") # üî≠üåï Galileo logging\n","\n","for itn in range(num_epochs):\n","    dq.set_epoch(itn) # üî≠üåï Galileo logging\n","    print(f\"Starting Epoch {itn}\")\n","\n","    dq.set_split(\"training\") # üî≠üåï Galileo logging    \n","    random.shuffle(train_examples)\n","    batches = minibatch(train_examples, batch_size)\n","    losses = {}\n","    for batch in batches:\n","        nlp.update(batch, drop=0.5, sgd=optimizer, losses=losses)\n","\n","    dq.set_split(\"test\") # üî≠üåï Galileo logging\n","    scores = nlp.evaluate(test_examples)\n","    print(f\"Score is {scores} for epoch: {itn}\")\n","\n","dq.finish()"]},{"cell_type":"markdown","metadata":{"id":"QkUoYnK0oFK_"},"source":["# General Help and Docs\n","- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n","- To see more general data and model logging docs, run `dq.docs()`"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"V9o7ZwZVZIiM"},"outputs":[],"source":["dq.get_data_logger().doc()\n","help(dq.log_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1IQ99tSgicpVpCrkhjTp-Dp_f2ucBhvy3","timestamp":1659469611341}],"private_outputs":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}