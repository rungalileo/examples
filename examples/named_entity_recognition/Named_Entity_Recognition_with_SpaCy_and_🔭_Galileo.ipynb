{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j5JVjGAAJOTn"
      },
      "source": [
        "# Named Entity Recognition with SpaCy and ðŸ”­ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with SpaCy and explore the results in Galileo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UfMcPbg19uz1"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality`\n",
        "\n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip\n",
        "    !pip install -U pip &> /dev/null\n",
        "    # A higher version of spacy comes preinstalled on colab\n",
        "    !pip uninstall -U en-core-web-sm &> /dev/null\n",
        "    # Install HF datasets for downloading the example datasets\n",
        "    !pip install -U \"dataquality==0.8.55.2\" datasets \"spacy==3.2.1\" &> /dev/null\n",
        "    \n",
        "    print('ðŸ‘‹ Installed necessary libraries.')\n",
        "    print('ðŸ™ Continue with the rest of the notebook or hit \"Run All\" again!')\n",
        "\n",
        "    # Restart the runtime\n",
        "    import os, time\n",
        "    time.sleep(1) # gives the print statements time to flush\n",
        "    os._exit(0) # exits without allowing the next cell to run"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "atuhn2xgB_ct"
      },
      "source": [
        "# SpaCy Data Preparation \n",
        "We load NER datasets from HuggingFaceðŸ¤— registry, which provide word indexed NER spans. The data is formatted to be compatible with SpaCy pipelines by converting the spans to be character indexed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xWBoiMrseLKY"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ¤— HuggingFace Dataset\n",
        "#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:token-classification&task_ids=task_ids:named-entity-recognition&sort=downloads).\n",
        "\n",
        "dataset_name = 'conllpp' #@param [\"wnut_17\", \"conllpp\", \"wikiann\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, trust_remote_code=True)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"ðŸ–– Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config, trust_remote_code=True)\n",
        "\n",
        "# Check that the dataset has at least train and either of validation/test\n",
        "assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n",
        "f\"ðŸ’¾ The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n",
        "\n",
        "print(f\"\\nðŸ† Dataset {dataset_name} loaded succesfully\")\n",
        "\n",
        "# A small function for minimizing the dataset for testing\n",
        "import os\n",
        "\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "u3yKR4e9d3xa"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#@markdown Converting HF-formatted NER datasets to the Spacy format\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class NerDataset:\n",
        "    \"\"\"\n",
        "    Helper class to prepare the HF files for model input and output.\n",
        "    \"\"\"\n",
        "    # text and gold_spans \n",
        "    def __init__(self, split, labels, label_col=\"ner_tags\"):\n",
        "      self.idx2label = {k:v for (k,v) in enumerate(labels)}\n",
        "      self.list_of_labels = labels\n",
        "      self.ner_token_tags = data[split][label_col]\n",
        "      self.tokens = data[split]['tokens']\n",
        "      self.split = split\n",
        "      self.text_inputs = [' '.join(_tokens) for _tokens in self.tokens]\n",
        "      self.gold_spans = [self.extract_spans(sample_tokens, sample_ner_token_tags) for (sample_tokens, sample_ner_token_tags) in zip(self.tokens, self.ner_token_tags)]\n",
        "\n",
        "    def extract_spans(self, sample_tokens, sample_ner_token_tags):\n",
        "      \"\"\"\n",
        "      HF uses word indexed spans. \n",
        "      Extract character indexed spans for SpaCy. Compatible with BIOES, BILOU, BIO schema\n",
        "      \"\"\"\n",
        "      gold_tokens_len = [0] # n position value tracks the character length of \n",
        "      # a sentence uptill token n\n",
        "      count = 0\n",
        "      for _token in sample_tokens:\n",
        "        count+=len(_token)+1\n",
        "        gold_tokens_len.append(count)\n",
        "      gold_sequence = [self.idx2label[ner_token_tag] for ner_token_tag in sample_ner_token_tags]\n",
        "      \n",
        "      gold_spans = []\n",
        "      total_b_count = 0\n",
        "      idx = 0\n",
        "      while idx < len(gold_sequence):\n",
        "          ner_label = gold_sequence[idx]\n",
        "          next_idx = idx + 1\n",
        "          if ner_label not in self.list_of_labels:\n",
        "              raise Exception\n",
        "\n",
        "          if ner_label.startswith(\"U\") or ner_label.startswith(\"S\"):\n",
        "              ner_tag, ner_class = ner_label.split(\"-\", 1)\n",
        "              total_b_count += 1\n",
        "              gold_spans.append(\n",
        "                  {\n",
        "                      \"start\": gold_tokens_len[idx],\n",
        "                      \"end\": gold_tokens_len[idx + 1],\n",
        "                      \"label\": ner_class,\n",
        "                  }\n",
        "              )\n",
        "              idx += 1\n",
        "              continue\n",
        "\n",
        "          if not ner_label.startswith(\"B\"):\n",
        "              idx += 1\n",
        "              continue\n",
        "\n",
        "          total_b_count += 1\n",
        "          ner_tag, ner_class = ner_label.split(\"-\", 1)\n",
        "          for next_tok in gold_sequence[idx + 1 :]:\n",
        "              if next_tok not in self.list_of_labels:\n",
        "                  raise Exception\n",
        "              if next_tok.startswith(\"I\") and next_tok.split(\"-\", 1)[1] == ner_class:\n",
        "                  next_idx += 1\n",
        "              elif (next_tok.startswith(\"L\") and next_tok.split(\"-\", 1)[1] == ner_class) or (next_tok.startswith(\"E\") and next_tok.split(\"-\", 1)[1] == ner_class):\n",
        "                  next_idx += 1\n",
        "                  break\n",
        "              else:\n",
        "                  break\n",
        "          gold_spans.append(\n",
        "              {\n",
        "                  \"start\": gold_tokens_len[idx],\n",
        "                  \"end\": gold_tokens_len[next_idx] - 1,\n",
        "                  \"label\": ner_class,\n",
        "              }\n",
        "          )\n",
        "          idx = next_idx\n",
        "\n",
        "      assert total_b_count == len(gold_spans)\n",
        "      return gold_spans\n",
        "\n",
        "# Find the name of the ground truth column\n",
        "good_col_names = [name for name in list(data['train'].features) if \"tags\" in name]\n",
        "if len(good_col_names) == 1:\n",
        "  label_col = good_col_names[0]\n",
        "elif \"ner_tags\" in good_col_names:\n",
        "  label_col = \"ner_tags\"\n",
        "else:\n",
        "  col_names = list(data['train'].features)\n",
        "  print(f\"The name of the columns are {col_names}.\")\n",
        "  label_col = input(f\"ðŸ… Please enter the name of the column containing the ner tags: \")\n",
        "  assert label_col in col_names, f\"{label_col} is not an existing column\"\n",
        "\n",
        "labels = data[\"train\"].features[label_col].feature.names\n",
        "train_data = NerDataset(split=\"train\", labels=labels)\n",
        "test_split_name = \"validation\" if \"validation\" in data else \"test\"\n",
        "test_data = NerDataset(split=test_split_name, labels=labels, label_col=label_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FX8KsPk_m6Au"
      },
      "outputs": [],
      "source": [
        "#@markdown Convert datasets to Spacy examples\n",
        "import spacy\n",
        "from typing import List\n",
        "from spacy.training import Example\n",
        "\n",
        "def generate_examples(texts, samples_annotations, nlp):\n",
        "  examples = []\n",
        "  for text, annotations in zip(texts, samples_annotations):\n",
        "    # spacy requires annotations in this format\n",
        "    annotations = {\n",
        "        \"entities\": [\n",
        "            (annotation[\"start\"], annotation[\"end\"], annotation[\"label\"])\n",
        "            for annotation in annotations\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Generating the docs/examples\n",
        "    doc = nlp.make_doc(text)\n",
        "    examples.append(Example.from_dict(doc, annotations))\n",
        "  return examples\n",
        "\n",
        "# Create Spacy NER model\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"ner\", last=True)\n",
        "\n",
        "if _minimize_for_ci():\n",
        "  train_data.text_inputs, train_data.gold_spans = train_data.text_inputs[:1000], train_data.gold_spans[:1000]\n",
        "  test_data.text_inputs, test_data.gold_spans = test_data.text_inputs[:1000], test_data.gold_spans[:1000]\n",
        "\n",
        "# Create train and test examples\n",
        "train_examples = generate_examples(train_data.text_inputs, train_data.gold_spans, nlp)\n",
        "test_examples = generate_examples(test_data.text_inputs, test_data.gold_spans, nlp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV9XnJaSg9j"
      },
      "source": [
        "# Training with Galileo\n",
        "Input samples are logged to Galileo using `log_input_examples`. Model data is logged by wrapping the `nlp` object using `watch`. This automatically logs the logits and embeddings from your model to Galileo with just 1 line of code. \n",
        "\n",
        "We complete the training pipeline by using a standard SpaCy training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "import dataquality as dq\n",
        "from dataquality.integrations.spacy import log_input_examples, watch\n",
        "\n",
        "num_epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "# ðŸ”­ðŸŒ• Initializing a new run in Galileo. Each run is part of a project.\n",
        "dq.init(task_type=\"text_ner\", \n",
        "        project_name=\"named_entity_recognition_spacy\", \n",
        "        run_name=f\"example_run_{dataset_name.replace('/', '-')}\")\n",
        "\n",
        "optimizer = nlp.initialize(lambda: train_examples+test_examples)\n",
        "\n",
        "watch(nlp) # ðŸ”­ðŸŒ• One line of Galileo code to capture the model's predictions on the inputs\n",
        "log_input_examples(train_examples, \"training\") # ðŸ”­ðŸŒ• Logging the training examples with Galileo\n",
        "log_input_examples(test_examples, \"test\") # ðŸ”­ðŸŒ• Logging the test examples  with Galileo\n",
        "\n",
        "for itn in range(num_epochs):\n",
        "    dq.set_epoch(itn) # ðŸ”­ðŸŒ• Setting the epoch\n",
        "    print(f\"Starting Epoch {itn}\")\n",
        "\n",
        "    dq.set_split(\"training\") # ðŸ”­ðŸŒ• Setting split to training\n",
        "    random.shuffle(train_examples)\n",
        "    batches = minibatch(train_examples, batch_size)\n",
        "    losses = {}\n",
        "    for batch in batches:\n",
        "        nlp.update(batch, drop=0.5, sgd=optimizer, losses=losses)\n",
        "\n",
        "    dq.set_split(\"test\") # ðŸ”­ðŸŒ• Setting split to test\n",
        "    scores = nlp.evaluate(test_examples)\n",
        "    print(f\"Score is {scores} for epoch: {itn}\")\n",
        "\n",
        "# ðŸ”­ðŸŒ• Complete the Galileo workflow with a call to dq.finish()\n",
        "dq.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logging Inference Data\n",
        "\n",
        "To log inference data, save the model to disk and check out the NER Inference with Spacy and Galileo notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp.to_disk(\"my_model\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUoYnK0oFK_"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V9o7ZwZVZIiM"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1IQ99tSgicpVpCrkhjTp-Dp_f2ucBhvy3",
          "timestamp": 1659469611341
        }
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "a233f9af8164ad7126959a4556cea8249e0fb02fb25eacb38c78037d5824a315"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
