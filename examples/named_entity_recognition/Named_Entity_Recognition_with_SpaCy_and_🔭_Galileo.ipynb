{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5JVjGAAJOTn"
   },
   "source": [
    "# Named Entity Recognition with SpaCy and 🔭 Galileo\n",
    "\n",
    "In this tutorial, we'll train a model with SpaCy and explore the results in Galileo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "UfMcPbg19uz1"
   },
   "outputs": [],
   "source": [
    "#@title Install `dataquality`\n",
    "\n",
    "try:\n",
    "    import dataquality as dq\n",
    "except ImportError:\n",
    "    # Upgrade pip\n",
    "    !pip install -U pip &> /dev/null\n",
    "    # A higher version of spacy comes preinstalled on colab\n",
    "    !pip uninstall -U en-core-web-sm &> /dev/null\n",
    "    # Install HF datasets for downloading the example datasets\n",
    "    !pip install -U dataquality datasets \"spacy==3.2.1\" &> /dev/null\n",
    "    \n",
    "    print('👋 Installed necessary libraries and restarting runtime! This should only need to happen once.')\n",
    "    print('🙏 Continue with the rest of the notebook or hit \"Run All\" again!')\n",
    "\n",
    "    # Restart the runtime\n",
    "    import os, time\n",
    "    time.sleep(1) # gives the print statements time to flush\n",
    "    os._exit(0) # exits without allowing the next cell to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QyHXYMZJw3H"
   },
   "source": [
    "# Login to Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rFe0jpme1fR"
   },
   "outputs": [],
   "source": [
    "import dataquality as dq\n",
    "\n",
    "dq.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atuhn2xgB_ct"
   },
   "source": [
    "# SpaCy Data Preparation \n",
    "We load NER datasets from HuggingFace🤗 registry, which provide word indexed NER spans. The data is formatted to be compatible with SpaCy pipelines by converting the spans to be character indexed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xWBoiMrseLKY"
   },
   "outputs": [],
   "source": [
    "#@title 🤗 HuggingFace Dataset\n",
    "#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:token-classification&task_ids=task_ids:named-entity-recognition&sort=downloads).\n",
    "\n",
    "dataset_name = 'conllpp' #@param [\"wnut_17\", \"conllpp\", \"wikiann\"] {allow-input: true}\n",
    "print(f\"You selected the {dataset_name} dataset\")\n",
    "\n",
    "from IPython.utils import io\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "\n",
    "# Try to load the data. If a config (subset) is needed, pick one\n",
    "try:\n",
    "  with io.capture_output() as captured:\n",
    "    data = load_dataset(dataset_name)\n",
    "except ValueError as e:\n",
    "  if \"Config name is missing\" not in repr(e):\n",
    "    raise e\n",
    "\n",
    "  configs = get_dataset_config_names(dataset_name)\n",
    "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
    "  config = input(f\"🖖 Enter the name of the subset to pick (or leave blank for any): \")\n",
    "  if config:\n",
    "    assert config in configs, f\"{config} is not a valid subset\"\n",
    "  else:\n",
    "    config = configs[0]\n",
    "  with io.capture_output() as captured:\n",
    "    data = load_dataset(dataset_name, name=config)\n",
    "\n",
    "# Check that the dataset has at least train and either of validation/test\n",
    "assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n",
    "f\"💾 The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n",
    "\n",
    "print(f\"\\n🏆 Dataset {dataset_name} loaded succesfully\")\n",
    "\n",
    "# A small function for minimizing the dataset for testing\n",
    "import os\n",
    "\n",
    "def _minimize_for_ci() -> bool:\n",
    "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "u3yKR4e9d3xa"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown Converting HF-formatted NER datasets to the Spacy format\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class NerDataset:\n",
    "    \"\"\"\n",
    "    Helper class to prepare the HF files for model input and output.\n",
    "    \"\"\"\n",
    "    # text and gold_spans \n",
    "    def __init__(self, split, labels, label_col=\"ner_tags\"):\n",
    "      self.idx2label = {k:v for (k,v) in enumerate(labels)}\n",
    "      self.list_of_labels = labels\n",
    "      self.ner_token_tags = data[split][label_col]\n",
    "      self.tokens = data[split]['tokens']\n",
    "      self.split = split\n",
    "      self.text_inputs = [' '.join(_tokens) for _tokens in self.tokens]\n",
    "      self.gold_spans = [self.extract_spans(sample_tokens, sample_ner_token_tags) for (sample_tokens, sample_ner_token_tags) in zip(self.tokens, self.ner_token_tags)]\n",
    "\n",
    "    def extract_spans(self, sample_tokens, sample_ner_token_tags):\n",
    "      \"\"\"\n",
    "      HF uses word indexed spans. \n",
    "      Extract character indexed spans for SpaCy. Compatible with BIOES, BILOU, BIO schema\n",
    "      \"\"\"\n",
    "      gold_tokens_len = [0] # n position value tracks the character length of \n",
    "      # a sentence uptill token n\n",
    "      count = 0\n",
    "      for _token in sample_tokens:\n",
    "        count+=len(_token)+1\n",
    "        gold_tokens_len.append(count)\n",
    "      gold_sequence = [self.idx2label[ner_token_tag] for ner_token_tag in sample_ner_token_tags]\n",
    "      \n",
    "      gold_spans = []\n",
    "      total_b_count = 0\n",
    "      idx = 0\n",
    "      while idx < len(gold_sequence):\n",
    "          ner_label = gold_sequence[idx]\n",
    "          next_idx = idx + 1\n",
    "          if ner_label not in self.list_of_labels:\n",
    "              raise Exception\n",
    "\n",
    "          if ner_label.startswith(\"U\") or ner_label.startswith(\"S\"):\n",
    "              ner_tag, ner_class = ner_label.split(\"-\", 1)\n",
    "              total_b_count += 1\n",
    "              gold_spans.append(\n",
    "                  {\n",
    "                      \"start\": gold_tokens_len[idx],\n",
    "                      \"end\": gold_tokens_len[idx + 1],\n",
    "                      \"label\": ner_class,\n",
    "                  }\n",
    "              )\n",
    "              idx += 1\n",
    "              continue\n",
    "\n",
    "          if not ner_label.startswith(\"B\"):\n",
    "              idx += 1\n",
    "              continue\n",
    "\n",
    "          total_b_count += 1\n",
    "          ner_tag, ner_class = ner_label.split(\"-\", 1)\n",
    "          for next_tok in gold_sequence[idx + 1 :]:\n",
    "              if next_tok not in self.list_of_labels:\n",
    "                  raise Exception\n",
    "              if next_tok.startswith(\"I\") and next_tok.split(\"-\", 1)[1] == ner_class:\n",
    "                  next_idx += 1\n",
    "              elif (next_tok.startswith(\"L\") and next_tok.split(\"-\", 1)[1] == ner_class) or (next_tok.startswith(\"E\") and next_tok.split(\"-\", 1)[1] == ner_class):\n",
    "                  next_idx += 1\n",
    "                  break\n",
    "              else:\n",
    "                  break\n",
    "          gold_spans.append(\n",
    "              {\n",
    "                  \"start\": gold_tokens_len[idx],\n",
    "                  \"end\": gold_tokens_len[next_idx] - 1,\n",
    "                  \"label\": ner_class,\n",
    "              }\n",
    "          )\n",
    "          idx = next_idx\n",
    "\n",
    "      assert total_b_count == len(gold_spans)\n",
    "      return gold_spans\n",
    "\n",
    "# Find the name of the ground truth column\n",
    "good_col_names = [name for name in list(data['train'].features) if \"tags\" in name]\n",
    "if len(good_col_names) == 1:\n",
    "  label_col = good_col_names[0]\n",
    "elif \"ner_tags\" in good_col_names:\n",
    "  label_col = \"ner_tags\"\n",
    "else:\n",
    "  col_names = list(data['train'].features)\n",
    "  print(f\"The name of the columns are {col_names}.\")\n",
    "  label_col = input(f\"🏅 Please enter the name of the column containing the ner tags: \")\n",
    "  assert label_col in col_names, f\"{label_col} is not an existing column\"\n",
    "\n",
    "labels = data[\"train\"].features[label_col].feature.names\n",
    "train_data = NerDataset(split=\"train\", labels=labels)\n",
    "test_split_name = \"validation\" if \"validation\" in data else \"test\"\n",
    "test_data = NerDataset(split=test_split_name, labels=labels, label_col=label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "FX8KsPk_m6Au"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 11:49:58.946454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_minimize_for_ci' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mblank(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_minimize_for_ci\u001b[49m():\n\u001b[1;32m     27\u001b[0m   train_data\u001b[38;5;241m.\u001b[39mtext_inputs, train_data\u001b[38;5;241m.\u001b[39mgold_spans \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mtext_inputs[:\u001b[38;5;241m1000\u001b[39m], train_data\u001b[38;5;241m.\u001b[39mgold_spans[:\u001b[38;5;241m1000\u001b[39m]\n\u001b[1;32m     28\u001b[0m   test_data\u001b[38;5;241m.\u001b[39mtext_inputs, test_data\u001b[38;5;241m.\u001b[39mgold_spans \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mtext_inputs[:\u001b[38;5;241m1000\u001b[39m], test_data\u001b[38;5;241m.\u001b[39mgold_spans[:\u001b[38;5;241m1000\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name '_minimize_for_ci' is not defined"
     ]
    }
   ],
   "source": [
    "#@markdown Convert datasets to Spacy examples\n",
    "import spacy\n",
    "from typing import List\n",
    "from spacy.training import Example\n",
    "\n",
    "def generate_examples(texts, samples_annotations, nlp):\n",
    "  examples = []\n",
    "  for text, annotations in zip(texts, samples_annotations):\n",
    "    # spacy requires annotations in this format\n",
    "    annotations = {\n",
    "        \"entities\": [\n",
    "            (annotation[\"start\"], annotation[\"end\"], annotation[\"label\"])\n",
    "            for annotation in annotations\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Generating the docs/examples\n",
    "    doc = nlp.make_doc(text)\n",
    "    examples.append(Example.from_dict(doc, annotations))\n",
    "  return examples\n",
    "\n",
    "# Create Spacy NER model\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "if _minimize_for_ci():\n",
    "  train_data.text_inputs, train_data.gold_spans = train_data.text_inputs[:1000], train_data.gold_spans[:1000]\n",
    "  test_data.text_inputs, test_data.gold_spans = test_data.text_inputs[:1000], test_data.gold_spans[:1000]\n",
    "\n",
    "# Create train and test examples\n",
    "train_examples = generate_examples(train_data.text_inputs, train_data.gold_spans, nlp)\n",
    "test_examples = generate_examples(test_data.text_inputs, test_data.gold_spans, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMV9XnJaSg9j"
   },
   "source": [
    "# Training with Galileo\n",
    "Input samples are logged to Galileo using `log_input_examples`. Model data is logged by wrapping the `nlp` object using `watch`. This automatically logs the logits and embeddings from your model to Galileo with just 1 line of code. \n",
    "\n",
    "We complete the training pipeline by using a standard SpaCy training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kvfEg4cnzTZ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "from dataquality.integrations.spacy import log_input_examples, watch\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# 🔭🌕 Initializing a new run in Galileo. Each run is part of a project.\n",
    "dq.init(task_type=\"text_ner\", \n",
    "        project_name=\"named_entity_recognition_spacy\", \n",
    "        run_name=f\"example_run_{dataset_name.replace('/', '-')}\")\n",
    "\n",
    "optimizer = nlp.initialize(lambda: train_examples+test_examples)\n",
    "\n",
    "watch(nlp) # 🔭🌕 One line of Galileo code to capture the model's predictions on the inputs\n",
    "log_input_examples(train_examples, \"training\") # 🔭🌕 Logging the training examples with Galileo\n",
    "log_input_examples(test_examples, \"test\") # 🔭🌕 Logging the test examples  with Galileo\n",
    "\n",
    "for itn in range(num_epochs):\n",
    "    dq.set_epoch(itn) # 🔭🌕 Setting the epoch\n",
    "    print(f\"Starting Epoch {itn}\")\n",
    "\n",
    "    dq.set_split(\"training\") # 🔭🌕 Setting split to training\n",
    "    random.shuffle(train_examples)\n",
    "    batches = minibatch(train_examples, batch_size)\n",
    "    losses = {}\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, drop=0.5, sgd=optimizer, losses=losses)\n",
    "\n",
    "    dq.set_split(\"test\") # 🔭🌕 Setting split to test\n",
    "    scores = nlp.evaluate(test_examples)\n",
    "    print(f\"Score is {scores} for epoch: {itn}\")\n",
    "\n",
    "# 🔭🌕 Complete the Galileo workflow with a call to dq.finish()\n",
    "dq.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkUoYnK0oFK_"
   },
   "source": [
    "# General Help and Docs\n",
    "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
    "- To see more general data and model logging docs, run `dq.docs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "V9o7ZwZVZIiM"
   },
   "outputs": [],
   "source": [
    "dq.get_data_logger().doc()\n",
    "help(dq.log_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1IQ99tSgicpVpCrkhjTp-Dp_f2ucBhvy3",
     "timestamp": 1659469611341
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d47fb0f7b925a064d1c5a8fa81282dc8f4377d8a7fa5a29bcda49ac963d4dd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
