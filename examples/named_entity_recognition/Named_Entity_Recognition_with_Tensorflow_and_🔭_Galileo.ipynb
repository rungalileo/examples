{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5JVjGAAJOTn"
      },
      "source": [
        "# Named Entity Recognition with TensorFlow and üî≠ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with Tensorflow and explore the results in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UfMcPbg19uz1"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality`\n",
        "\n",
        "# A small function for minimizing the dataset for testing\n",
        "import os\n",
        "\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\"\n",
        "\n",
        "    \n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip\n",
        "    !pip install -U pip &> /dev/null\n",
        "\n",
        "    # Install HF datasets for downloading the example datasets\n",
        "    !pip install -U dataquality datasets transformers &> /dev/null\n",
        "    \n",
        "    print('üëã Installed necessary libraries and restarting runtime! This should only need to happen once.')\n",
        "    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n",
        "\n",
        "    # Restart the runtime\n",
        "    import os, time\n",
        "    time.sleep(1) # gives the print statements time to flush\n",
        "    os._exit(0) # exits without allowing the next cell to run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxoFrrnXhwG_"
      },
      "source": [
        "# 1. Login to Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pO6DdYFob5UV"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "\n",
        "dq.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSR6g3ejucMH"
      },
      "source": [
        "# 2. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PHOtTBapzNOY"
      },
      "outputs": [],
      "source": [
        "#@title ü§ó HuggingFace Dataset\n",
        "#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:token-classification&task_ids=task_ids:named-entity-recognition&sort=downloads) which contains train/validation/test splits and an `ner_tags` column.\n",
        "\n",
        "dataset_name = 'conllpp' #@param [\"wnut_17\", \"conllpp\", \"wikiann\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config)\n",
        "\n",
        "# Check that the dataset has at least train and either of validation/test\n",
        "assert {\"train\", \"validation\", \"test\"}.issubset(data), \\\n",
        "f\"üíæ The dataset {dataset_name} has either no train, validation or test splits, select another one.\"\n",
        "\n",
        "print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifNndf5b7RPS"
      },
      "source": [
        "# 3. Initialize Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvxgKyp47oDR"
      },
      "outputs": [],
      "source": [
        "# üî≠üåï Initializing a new run in Galileo. Each run goes under a project.\n",
        "dq.init(task_type=\"text_ner\", \n",
        "        project_name=\"named_entity_recognition_tensorflow\", \n",
        "        run_name=f\"example_run_{dataset_name.replace('/', '-')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyMCfBzyvMzc"
      },
      "source": [
        "# 4. Tokenize and Log Dataset\n",
        "Galileo provides HF integrations to allow tokenization, and label alignment, while automatically logging your input data. Next, we prepare a dataset loader for each split.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3yKR4e9d3xa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from dataquality.integrations import hf\n",
        "from transformers import AutoTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "MINIBATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# üî≠üåï Galileo tokenizes the HuggingFace DatasetDict logs the dataset(s) present in it\n",
        "tokenized_datasets = hf.tokenize_and_log_dataset(data, tokenizer)\n",
        "\n",
        "# Train and test datasets\n",
        "tf_train_set = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"id\"],\n",
        "    shuffle=True,\n",
        "    batch_size=MINIBATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"id\"],\n",
        "    shuffle=False,\n",
        "    batch_size=MINIBATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = tokenized_datasets[\"test\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"id\"],\n",
        "    shuffle=False,\n",
        "    batch_size=MINIBATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kqOV9V5wl4J"
      },
      "source": [
        "# 5. Prepare and Log NER Model \n",
        "\n",
        "Model data can be logged via `log_model_outputs`. This step will log the model logits and embeddings. You can achieve this by adding 1 line of code to any NER model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEVmYug7wqCc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForTokenClassification, TFAutoModel\n",
        "\n",
        "class TokenClassificationModel(tf.keras.Model):\n",
        "  \"\"\"Example TF Model-subclass-style TokenClassificationModel model\n",
        "\n",
        "  Internally uses a huggingface TF model for the feature extractor\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_labels):\n",
        "      super().__init__()\n",
        "      self.feature_extractor = TFAutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "      self.classifier = tf.keras.layers.Dense(num_labels)\n",
        "\n",
        "  def call(self, x, ids):\n",
        "      input_ids, attention_ids = x\n",
        "\n",
        "      encoded_layers = self.feature_extractor(input_ids, attention_ids).last_hidden_state\n",
        "      logits = self.classifier(encoded_layers)\n",
        "\n",
        "      # üî≠üåï Log the model's logits and embeddings\n",
        "      dq.log_model_outputs(\n",
        "          embs=encoded_layers[:, 1:, :],\n",
        "          logits=logits[:, 1:, :],\n",
        "          ids=ids,\n",
        "      )\n",
        "\n",
        "      return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV9XnJaSg9j"
      },
      "source": [
        "# 6. Putting into Action: Training a Model\n",
        "\n",
        "Complete the training pipeline using a standard TensorFlow training setup. While training, log the current `epoch` and `split`.\n",
        "\n",
        "Call `dq.finish()` after training to complete logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from transformers.modeling_tf_utils import shape_list\n",
        "\n",
        "def compute_loss(labels, logits):\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "  )\n",
        "  # make sure only labels that are not equal to -100\n",
        "  # are taken into account as loss\n",
        "  active_loss = tf.reshape(labels, (-1,)) != -100\n",
        "  reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n",
        "  labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
        "  return loss_fn(labels, reduced_logits)\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "num_labels = len(data[list(data)[0]].features[\"ner_tags\"].feature.names)\n",
        "model = TokenClassificationModel(num_labels=num_labels)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "# Sparse means label can be provided as class idx, not one hot\n",
        "train_metric = tf.metrics.SparseCategoricalAccuracy()\n",
        "test_metric = tf.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Iterate over epochs.\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(\"Start of epoch %d\" % (epoch,))\n",
        "  dq.set_epoch(epoch) # üî≠üåï Galileo logging\n",
        "  train_metric.reset_states()\n",
        "  test_metric.reset_states()\n",
        "\n",
        "  # Iterate over the batches of the train dataset.\n",
        "  dq.set_split(\"training\") # üî≠üåï Galileo logging\n",
        "  for train_batch in tqdm(tf_train_set):\n",
        "      y = train_batch[\"labels\"]\n",
        "      x = (train_batch[\"input_ids\"], train_batch[\"attention_mask\"])\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          logits = model(x,ids=train_batch[\"id\"])\n",
        "          loss = compute_loss(y, logits)\n",
        "\n",
        "      train_metric.update_state(y, logits)\n",
        "      grads = tape.gradient(loss, model.trainable_weights)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "      if _minimize_for_ci():\n",
        "        break\n",
        "\n",
        "\n",
        "  # Iterate over the batches of the test dataset.\n",
        "  dq.set_split(\"test\") # üî≠üåï Galileo logging\n",
        "  for test_batch in tqdm(tf_test_set):\n",
        "      y = test_batch[\"labels\"]\n",
        "      x = (test_batch[\"input_ids\"], test_batch[\"attention_mask\"])\n",
        "\n",
        "      logits = model(x, ids=test_batch[\"id\"])\n",
        "      loss = compute_loss(y, logits)\n",
        "      test_metric.update_state(y, logits)\n",
        "      \n",
        "      if _minimize_for_ci():\n",
        "        break\n",
        "\n",
        "dq.finish() # üî≠üåï Galileo logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUoYnK0oFK_"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V9o7ZwZVZIiM"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1IQ99tSgicpVpCrkhjTp-Dp_f2ucBhvy3",
          "timestamp": 1659469611341
        }
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "0d47fb0f7b925a064d1c5a8fa81282dc8f4377d8a7fa5a29bcda49ac963d4dd1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
