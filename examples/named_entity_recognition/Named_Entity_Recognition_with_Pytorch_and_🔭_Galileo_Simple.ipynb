{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5JVjGAAJOTn"
      },
      "source": [
        "# Text Classification using PyTorch and üî≠ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UfMcPbg19uz1"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality` with `pip install dataquality`\n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip and install dependencies\n",
        "    !pip install -U pip &> /dev/null\n",
        "    !pip install dataquality transformers datasets torchmetrics==0.10.0 --upgrade 1> /dev/null\n",
        "\n",
        "    print('üëã Installed necessary libraries.')\n",
        "    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKPfMhYvgeL"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C7B55N_B30kG"
      },
      "outputs": [],
      "source": [
        "#@title ü§ó HuggingFace Dataset\n",
        "#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:token-classification&task_ids=task_ids:named-entity-recognition&sort=downloads) which contains train/test splits and an `ner_tags` column.\n",
        "\n",
        "dataset_name = 'conllpp' #@param [\"wnut_17\", \"conllpp\", \"wikiann\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config)\n",
        "\n",
        "# A small function for minimizing the dataset for testing\n",
        "import os\n",
        "\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\"\n",
        "\n",
        "if _minimize_for_ci():\n",
        "  data[\"train\"] = data[\"train\"].select(range(1000))\n",
        "  data[\"test\"] = data[\"test\"].select(range(1000))\n",
        "\n",
        "# Check that the dataset has at least train and test splits\n",
        "assert {\"train\", \"test\"}.issubset(data), \\\n",
        "f\"üíæ The dataset {dataset_name} does no have train/test splits, please pick another one.\"\n",
        "\n",
        "print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "nTrPjUO1FuEu"
      },
      "outputs": [],
      "source": [
        "#@title 2. Create model and prepare training\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AutoModel\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "\n",
        "class TokenClassificationModel(torch.nn.Module):\n",
        "  \"\"\"Defines a Pytorch text classification bert based model.\"\"\"\n",
        "\n",
        "  def __init__(self, num_labels: int):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    self.classifier = Linear(self.feature_extractor.config.hidden_size, num_labels)\n",
        "\n",
        "  def forward(self, x, attention_mask):\n",
        "    \"\"\"Model forward function.\"\"\"\n",
        "    encoded_layers = self.feature_extractor(\n",
        "        input_ids=x, attention_mask=attention_mask\n",
        "    ).last_hidden_state\n",
        "    logits = self.classifier(encoded_layers)\n",
        "    return logits\n",
        "\n",
        "num_classes = len(data['train'].features['ner_tags'].feature.names)\n",
        "\n",
        "model = TokenClassificationModel(num_classes)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5\n",
        ")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "val_acc = torchmetrics.Accuracy().to(device)\n",
        "train_acc = torchmetrics.Accuracy().to(device)\n",
        "loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def extract_batch(batch):\n",
        "  x = batch.pop(\"input_ids\").to(device)\n",
        "  y = batch.pop(\"labels\").to(device)\n",
        "  attention_mask = batch.pop(\"attention_mask\").to(device)\n",
        "  return x, y, attention_mask\n",
        "\n",
        "def train_loop(model,dataloader,optimizer,loss_fn):\n",
        "  model.train()\n",
        "  for batch in tqdm(dataloader):\n",
        "      # print statistics\n",
        "      x, y, attention_mask = extract_batch(batch)\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward + backward + optimize\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits.transpose(1, 2), y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "   \n",
        "def test_loop(model, dataloader, loss_fn, epoch):\n",
        "  model.eval()\n",
        "  test_loss, num_test_batches = 0.0, 0\n",
        "  pbar = tqdm(dataloader)\n",
        "  for batch in pbar:\n",
        "    x, y, attention_mask = extract_batch(batch)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits.transpose(1, 2), y)\n",
        "    test_loss += loss.item()\n",
        "    num_test_batches += 1\n",
        "    pbar.set_description(\"[epoch %d] Validation loss: %.3f\" % (epoch + 1, test_loss / num_test_batches))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6zJHmvxTJ2pN"
      },
      "source": [
        "## 3. Monitor with Galileo\n",
        "\n",
        "After simply logging the orignal dataset, we can hook the dataquality client in our model and dataloaders.\n",
        "\n",
        "```python\n",
        "import dataquality as dq\n",
        "from dataquality.integrations import hf\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "dq.init(...)\n",
        "dq.log_dataset(...)\n",
        "dq.set_labels_for_run(...)\n",
        "watch(model, [train_dataloader, test_dataloader])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX8KsPk_m6Au"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "from dataquality.integrations import hf\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "# üî≠üåï Initialize the project\n",
        "dq.init('text_ner',\n",
        "        \"named-entity-recognition-demo\",\n",
        "        f\"run_{dataset_name}\"\n",
        "        )\n",
        "\n",
        "\n",
        "# üî≠üåï Galileo tokenizes the HuggingFace DatasetDict logs the dataset(s) present in it\n",
        "tokenized_datasets = hf.tokenize_and_log_dataset(data, tokenizer)\n",
        "labels = tokenized_datasets['train'].features['ner_tags'].feature.names  \n",
        "\n",
        "train_dataloader = hf.get_dataloader(tokenized_datasets[\"train\"], collate_fn=data_collator, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = hf.get_dataloader(tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# üî≠üåï Logging the labels in order for Galileo\n",
        "dq.set_labels_for_run(labels)\n",
        "\n",
        "# üî≠üåï Monitor the model with Galileo\n",
        "watch(model, [train_dataloader, test_dataloader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV9XnJaSg9j"
      },
      "source": [
        "# 4. Training\n",
        "\n",
        "After hooking into the model we log epochs and split during the training process.\n",
        "```python\n",
        "dq.set_split(\"train\")\n",
        "dq.set_epoch(epoch)\n",
        "```\n",
        "\n",
        "When training is complete\n",
        "```dq.finish()```\n",
        "must be called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    dq.set_epoch(epoch) # üî≠üåï Setting the epoch\n",
        "    dq.set_split(\"training\") # üî≠üåï Setting split to training\n",
        "    train_loop(model,train_dataloader,optimizer,loss_fn)\n",
        "    dq.set_split(\"test\") # üî≠üåï Setting split to validation\n",
        "    test_loop(model, test_dataloader, loss_fn, epoch)\n",
        "print(\"Finished Training\")\n",
        "\n",
        "dq.finish() # üî≠üåï Finishing the run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUoYnK0oFK_"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9o7ZwZVZIiM"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mqfb_JFuEy"
      },
      "outputs": [],
      "source": [
        "# Get insights to the metrics\n",
        "dq.metrics.get_dataframe(\n",
        "        \"named-entity-recognition-demo\",\n",
        "        f\"run_{dataset_name}\",\n",
        "        \"train\").head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Oct 13 2022, 16:12:30) \n[Clang 12.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
