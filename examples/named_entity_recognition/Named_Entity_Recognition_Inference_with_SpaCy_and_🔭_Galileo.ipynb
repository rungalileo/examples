{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "j5JVjGAAJOTn"
   },
   "source": [
    "# Inference Named Entity Recognition with SpaCy and ðŸ”­ Galileo\n",
    "\n",
    "In this tutorial, we'll log an inference dataset on a pre-trained model with SpaCy and explore the results in Galileo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "UfMcPbg19uz1"
   },
   "outputs": [],
   "source": [
    "#@title Install `dataquality` and other dependencies\n",
    "\n",
    "# Upgrade pip\n",
    "!pip install -U pip &> /dev/null\n",
    "# Install HF datasets for downloading the example datasets\n",
    "!pip install -U \"dataquality==0.8.55.2\" datasets \"spacy==3.2.1\" &> /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9QyHXYMZJw3H"
   },
   "source": [
    "# Login to Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rFe0jpme1fR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dataquality as dq\n",
    "import spacy\n",
    "\n",
    "# Set enterprise env variables, you cannot use this in Galileo Cloud!\n",
    "os.environ[\"GALILEO_CONSOLE_URL\"] = os.getenv(\"GALILEO_CONSOLE_URL_ENTERPRISE\")\n",
    "os.environ[\"GALILEO_USERNAME\"] = os.getenv(\"GALILEO_USERNAME_ENTERPRISE\")\n",
    "os.environ[\"GALILEO_PASSWORD\"] = os.getenv(\"GALILEO_PASSWORD_ENTERPRISE\")\n",
    "\n",
    "dq.configure()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "atuhn2xgB_ct"
   },
   "source": [
    "# Inference DataSet Preparation \n",
    "We load text samples from HuggingFaceðŸ¤— dataset. This data can be thought of as production data that our model is making predictions on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xWBoiMrseLKY"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ¤— HuggingFace Dataset\n",
    "#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:token-classification&task_ids=task_ids:named-entity-recognition&sort=downloads).\n",
    "\n",
    "dataset_name = 'conllpp' #@param [\"wnut_17\", \"conllpp\", \"wikiann\"] {allow-input: true}\n",
    "print(f\"You selected the {dataset_name} dataset\")\n",
    "\n",
    "from IPython.utils import io\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "\n",
    "# Try to load the data. If a config (subset) is needed, pick one\n",
    "try:\n",
    "  with io.capture_output() as captured:\n",
    "    data = load_dataset(dataset_name, trust_remote_code=True)\n",
    "except ValueError as e:\n",
    "  if \"Config name is missing\" not in repr(e):\n",
    "    raise e\n",
    "\n",
    "  configs = get_dataset_config_names(dataset_name)\n",
    "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
    "  config = input(f\"ðŸ–– Enter the name of the subset to pick (or leave blank for any): \")\n",
    "  if config:\n",
    "    assert config in configs, f\"{config} is not a valid subset\"\n",
    "  else:\n",
    "    config = configs[0]\n",
    "  with io.capture_output() as captured:\n",
    "    data = load_dataset(dataset_name, name=config, trust_remote_code=True)\n",
    "\n",
    "# Check that the dataset has at least a validation/test set to use as inference data\n",
    "assert {\"validation\", \"test\"}.intersection(data), \\\n",
    "f\"ðŸ’¾ The dataset {dataset_name} has no validation or test splits, select another one.\"\n",
    "\n",
    "print(f\"\\nðŸ† Dataset {dataset_name} loaded succesfully\")\n",
    "\n",
    "# A small function for minimizing the dataset for testing\n",
    "import os\n",
    "\n",
    "def _minimize_for_ci() -> bool:\n",
    "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_docs(data, nlp) -> list:\n",
    "    # Get inference split name\n",
    "    inf_split_name = \"validation\" if \"validation\" in data else \"test\"\n",
    "    tokens = data[inf_split_name]['tokens']\n",
    "    text_inputs = [' '.join(_tokens) for _tokens in tokens]\n",
    "\n",
    "    if _minimize_for_ci():\n",
    "        text_inputs = text_inputs[:50]\n",
    "\n",
    "    # Get inference docs\n",
    "    inference_docs = [nlp.make_doc(text) for text in text_inputs]\n",
    "\n",
    "    return inference_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model\n",
    "\n",
    "Inference data can be logged as part of an existing run with previously logged training, validation, and test splits, or it can be logged in isolation.\n",
    "\n",
    "If you want to access the full suite of inference features, including automated drift detection, first run the [NER Spacy and Galileo training notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/named_entity_recognition/Named_Entity_Recognition_with_SpaCy_and_%F0%9F%94%AD_Galileo.ipynb#scrollTo=QkUoYnK0oFK_).\n",
    "\n",
    "Otherwise, we will download a small pretrained model to use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "model_type = \"pretrained_web\" #@param [\"pretrained_web\", \"my_model\"] {allow-input: false}\n",
    "\n",
    "if model_type == \"pretrained_web\":\n",
    "    # Download a small English model for running inference\n",
    "    !python -m spacy download en_core_web_sm &> /dev/null\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "elif model_type == \"my_model\":\n",
    "    # Load trained model from NER Spacy training notebook\n",
    "    nlp = spacy.load(\"my_model\")  # TODO: Update path to model\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type {model_type}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with Galileo\n",
    "Inference samples are logged to Galileo using `log_input_docs`. Model data is logged by wrapping the `nlp` object using `watch`. This automatically logs the logits and embeddings from your model to Galileo with just 1 line of code. \n",
    "\n",
    "We complete the inference pipeline by setting the split to `inference` and then passing inference Doc objects through our `nlp` model. To complete logging, we call `dq.finish()` after logging inference data.\n",
    "\n",
    "Each inference run must have a unique `inference_name` that must be consistent for input and model logs. You can log multiple inference datasets with different `inference_name`s on the same Project/Run pair.\n",
    "\n",
    "**Note:** If you want to connect this inference run to an existing training run, make sure to use the same `project_name` and `run_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataquality.integrations.spacy import log_input_docs, unwatch, watch\n",
    "\n",
    "# ðŸ”­ðŸŒ• Initializing a new run in Galileo. Each run is part of a project.\n",
    "dq.init(task_type=\"text_ner\", \n",
    "        project_name=\"named_entity_recognition_inference_spacy\", \n",
    "        run_name=f\"example_run_{dataset_name.replace('/', '-')}\")\n",
    "\n",
    "inference_docs = get_inference_docs(data, nlp)\n",
    "inference_name = \"example1\"\n",
    "meta = {\n",
    "    \"color\": random.choices([\"red\", \"blue\", \"green\"], k=len(inference_docs)),\n",
    "    \"ranking\": random.choices(range(1,101), k=len(inference_docs))\n",
    "}\n",
    "\n",
    "watch(nlp) # ðŸ”­ðŸŒ• One line of Galileo code to capture the model's predictions on the inputs\n",
    "log_input_docs(inference_docs, inference_name=inference_name, meta=meta) # ðŸ”­ðŸŒ• Logging the inference docs with Galileo\n",
    "\n",
    "dq.set_split(\"inference\", inference_name=inference_name)\n",
    "for doc in inference_docs:\n",
    "    nlp(doc)\n",
    "\n",
    "dq.finish()\n",
    "unwatch(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9o7ZwZVZIiM"
   },
   "outputs": [],
   "source": [
    "dq.get_data_logger().doc()\n",
    "help(dq.log_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1IQ99tSgicpVpCrkhjTp-Dp_f2ucBhvy3",
     "timestamp": 1659469611341
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
