{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e8a996",
   "metadata": {},
   "source": [
    "# Evaluating LLMs for summarization tasks with Galileo Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d63f",
   "metadata": {},
   "source": [
    "In this tutorial, we'll compare 5 different LLMs for summarization tasks with Galileo Evaluate.\n",
    "\n",
    "This notebook pulls data from huggingface for its datasource and uses Open AI, Mistral, Gemini and Together for LLMs. Feel Free to change these sources as you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd504ea-4d94-4868-ba40-a929e015de08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set-up of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f544bc-442f-4c3e-8fa2-c35907eb29bc",
   "metadata": {},
   "source": [
    "Let's start by installing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c13e1-c5a0-47be-8cf0-c460fa788a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install promptquality openai mistralai ipywidgets together google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc56b04-eca5-48d5-93cc-f145bd02dedc",
   "metadata": {},
   "source": [
    "## 2. Set-up Galileo Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f2bc4-500c-4c71-9e52-f5eb8e0183da",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we will setup Galileo Evaluate client. Here we also define the metrics we wish to evaluate the models on. For this lab we will be using 9 metrics. Feel free to change them as needed and play aroundYou will need to enter model API keys, note that all these keys are not compulsory, you can skip them if you are not using that model, just make sure to choose the model names accordingly later in the notebook - \n",
    " - OPENAI API KEY: Enter your Open AI API Key here\n",
    " - MISTRAL API KEY: Enter your Mistral API Key here\n",
    " - GOOGLE API KEY: Enter your Google API Key here\n",
    " - TOGETHER API KEY: Enter your Together API Key here\n",
    " - Project Name - Define a name for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb69761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import promptquality as pq\n",
    "\n",
    "GALILEO_URL = \"\" # Enter Galileo Console URL here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Enter Open AI Key here\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\" # Enter Mistral Key here\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\" # Enter Google Key here\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"\" # Enter Together Key here\n",
    "pq.login(GALILEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f4b885-0fd7-49d9-8676-424a0603a2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from promptquality import EvaluateRun\n",
    "\n",
    "PROJECT_NAME = \"cookbook_summarization_evaluation\"\n",
    "metrics = [pq.Scorers.completeness_luna, pq.Scorers.correctness, pq.Scorers.instruction_adherence_plus, pq.Scorers.sexist, pq.Scorers.tone, pq.Scorers.toxicity, pq.Scorers.prompt_injection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcfaa9-16f9-49c5-9b2b-60221bbf9bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Loading and Preparing Data\n",
    "\n",
    "For this lab we will use a fictuous use case where we want to build a bot to summarize patient notes.\n",
    "\n",
    "Now in order to build the bot, we will \n",
    " - Fetch some patient notes from a website, \n",
    " - Ask different LLMs to summarize the notes\n",
    " - Evaluate the LLMs' responses with the help of Galileo Evaluate\n",
    "\n",
    "In our case let's start by downloading some patient notes from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba938331-e2ed-4eb1-a907-9828d320807b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ncbi/Open-Patients\")\n",
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7730e",
   "metadata": {},
   "source": [
    "Lets have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791829cf-a1c6-449d-b98f-e9c66c9a331f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = int(df['description'].apply(len).mean())\n",
    "print(f'Average length of the documents is {avg_doc_length} characters.')\n",
    "std_doc_length = int(df['description'].apply(len).std())\n",
    "print(f'Standard deviation of the documents is {std_doc_length} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c5fb5",
   "metadata": {},
   "source": [
    "# 4. Create summarization bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15174c",
   "metadata": {},
   "source": [
    "Here we define the functions to generate responses from the different LLMs. We are using the `openai`, `mistralai`, `google.generativeai` and `together` libraries to generate responses from the different LLMs. Feel free to add any more custom functions to generate responses from other LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a099df-8c78-4878-8c0c-089ea66e088f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "import google.generativeai as genai\n",
    "from together import Together\n",
    "\n",
    "def generate_response_openai(prompt: str, model_name: str = \"gpt-4o-mini\"):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=1,\n",
    "        top_p=1\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    return response_text, input_tokens, output_tokens, total_tokens\n",
    "\n",
    "def generate_mistral_response(prompt: str, model_name: str = \"mistral-medium\"):\n",
    "    client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "    response = client.chat.complete(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=1,\n",
    "        top_p=1\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    return response_text, input_tokens, output_tokens, total_tokens\n",
    "\n",
    "def generate_gemini_response(prompt: str, model_name: str = \"gemini-1.5-flash\"):\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    response_text = response.text\n",
    "    input_tokens = response.usage_metadata.prompt_token_count\n",
    "    output_tokens = response.usage_metadata.candidates_token_count\n",
    "    total_tokens = response.usage_metadata.total_token_count\n",
    "    \n",
    "    return response_text, input_tokens, output_tokens, total_tokens\n",
    "\n",
    "def generate_llama_response(prompt: str, model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\"):\n",
    "    client = Together()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=1,\n",
    "        top_p=1\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    input_tokens = response.usage.prompt_tokens  \n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    return response_text, input_tokens, output_tokens, total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29952857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, model_name: str):\n",
    "    if \"gpt\" in model_name:\n",
    "        return generate_response_openai(prompt, model_name)\n",
    "    elif \"mistral\" in model_name:\n",
    "        return generate_mistral_response(prompt, model_name)\n",
    "    elif \"gemini\" in model_name:\n",
    "        return generate_gemini_response(prompt, model_name)\n",
    "    elif \"llama\" in model_name:\n",
    "        return generate_llama_response(prompt, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515e1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summarization_prompt(text: str) -> str:\n",
    "    prompt = f\"\"\"Please provide a clear and concise summary of the following patient notes. \n",
    "Focus on key medical information, diagnoses, treatments, and important observations.\n",
    "Keep the summary professional and maintain medical accuracy.\n",
    "\n",
    "Patient Notes:\n",
    "{text}\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d582e",
   "metadata": {},
   "source": [
    "## 4. Run Inference for 5 models with Galileo Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60468525",
   "metadata": {},
   "source": [
    "Here we define the models we want to evaluate, and the system prompt for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c9687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "models = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"mistral-medium\",\n",
    "    \"gemini-1.5-flash\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd136dc9-75fb-4b0c-ad6e-dd6eaec6f9c7",
   "metadata": {},
   "source": [
    "Now let's run the actual inference and log the information to Galileo! If you want to run the LLM summarization for more notes, set the sample size accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56868b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_notes = df.sample(10)['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for model_name in models:\n",
    "    evaluate_run = EvaluateRun(run_name=model_name, project_name=PROJECT_NAME, scorers=metrics)\n",
    "    for patient_note in tqdm(patient_notes):\n",
    "\n",
    "        prompt = create_summarization_prompt(patient_note)\n",
    "\n",
    "        # Create your workflow to log to Galileo.\n",
    "        wf = evaluate_run.add_workflow(input={\"prompt\": prompt, \"model_name\": model_name}, name=model_name, metadata={\"env\": \"demo\"})\n",
    "        \n",
    "        model_response, input_tokens, output_tokens, total_tokens = generate_response(prompt, model_name)\n",
    "\n",
    "        # Log your llm call step to Galileo.\n",
    "        wf.add_llm(\n",
    "            input=prompt,\n",
    "            output=model_response,\n",
    "            model=model_name,\n",
    "            input_tokens=input_tokens,\n",
    "            output_tokens=output_tokens,\n",
    "            total_tokens=total_tokens,\n",
    "            metadata={\"env\": \"demo\"},\n",
    "            name=f\"LLM run\",\n",
    "        )\n",
    "\n",
    "        # Conclude the workflow.\n",
    "        wf.conclude(output={\"output\": model_response})\n",
    "    evaluate_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcb67f",
   "metadata": {},
   "source": [
    "You can have a look at the final results in the console via the link generated from the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d5206-8856-4719-aec2-59d2f2b07e97",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Throughout this notebook, we have explored the process of creating and evaluating 5 summarization bots for patient notes via Open AI, Mistral, Gemini and Together with Galileo Evaluate. We covered essential steps, including setting up the environment, loading and preparing patient notes, generating summaries, and logging to Galileo."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "wizard_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
