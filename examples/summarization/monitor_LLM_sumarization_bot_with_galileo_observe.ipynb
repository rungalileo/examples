{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e8a996",
   "metadata": {},
   "source": [
    "# Building a Trustworthy LLM Summarization Bot with Galileo Observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d63f",
   "metadata": {},
   "source": [
    "In this tutorial, we'll build a LLM based summarization bot and monitor the results in Galileo Observe.\n",
    "\n",
    "This notebook pulls data from huggingface for its datasource and uses Open AI for LLM. Feel Free to change these sources as you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd504ea-4d94-4868-ba40-a929e015de08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set-up of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f544bc-442f-4c3e-8fa2-c35907eb29bc",
   "metadata": {},
   "source": [
    "Let's start by installing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c13e1-c5a0-47be-8cf0-c460fa788a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install galileo-observe openai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc56b04-eca5-48d5-93cc-f145bd02dedc",
   "metadata": {},
   "source": [
    "## 2. Set-up Galileo Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f2bc4-500c-4c71-9e52-f5eb8e0183da",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we will setup Galileo Observe client. You will need to enter 2 things - \n",
    " - GALILEO API KEY: This is the API key used to connect to the client. You can fetch this from the console\n",
    " - OPENAI API KEY: For this notebook we are using Open AI so enter your Open AI Key here. If you are using some other model, you can skip this\n",
    " - Project Name - Define a name for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4b885-0fd7-49d9-8676-424a0603a2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from galileo_observe import ObserveWorkflows\n",
    "\n",
    "os.environ[\"GALILEO_CONSOLE_URL\"] = \"https://console.dev.rungalileo.io\"\n",
    "os.environ[\"GALILEO_API_KEY\"] = \"\" # Enter Galileo Key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Enter Open AI Key here\n",
    "observe_logger = ObserveWorkflows(project_name=\"cookbook_summarization_monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcfaa9-16f9-49c5-9b2b-60221bbf9bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Loading and Preparing Data\n",
    "\n",
    "For this lab we will use a fictuous use case where we want to build a bot to summarize patient notes.\n",
    "\n",
    "Now in order to build the bot, we will \n",
    " - Fetch some patient notes from a website, \n",
    " - Use an openAI powered LLM to summarize the notes\n",
    " - Monitor the LLM's responses with the help of Galileo Observe\n",
    "\n",
    "In our case let's start by downloading some patient notes from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba938331-e2ed-4eb1-a907-9828d320807b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ncbi/Open-Patients\")\n",
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f2255-2a7c-4cb5-b8c7-1a50b58d9027",
   "metadata": {},
   "source": [
    "Lets have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791829cf-a1c6-449d-b98f-e9c66c9a331f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = int(df['description'].apply(len).mean())\n",
    "print(f'Average length of the documents is {avg_doc_length} characters.')\n",
    "std_doc_length = int(df['description'].apply(len).std())\n",
    "print(f'Standard deviation of the documents is {std_doc_length} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66065ed2-1b4b-42dc-830b-d4dcb10c2dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run Inference with Galileo Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a099df-8c78-4878-8c0c-089ea66e088f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a function to generate a response using Open AI\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_response(prompt: str, history: list = [], model_name: str = \"gpt-4o-mini\"):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=history + [{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=1,\n",
    "        top_p=1\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    return response_text, input_tokens, output_tokens, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60468525",
   "metadata": {},
   "source": [
    "Here we define the model we want to use, and the system prompt for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624eb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summarization_prompt(text: str) -> str:\n",
    "    prompt = f\"\"\"Please provide a clear and concise summary of the following patient notes. \n",
    "Focus on key medical information, diagnoses, treatments, and important observations.\n",
    "Keep the summary professional and maintain medical accuracy.\n",
    "\n",
    "Patient Notes:\n",
    "{text}\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45b7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd136dc9-75fb-4b0c-ad6e-dd6eaec6f9c7",
   "metadata": {},
   "source": [
    "Now let's run the actual inference and log the information to Galileo! If you want to run the LLM summarization for more notes, set the sample size accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19ec73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_notes = df.sample(10)['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for patient_note in tqdm(patient_notes):\n",
    "    prompt = create_summarization_prompt(patient_note)\n",
    "\n",
    "    # Create your workflow to log to Galileo.\n",
    "    wf = observe_logger.add_workflow(input={\"prompt\": prompt}, name=\"Summarization\", metadata={\"env\": \"demo\"})\n",
    "    \n",
    "    model_response, input_tokens, output_tokens, total_tokens = generate_response(prompt, model_name=MODEL_ID)\n",
    "\n",
    "    # Log your llm call step to Galileo.\n",
    "    wf.add_llm(\n",
    "        input=prompt,\n",
    "        output=model_response,\n",
    "        model=MODEL_ID,\n",
    "        input_tokens=input_tokens,\n",
    "        output_tokens=output_tokens,\n",
    "        total_tokens=total_tokens,\n",
    "        metadata={\"env\": \"demo\"},\n",
    "        name=\"Summarization\",\n",
    "    )\n",
    "\n",
    "    # Conclude the workflow.\n",
    "    wf.conclude(output={\"output\": model_response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf26966-dea1-456f-b0d5-07e430da58d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "As a last step, we shall upload all the gathered information to Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1aba67-8b30-4f2f-9622-37b56f257be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log the workflow to Galileo.\n",
    "logged_workflows = observe_logger.upload_workflows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcb67f",
   "metadata": {},
   "source": [
    "You can have a look at the final results in the console via the link generated from the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d5206-8856-4719-aec2-59d2f2b07e97",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Throughout this notebook, we have explored the process of creating and monitoring a summarization bot for a patient notes using GPT 4o mini via Open AI. We covered essential steps, including setting up the environment, loading and preparing patient notes, generating summaries, and logging to Galileo.\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "wizard_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
