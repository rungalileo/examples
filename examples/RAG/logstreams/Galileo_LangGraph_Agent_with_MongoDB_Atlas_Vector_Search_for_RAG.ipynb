{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Galileo: LangGraph Agent with MongoDB Atlas Vector Search for RAG\n",
        "\n",
        "This notebook demonstrates how to build a RAG (Retrieval Augmented Generation) agent using LangGraph, with MongoDB Atlas serving as the vector store. The agent will retrieve information from Lilian Weng's blog posts to answer questions.\n",
        "\n",
        "\n",
        "**Key Components:**\n",
        "1.  **Data Loading & Preprocessing:** Loads blog posts, splits them into chunks.\n",
        "2.  **MongoDB Atlas Vector Store:** Initializes and populates a vector store with document embeddings.\n",
        "3.  **Retriever:** Creates a retriever from the vector store.\n",
        "4.  **LangGraph Agent:** Defines an agent that can use the retriever tool to answer questions.\n",
        "5.  **Galileo Logging:** Integrates Galileo for observability."
      ],
      "metadata": {
        "id": "markdown-cell-1"
      },
      "id": "markdown-cell-1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation\n",
        "\n",
        "Install the necessary Python packages. The original script used `%pip install`, which is suitable for notebooks."
      ],
      "metadata": {
        "id": "markdown-cell-2"
      },
      "id": "markdown-cell-2"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet galileo \"pymongo[srv]\" langchain-mongodb tiktoken langchain-google-genai langchain-openai langgraph langchain-community langchain-text-splitters -q"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-1"
      },
      "id": "code-cell-1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup Environment and API Keys\n",
        "\n",
        "Import libraries and configure API keys. This notebook uses OpenAI for embeddings and the LLM, and a MongoDB Atlas URI for the vector store."
      ],
      "metadata": {
        "id": "markdown-cell-3"
      },
      "id": "markdown-cell-3"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.vectorstores import InMemoryVectorStore # Not used in final setup but present in original imports\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from pymongo import MongoClient\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from typing import Annotated, Sequence, TypedDict, Literal\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
        "from langchain_core.prompts import PromptTemplate # Not explicitly used but good practice to import if planning prompts\n",
        "\n",
        "from galileo.handlers.langchain import GalileoCallback\n",
        "from galileo import GalileoLogger\n",
        "\n",
        "# Configure OpenAI API Key\n",
        "# If running in Google Colab, it attempts to load the key from Colab secrets.\n",
        "# Otherwise, ensure the OPENAI_API_KEY environment variable is set.\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "  try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"OpenAI API Key loaded from Colab secrets.\")\n",
        "  except ImportError:\n",
        "    print(\"Not in Colab environment. Please set the OPENAI_API_KEY environment variable.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-2"
      },
      "id": "code-cell-2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load and Prepare Documents\n",
        "\n",
        "Load content from specified URLs (Lilian Weng's blog posts) and then split them into smaller, manageable chunks for embedding and retrieval."
      ],
      "metadata": {
        "id": "markdown-cell-4"
      },
      "id": "markdown-cell-4"
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
        "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
        "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "print(f\"Loaded {len(docs_list)} documents.\")\n",
        "print(f\"Split into {len(doc_splits)} chunks.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-4"
      },
      "id": "code-cell-4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Setup MongoDB Atlas Vector Store\n",
        "\n",
        "Initialize the MongoDB client and define database, collection, and vector search index names. Then, set up the `MongoDBAtlasVectorSearch` instance.\n",
        "For more info  [read more.](https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/vectorstores/langchain_mongodb.vectorstores.MongoDBAtlasVectorSearch.html#langchain_mongodb.vectorstores.MongoDBAtlasVectorSearch)\n",
        "\n",
        "\n",
        "\n",
        "If your MongoDB Atlas cluster has IP whitelisting enabled, you might need to add the IP address of your current environment. The command below helps find your public IP. Add this IP to your Atlas cluster's Network Access list.\n",
        "You can check your IP via `curl checkip.amazonaws.com`.\n",
        "\n",
        "For `MongoDBAtlasVectorSearch` to work, a vector search index (named `ATLAS_VECTOR_SEARCH_INDEX_NAME` in this case) must exist in your MongoDB Atlas collection"
      ],
      "metadata": {
        "id": "markdown-cell-5"
      },
      "id": "markdown-cell-5"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MongoDB Atlas connection URI\n",
        "# Replace with your actual MongoDB Atlas URI if different\n",
        "uri = \"mongodb+srv://<ATLAS-ACCOUNT>.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "# Initialize MongoDB python client\n",
        "client = MongoClient(uri)\n",
        "\n",
        "DB_NAME = \"langchain_test_db\"\n",
        "COLLECTION_NAME = \"langchain_test_vectorstores\"\n",
        "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"langchain-test-index-vectorstores\"\n",
        "\n",
        "MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
        "\n",
        "print(f\"Using MongoDB Database: {DB_NAME}, Collection: {COLLECTION_NAME}\")\n",
        "print(f\"Expected Atlas Vector Search Index: {ATLAS_VECTOR_SEARCH_INDEX_NAME}\")\n",
        "# Initialize OpenAI Embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Initialize MongoDBAtlasVectorSearch\n",
        "vector_store = MongoDBAtlasVectorSearch(\n",
        "    collection=MONGODB_COLLECTION,\n",
        "    embedding=embeddings,\n",
        "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "    relevance_score_fn=\"cosine\", # Optional: specifies the score function, cosine is common\n",
        ")\n",
        "\n",
        "vector_store.create_vector_search_index(dimensions=1536)\n",
        "\n",
        "print(\"MongoDBAtlasVectorSearch initialized.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-7"
      },
      "id": "code-cell-7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Populate Vector Store and Create Retriever\n",
        "\n",
        "Add the processed document chunks to the MongoDB Atlas vector store and then create a retriever interface for querying."
      ],
      "metadata": {
        "id": "markdown-cell-8"
      },
      "id": "markdown-cell-8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique IDs for each document chunk\n",
        "uuids = [str(uuid4()) for _ in range(len(doc_splits))]\n",
        "\n",
        "# Add documents to the vector store\n",
        "# This will embed the documents and store them in MongoDB Atlas.\n",
        "# Ensure your MongoDB collection is empty or you manage IDs appropriately if re-running.\n",
        "print(f\"Adding {len(doc_splits)} document chunks to MongoDB Atlas...\")\n",
        "try:\n",
        "    # For a clean run, you might want to delete existing documents if re-populating the same collection.\n",
        "    # MONGODB_COLLECTION.delete_many({})\n",
        "    # print(\"Cleared existing documents from the collection.\")\n",
        "    vector_store.add_documents(documents=doc_splits, ids=uuids)\n",
        "    print(\"Documents added successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error adding documents: {e}\")\n",
        "    print(\"This might be due to issues with index existence/configuration or network access.\")\n",
        "\n",
        "# Create a retriever from the vector store\n",
        "retriever = vector_store.as_retriever()\n",
        "print(\"Retriever created.\")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_blog_posts\",\n",
        "    \"Search and return information about Lilian Weng blog posts.\",\n",
        ")\n",
        "\n",
        "tools = [retriever_tool]\n",
        "\n",
        "print(\"Retriever tool created.\")\n",
        "\n",
        "# Test the retriever tool (optional)\n",
        "print(\"\\nTesting retriever tool...\")\n",
        "try:\n",
        "    test_retrieval = retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n",
        "    print(\"Retriever tool test output:\", test_retrieval)\n",
        "except Exception as e:\n",
        "    print(f\"Error testing retriever tool: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-8"
      },
      "id": "code-cell-8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Build the LangGraph Agent\n",
        "\n",
        "Define the agent's state, the core agent logic (which decides whether to use tools or respond directly), and construct the graph."
      ],
      "metadata": {
        "id": "markdown-cell-10"
      },
      "id": "markdown-cell-10"
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "  # The add_messages function defines how an update should be processed\n",
        "  # Default is to replace. add_messages says \"append\"\n",
        "  messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "def agent(state):\n",
        "  \"\"\"\n",
        "  Invokes the agent model to generate a response based on the current state. Given\n",
        "  the question, it will decide to retrieve using the retriever tool, or simply end.\n",
        "\n",
        "  Args:\n",
        "      state (messages): The current state\n",
        "\n",
        "  Returns:\n",
        "      dict: The updated state with the agent response appended to messages\n",
        "  \"\"\"\n",
        "  print(\"--- CALLING AGENT ---\")\n",
        "  messages = state[\"messages\"]\n",
        "  # Using gpt-4-turbo as it's generally better at tool use\n",
        "  model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo-preview\") # or \"gpt-3.5-turbo\"\n",
        "  model = model.bind_tools(tools)\n",
        "  response = model.invoke(messages)\n",
        "  # We return a list, because this will get added to the existing list\n",
        "  return {\"messages\": [response]}\n",
        "\n",
        "retrieve_node = ToolNode([retriever_tool])\n",
        "\n",
        "# Define the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"agent\", agent)\n",
        "workflow.add_node(\"retrieve\", retrieve_node)\n",
        "\n",
        "# Define edges\n",
        "workflow.add_edge(START, \"agent\") # Start with the agent\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tools_condition, # LangGraph's built-in function to check for tool calls\n",
        "    {\n",
        "        # If the agent decided to call a tool, execute the retrieve node\n",
        "        \"tools\": \"retrieve\",\n",
        "        # Otherwise, END the workflow\n",
        "        END: END,\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"retrieve\", \"agent\") # After retrieval, go back to the agent to process results\n",
        "\n",
        "# Compile the graph\n",
        "graph = workflow.compile()\n",
        "print(\"LangGraph agent compiled.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-10"
      },
      "id": "code-cell-10"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Setup Galileo Logging ✨\n",
        "\n",
        "Configure environment variables for Galileo and initialize the Galileo callback handler for Langchain.\n",
        "\n",
        "🔗 See full documentation [here](https://v2docs.galileo.ai/references/faqs/faqs#q%3A-how-do-i-authenticate-with-the-galileo-api%3F)\n",
        "\n"
      ],
      "metadata": {
        "id": "markdown-cell-11"
      },
      "id": "markdown-cell-11"
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your Galileo API Key, Project Name, and desired Log Stream\n",
        "os.environ[\"GALILEO_API_KEY\"] = \"<YOUR_GALILEO_API_KEY>\" # Replace with your key\n",
        "os.environ[\"GALILEO_PROJECT\"]= \"elastic\" # Replace with your project name\n",
        "os.environ[\"GALILEO_LOG_STREAM\"] = \"my_log_stream\" # Replace with your stream name\n",
        "\n",
        "galileo_handler = GalileoCallback()\n",
        "\n",
        "print(\"Galileo callback handler initialized.\")\n",
        "print(f\"GALILEO_API_KEY set: {'GALILEO_API_KEY' in os.environ}\")\n",
        "print(f\"GALILEO_PROJECT: {os.getenv('GALILEO_PROJECT')}\")\n",
        "print(f\"GALILEO_LOG_STREAM: {os.getenv('GALILEO_LOG_STREAM')}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-13"
      },
      "id": "code-cell-13"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Run the Agent\n",
        "\n",
        "Define an input query and invoke the compiled LangGraph agent. The Galileo callback will capture traces."
      ],
      "metadata": {
        "id": "markdown-cell-12"
      },
      "id": "markdown-cell-12"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"What does Lilian Weng say about the types of agent memory?\"),\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Invoking agent with query: {inputs['messages'][0].content}\")\n",
        "\n",
        "try:\n",
        "    # The recursion_limit is important for agents that might loop.\n",
        "    # Callbacks list includes the Galileo handler.\n",
        "    final_state = graph.invoke(inputs, {\"recursion_limit\": 5, \"callbacks\": [galileo_handler]})\n",
        "\n",
        "    print(\"\\n--- FINAL RESPONSE ---\")\n",
        "    if final_state and 'messages' in final_state and final_state['messages']:\n",
        "        # The final response is usually the last message from the AI\n",
        "        print(final_state['messages'][-1].content)\n",
        "    else:\n",
        "        print(\"No final message content found in the state.\")\n",
        "        print(\"Final state:\", final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during graph invocation: {e}\")\n",
        "    print(\"This could be due to API key issues, network problems, or errors in the agent/tool logic.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "code-cell-14"
      },
      "id": "code-cell-14"
    }
  ]
}