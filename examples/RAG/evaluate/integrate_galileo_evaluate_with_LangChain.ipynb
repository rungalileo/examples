{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbKozotp-mZ2"
      },
      "source": [
        "## Setup: Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nv-WyzsC68MW"
      },
      "outputs": [],
      "source": [
        "! pip install promptquality\n",
        "! pip install --upgrade --quiet langchain langchain-openai langchain-community chromadb langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFPsYK-h-otD"
      },
      "source": [
        "## Construct Dataset and Embed Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hA2JZne5_ft"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Load sample data (text) from webpage\n",
        "loader = WebBaseLoader(\"https://www.rungalileo.io/blog/deep-dive-into-llm-hallucinations-across-generative-tasks\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split text into documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# Define key to embed docs via OpenAI embeddings\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Embed split text and insert into vector db\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "# Create our retriever\n",
        "retriever = vectordb.as_retriever(search_kwargs={'k': 3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0i69ZGTPZyG"
      },
      "source": [
        "## Define the Pieces of Our Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSDoc_lh8GUB"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.document import Document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import List\n",
        "\n",
        "def format_docs(docs: List[Document]) -> str:\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET_HNVASPtzz"
      },
      "source": [
        "## Run Our Chain and Submit Callback to Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OpktNAa8z8g"
      },
      "outputs": [],
      "source": [
        "import promptquality as pq\n",
        "\n",
        "# Environment variable 'GALILEO_API_KEY' will be retrieved by the login() sequence to the Galileo cluster url\n",
        "os.environ['GALILEO_API_KEY'] = '' # Enter Galileo key here\n",
        "os.environ['GALILEO_CONSOLE_URL'] = 'https://console.demo.rungalileo.io/'\n",
        "GALILEO_PROJECT_NAME = 'galileoblog-rag'\n",
        "config = pq.login(os.environ['GALILEO_CONSOLE_URL'])\n",
        "\n",
        "q_list = [\n",
        "    \"What are hallucinations in LLMs?\",\n",
        "    \"What is the difference between intrinsic and extrinsic hallucinations?\",\n",
        "    \"How do hallucinations impact abstractive summarization?\",\n",
        "    \"What are some examples of hallucinations in dialogue generation?\",\n",
        "    \"How does generative question answering lead to hallucinations?\",\n",
        "    \"What intrinsic and extrinsic errors occur in neural machine translation?\",\n",
        "    \"How does data-to-text generation exhibit hallucinations?\",\n",
        "    \"What are intrinsic and extrinsic object hallucinations in vision-language models?\",\n",
        "    \"Why is addressing hallucinations important for AI applications?\",\n",
        "    \"What methods are suggested to mitigate hallucinations in LLMs?\"\n",
        "]\n",
        "\n",
        "# Create callback handler\n",
        "prompt_handler = pq.GalileoPromptCallback(\n",
        "    project_name=GALILEO_PROJECT_NAME, scorers=[pq.Scorers.latency, pq.Scorers.groundedness, pq.Scorers.factuality]\n",
        ")\n",
        "\n",
        "# Run your chain experiments across multiple inputs with the galileo callback\n",
        "chain.batch(q_list, config=dict(callbacks=[prompt_handler]))\n",
        "\n",
        "# publish the results of your run\n",
        "prompt_handler.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
