{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e8a996",
   "metadata": {},
   "source": [
    "# Building a Trustworthy RAG Chatbot using ðŸ”­ Galileo Observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d63f",
   "metadata": {},
   "source": [
    "In this tutorial, we'll build a RAG based chatbot and monitor the results in Galileo Observe.\n",
    "\n",
    "This notebook pulls data from the web for its datasource and uses Open AI for LLM. Feel Free to change these sources as you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd504ea-4d94-4868-ba40-a929e015de08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set-up of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f544bc-442f-4c3e-8fa2-c35907eb29bc",
   "metadata": {},
   "source": [
    "Let's start by installing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c13e1-c5a0-47be-8cf0-c460fa788a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install galileo-observe langchain langchain-community langchain_openai faiss-cpu openai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc56b04-eca5-48d5-93cc-f145bd02dedc",
   "metadata": {},
   "source": [
    "## 2. Set-up Galileo Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f2bc4-500c-4c71-9e52-f5eb8e0183da",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we will setup Galileo Observe client. You will need to enter 2 things - \n",
    " - GALILEO API KEY: This is the API key used to connect to the client. You can fetch this from the console\n",
    " - OPENAI API KEY: For this notebook we are using Open AI so enter your Open AI Key here. If you are using some other model, you can skip this\n",
    " - Project Name - Define a name for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4b885-0fd7-49d9-8676-424a0603a2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from galileo_observe import ObserveWorkflows\n",
    "\n",
    "os.environ[\"GALILEO_CONSOLE_URL\"] = \"https://console.dev.rungalileo.io\"\n",
    "os.environ[\"GALILEO_API_KEY\"] = \"\" # Enter Galileo key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Enter Open AI Key here\n",
    "observe_logger = ObserveWorkflows(project_name=\"observe-rag-chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcfaa9-16f9-49c5-9b2b-60221bbf9bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Loading and Preparing Data\n",
    "\n",
    "For this lab we will use a fictuous use case where we want to build a chatbot to answer questions about Galileo. A typical technique to build such a chatbot is Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "Now in order to build the chatbot, we will first fetch some documents from Galileo's website, then create some questions, and then ask the chatbot those questions and check how are the responses based off the documents, with the help of Galileo Observe\n",
    "\n",
    "In our case let's start by downloading some documents for Galileo from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba938331-e2ed-4eb1-a907-9828d320807b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load data from a website URL\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://docs.rungalileo.io/galileo\",\n",
    "    \"https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-guardrail-metrics/context-adherence\",\n",
    "    \"https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-guardrail-metrics/context-relevance\"\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f2255-2a7c-4cb5-b8c7-1a50b58d9027",
   "metadata": {},
   "source": [
    "Now that the context data in the form of the documents has been downloaded we will now split them into smaller text chunks using the Langchain library. The CharacterTextSplitter divides the text into chunks of a specified size while allowing for overlap to prevent cutting sentences in half. When setting the chunk size, make sure it fits into the context window of your LLM and feel free to experiment with different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791829cf-a1c6-449d-b98f-e9c66c9a331f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a548e60-0e64-4b9d-bc11-31ec9b491643",
   "metadata": {},
   "source": [
    "Let's have a look at the size of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8b103-c80c-4e84-8c33-d2a3502f6227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print metadata of the loaded documents\n",
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(texts)\n",
    "print(f'Average length among {len(documents)} pages loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split you have {len(texts)}')\n",
    "print(f'Average length among {len(texts)} chunks is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6457d-0f85-4229-9e35-fd4510b385a0",
   "metadata": {},
   "source": [
    "Next we convert our chunks into embeddings and store them in a vector database. This is a common technique used in RAG where instead of always passing all the documents to the LLM as context, we will pull the chunks we feel are most relevant to a given question and only pass those to the LLM. This is achieved by doing a semantic similarity search within the vector DB between the question embeddings and the chunk embeddings. Passing concise information to the LLM helps improve its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc90761-6b98-47b8-8c86-ab45d7a3c211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66065ed2-1b4b-42dc-830b-d4dcb10c2dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run Inference with Galileo Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a099df-8c78-4878-8c0c-089ea66e088f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a function to generate a response using Open AI\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_response(prompt: str, history: list = [], model_name: str = \"gpt-4o-mini\"):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=history + [{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=1,\n",
    "        top_p=1\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    return response_text, input_tokens, output_tokens, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92673e-9d92-40d2-aa96-ae594919ed53",
   "metadata": {},
   "source": [
    "If you want to type in your own questions on the fly for the LLM for a richer chatbot experience, set `USE_PREDEFINED_QUESTIONS` to False. Otherwise the model will run on these pre-defined questions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fcfcb-7238-480b-a87e-0a7251c4ec63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_PREDEFINED_QUESTIONS = True\n",
    "\n",
    "questions = [\n",
    "    \"What does Galileo do?\",\n",
    "    \"What are some of the RAG Metrics Galileo provides?\",\n",
    "    \"What is LUNA and where is it used?\",\n",
    "    \"How does LUNA calculate context adherence?\",\n",
    "    \"What is chainpoll?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60468525",
   "metadata": {},
   "source": [
    "Here we define the model we want to use, and the system prompt for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gpt-4o-mini\"\n",
    "history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd136dc9-75fb-4b0c-ad6e-dd6eaec6f9c7",
   "metadata": {},
   "source": [
    "Now let's run the actual inference and log the information to Galileo! If you want to run the LLM chat longer, set the `max_rounds` variable accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 0\n",
    "max_rounds = 5\n",
    "\n",
    "while rounds < max_rounds:\n",
    "    if USE_PREDEFINED_QUESTIONS:\n",
    "        question = questions[rounds]\n",
    "    else:\n",
    "        import speech_recognition as sr\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening... (speak 'exit' to end)\")\n",
    "            audio = recognizer.listen(source)\n",
    "            try:\n",
    "                question = recognizer.recognize_google(audio)\n",
    "                print(\"You said:\", question)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "                question = \"\"\n",
    "            except sr.RequestError:\n",
    "                print(\"Could not request results\")\n",
    "                question = \"\"\n",
    "    \n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    # Retrieve relevant documents from the vector store\n",
    "    relevant_docs = vectorstore.similarity_search(question, k=3)\n",
    "    context_list = [doc.page_content for doc in relevant_docs]\n",
    "    context = \" \".join(context_list)\n",
    "    prompt = f\"\"\"Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    # Create your workflow to log to Galileo.\n",
    "    wf = observe_logger.add_workflow(input={\"question\": question}, name=\"Chatbot\", metadata={\"env\": \"demo\"})\n",
    "    wf.add_retriever(\n",
    "        input=question,\n",
    "        documents=context_list,\n",
    "        metadata={\"env\": \"demo\"},\n",
    "        name=\"Chatbot\",\n",
    "    )\n",
    "    \n",
    "    # Generate the response with the updated history\n",
    "    model_response, input_tokens, output_tokens, total_tokens = generate_response(prompt, model_name=MODEL_ID, history=history)\n",
    "    \n",
    "    # Add the current question to the history\n",
    "    history.append({\"role\": \"user\", \"content\": question})\n",
    "    # Update history with the new interaction\n",
    "    history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "    print(\"You: \", question)\n",
    "    print(f\"Assistant: {model_response}\")\n",
    "    print(\"*\" * 100)\n",
    "\n",
    "\n",
    "    # Log your llm call step to Galileo.\n",
    "    wf.add_llm(\n",
    "        input=prompt,\n",
    "        output=model_response,\n",
    "        model=MODEL_ID,\n",
    "        input_tokens=input_tokens,\n",
    "        output_tokens=output_tokens,\n",
    "        total_tokens=total_tokens,\n",
    "        metadata={\"env\": \"demo\"},\n",
    "        name=\"Chatbot\",\n",
    "    )\n",
    "\n",
    "    # Conclude the workflow.\n",
    "    wf.conclude(output={\"output\": model_response})\n",
    "    rounds += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf26966-dea1-456f-b0d5-07e430da58d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "As a last step, we shall upload all the gathered information to Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1aba67-8b30-4f2f-9622-37b56f257be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log the workflow to Galileo.\n",
    "logged_workflows = observe_logger.upload_workflows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcb67f",
   "metadata": {},
   "source": [
    "You can have a look at the final results in the console via the link generated from the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d5206-8856-4719-aec2-59d2f2b07e97",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Throughout this notebook, we have explored the process of creating and evaluation a chatbot for a QA-RAG application using GPT 4o mini via Open AI, Python, and Langchain. We covered essential steps, including setting up the environment, loading and preparing context data, extracting relevant context, answer generation, and logging to Galileo."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "wizard_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
