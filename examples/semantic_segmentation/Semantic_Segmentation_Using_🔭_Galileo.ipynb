{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"shw_4MU8mLbR"},"source":["# 1. Install dependencies and download the data should take about 5 miutes"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qalu2lgvvJNu","outputId":"4e64f37c-03ad-46d4-e4df-6ee46f77f0c4"},"outputs":[],"source":["!pip install -U dataquality  &> /dev/null\n","import os\n","from tqdm import tqdm\n","import torch \n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import numpy as np\n","\n","# download images\n","import os\n","os.system('gsutil cp -r \\\n","  \"gs://galileo-public-data/CV_datasets/Segmentation_Data\" \\\n","  . &> /dev/null')\n","\n","dataset_path = os.path.abspath(\"Segmentation_Data\")\n","\n","IMG_SIZE = 256\n","NC = 21  # Number of classes"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uikjPOjWvu8Q"},"source":["# 2. Create the dataset class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZS9o7-2qFoA"},"outputs":[],"source":["from typing import Optional, Dict, Union\n","from PIL import Image\n","\n","class coco_hf_dataset_disk(torch.utils.data.Dataset):\n","    def __init__(self, \n","                 dataset_path: str,\n","                 relative_img_path: str, \n","                 relative_mask_path: str,\n","                 mask_transform: transforms=None, \n","                 img_transform: transforms=None, \n","                 size: int=1024,) -> None:\n","        \"\"\"\"\n","        COCO val dataset from galileo-public-data/CV_datasets/COCO_seg_val_5000/all_images\n","        downloaded and located on disk.\n","        If no paths are provided we download the dataset from GCS and save it to disk.\n","\n","        :param dataset_path: path to dataset\n","        :param relative_img_path: path to images relative to the dataset path\n","        :param relative_maks_path: path to masks relative to the dataset path\n","        :param mask_transform: transforms to apply to masks\n","        :param img_transform: transforms to apply to images\n","        :param size: size of image and mask\n","        \"\"\"\n","        super(coco_hf_dataset_disk, self).__init__()\n","\n","        self.dataset_path = dataset_path\n","        self.relative_img_path = relative_img_path\n","        self.relative_mask_path = relative_mask_path\n","        self.images = sorted(os.listdir(os.path.join(dataset_path, relative_img_path)))\n","        self.masks = sorted(os.listdir(os.path.join(dataset_path, relative_mask_path)))\n","        # remove .DS_Store\n","        if self.images[0] == '.DS_Store':\n","            self.images = self.images[1:]\n","        if self.masks[0] == '.DS_Store':\n","            self.masks = self.masks[1:]\n","\n","        num_images = len(self.images)\n","        num_masks = len(self.masks)\n","        print(f\"Found dataset, there are {num_images} images and {num_masks} masks\")\n","\n","        self.mask_transform = mask_transform\n","        self.img_transform = img_transform\n","\n","        self.class_dict = { 'background': 0, 'airplane': 1, 'bicycle': 2,\n","                            'bird': 3, 'boat': 4, 'bottle': 5, 'bus': 6,\n","                            'car': 7, 'cat': 8, 'chair': 9, 'cow': 10,\n","                            'dining table': 11,'dog': 12,'horse': 13,\n","                            'motorcycle': 14,'person': 15,'potted plant': 16,\n","                            'sheep': 17, 'couch': 18, 'train': 19, 'tv': 20}\n","        self.labels = [i for i in self.class_dict]\n","                        \n","        self.int2str = {v: k for k, v in self.class_dict.items()}\n","        self.size = size\n","\n","    def __len__(self) -> int:\n","        # returns length of the dataset\n","        return len(self.images)\n","\n","    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, int, np.ndarray]]:\n","        # gets a single item from our dataset\n","        \n","        image_path = os.path.join(self.dataset_path, self.relative_img_path, self.images[idx])\n","        mask_path = os.path.join(self.dataset_path, self.relative_mask_path, self.masks[idx])\n","        image = Image.open(image_path)\n","        mask = Image.open(mask_path)\n","\n","        # resize image and mask to given size\n","        unnormalized_image = image.copy().resize((self.size, self.size), resample=Image.NEAREST)\n","        unnormalized_image = transforms.ToTensor()(unnormalized_image)\n","        unnormalized_image = expand_gray_channel()(unnormalized_image)\n","        unnormalized_image = np.array(unnormalized_image)\n","        \n","\n","        if self.img_transform:\n","            image = self.img_transform(image)\n","        if self.mask_transform:\n","            mask = self.mask_transform(mask)\n","        \n","        return {'image': image,\n","                'image_path': image_path,\n","                'mask_path': mask_path,\n","                'mask': mask,\n","                'idx': idx,\n","                'unnormalized_image': unnormalized_image}\n","\n","\n","class expand_gray_channel:\n","    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n","        # torch transform to expand gray channel to 3 channels\n","        if tensor.shape[0] > 3:\n","            tensor = tensor.unsqueeze(0)\n","        if tensor.shape[0] == 1:\n","            return tensor.expand(3, -1, -1)\n","        return tensor\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ALIZoeAKyQlj"},"source":["# 3. Create the datasets, dataloaders, model and optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2sdPZFjyO7B"},"outputs":[],"source":["img_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BICUBIC),\n","    expand_gray_channel(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","mask_transforms = transforms.Compose([\n","    transforms.PILToTensor(),\n","    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.NEAREST),\n","])\n","\n","train_image_path = 'train/images'\n","train_mask_path = 'train/masks'\n","validation_image_path = 'validation/images'\n","validation_mask_path = 'validation/masks'\n","\n","train_dataset = coco_hf_dataset_disk(dataset_path=dataset_path,\n","                                    relative_img_path=train_image_path, \n","                                    relative_mask_path=train_mask_path,\n","                                    img_transform=img_transforms,\n","                                     mask_transform=mask_transforms,\n","                                    size=IMG_SIZE)\n","validation_dataset = coco_hf_dataset_disk(dataset_path=dataset_path,\n","                                    relative_img_path=validation_image_path, \n","                                    relative_mask_path=validation_mask_path,\n","                                    img_transform=img_transforms,\n","                                     mask_transform=mask_transforms,\n","                                    size=IMG_SIZE)\n","labels = train_dataset.labels\n","train_dataloader = DataLoader(train_dataset, batch_size = 6, shuffle=True, num_workers=4)\n","validation_dataloader = DataLoader(validation_dataset, batch_size = 6, shuffle=True, num_workers=4)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True).to(device)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = .00001)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y7jC3C-K0F2b"},"source":["# 4. Train with Dataquality  ðŸ”­"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G06JZVz_0E7v"},"outputs":[],"source":["import dataquality as dq\n","from dataquality.integrations.cv.torch.semantic_segmentation import watch\n","\n","# set to avoid being prompted for credentials\n","# %env GALILEO_CONSOLE_URL = \"https://console.cloud.rungalileo.io/\" \n","# %env GALILEO_USERNAME = \n","# %env GALILEO_PASSWORD = \n","\n","# dq.configure()\n","dq.init(\"semantic_segmentation\", \"Segmentation_Project\", 'COCO_dataset')\n","\n","watch(\n","    model,\n","    bucket_url='https://storage.googleapis.com/galileo-public-data/CV_datasets/COCO_segmentation_example_data',\n","    dataset_path=dataset_path,\n","    dataloaders={\"training\": train_dataloader,\n","                 \"validation\": validation_dataloader},\n",")\n","dq.set_labels_for_run(labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40ca8s8tXmtL"},"outputs":[],"source":["# train for one epoch\n","\n","scaler = torch.cuda.amp.GradScaler()\n","epochs = 1\n","\n","with torch.autocast('cuda'):\n","    for epoch in range(epochs):\n","        dq.set_epoch_and_split(epoch, \"training\")\n","        for j, sample in enumerate(tqdm(train_dataloader)):\n","            imgs, masks = sample['image'], sample['mask']\n","            out = model(imgs.to(device))\n","\n","            # reshape to have loss for each pixel (bs * h * w, 21)\\n\",\n","            pred = out['out'].permute(0, 2, 3, 1).contiguous().view( -1, 21)\n","            masks = masks.long()\n","            msks_for_loss = masks.view(-1).to(device)\n","\n","            loss = criterion(pred, msks_for_loss)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CAqZVAbiXryu"},"outputs":[],"source":["dq.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPjTFUR776gZDthnyhxh+8R","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
