{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5JVjGAAJOTn"
      },
      "source": [
        "# Multi Label Text Classification using Pytorch and üî≠ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UfMcPbg19uz1"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality` with `pip install dataquality`\n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip and install dependencies\n",
        "    !pip install -U pip &> /dev/null\n",
        "    !pip install dataquality transformers datasets torchmetrics==0.10.0 1> /dev/null\n",
        "\n",
        "    print('üëã Installed necessary libraries and restarting runtime! This should only need to happen once.')\n",
        "    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKPfMhYvgeL"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C7B55N_B30kG"
      },
      "outputs": [],
      "source": [
        "#@title ü§ó HuggingFace Dataset\n",
        "#@markdown You can find more datasets [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:text-classification&task_ids=task_ids:multi-label-classification&sort=downloads).\n",
        "\n",
        "dataset_name = 'go_emotions' #@param [\"go_emotions\", \"lex_glue\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config)\n",
        "\n",
        "\n",
        "# Check that the dataset has at least train and either of validation/test\n",
        "assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n",
        "f\"üíæ The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n",
        "\n",
        "print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")\n",
        "\n",
        "# A small function for minimizing the dataset for testing\n",
        "import os\n",
        "\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B1Y0P4PP_U-a"
      },
      "outputs": [],
      "source": [
        "#@markdown Convert HF dataset to Pandas dataframes \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def load_pandas_df(data):\n",
        "  # Find the name of the ground truth column\n",
        "  good_col_names = [name for name in list(data['train'].features) if \"label\" in name]\n",
        "  if len(good_col_names) == 1:\n",
        "    label_col = good_col_names[0]\n",
        "  else:\n",
        "    col_names = list(data['train'].features)\n",
        "    print(f\"The name of the columns are {col_names}.\")\n",
        "    label_col = input(f\"üèÖ Please enter the name of the column containing the labels: \")\n",
        "    assert label_col in col_names, f\"{label_col} is not an existing column\"\n",
        "\n",
        "  # Load the labels in a dictionary\n",
        "  num_classes = len(data['train'].features[label_col].feature.names)\n",
        "  labels_cols = data['train'].features[label_col].feature.int2str(range(0, num_classes))\n",
        "\n",
        "  def binarize_label_indices(label_idxs):\n",
        "    a = np.zeros(len(labels_cols), dtype=int)\n",
        "    a[label_idxs] = 1\n",
        "    return a\n",
        "\n",
        "  # Load the train data into a frame\n",
        "  train_data = data[\"train\"]\n",
        "  train_df = pd.DataFrame.from_dict(train_data)\n",
        "  train_labels = list(map(binarize_label_indices, data['train'][label_col]))\n",
        "  _train_df_labels = pd.DataFrame(train_labels, columns=labels_cols)\n",
        "  train_df = pd.concat([train_df, _train_df_labels], axis=1)\n",
        "  train_df['id'] = train_df.index\n",
        "\n",
        "  # Load the test data into a frame\n",
        "  test_split_name = \"validation\" if \"validation\" in data else \"test\"\n",
        "  test_data = data[test_split_name]\n",
        "  test_df = pd.DataFrame.from_dict(test_data)\n",
        "  test_labels = list(map(binarize_label_indices, data[test_split_name][label_col]))\n",
        "  _test_df_labels = pd.DataFrame(test_labels, columns=labels_cols)\n",
        "  test_df = pd.concat([test_df, _test_df_labels], axis=1)\n",
        "  test_df['id'] = test_df.index\n",
        "  \n",
        "  return train_df, test_df, labels_cols\n",
        "\n",
        "# data = load_dataset(hf_dataset)\n",
        "train_df, test_df, labels_cols = load_pandas_df(data)\n",
        "\n",
        "if _minimize_for_ci():\n",
        "  train_df, test_df = train_df[:10], test_df[:10]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "nTrPjUO1FuEu"
      },
      "outputs": [],
      "source": [
        "#@title 2. Preprocess dataset and prepare training\n",
        "#@markdown Most PyTorch training is done with the Dataloader.\n",
        "#@markdown After preprocessing the data (tokenizing) we pass it on to it.\n",
        "#@markdown Galileo relies on PyTorch's DataLoader.\n",
        "#@markdown\n",
        "#@markdown ```python\n",
        "#@markdown train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "#@markdown val_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "#@markdown ```\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from transformers import AutoModel\n",
        "import torchmetrics\n",
        "from torchmetrics import HammingDistance\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class MultiLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset: pd.DataFrame, split: str, list_of_labels: List[str] = None):\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.num_samples = len(self.dataset)\n",
        "\n",
        "        self.labels = list_of_labels\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.encodings = tokenizer(\n",
        "            self.dataset[\"text\"].tolist(), truncation=True, padding=True\n",
        "        )\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.encodings[\"input_ids\"][idx])\n",
        "        attention_mask = torch.tensor(self.encodings[\"attention_mask\"][idx])\n",
        "        y = torch.tensor(self.dataset[self.labels].iloc[idx])\n",
        "        return x, attention_mask, self.dataset['id'].iloc[idx], y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "train_dataset = MultiLabelDataset(\n",
        "    train_df, \n",
        "    split=\"training\", \n",
        "    list_of_labels = labels_cols, \n",
        ")\n",
        "\n",
        "test_dataset = MultiLabelDataset(\n",
        "    test_df, \n",
        "    split=\"validation\",\n",
        "    list_of_labels = labels_cols,\n",
        ")\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        ")\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "class TextMultiLabelClassificationModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_tasks):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        hidden_size = self.feature_extractor.config.hidden_size\n",
        "        \n",
        "        self.pre_classifier = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier = torch.nn.Linear(hidden_size, num_tasks)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        \"\"\"Model forward function.\"\"\"\n",
        "        encoded_layers = self.feature_extractor(\n",
        "            input_ids=x, attention_mask=attention_mask\n",
        "        ).last_hidden_state\n",
        "        # Extract [CLS]-token\n",
        "        classification_embedding = encoded_layers[:, 0]\n",
        "\n",
        "        emb = self.pre_classifier(classification_embedding)\n",
        "        emb = self.relu(emb)\n",
        "        emb = self.dropout(emb)\n",
        "        logits = self.classifier(emb)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model = TextMultiLabelClassificationModel(num_tasks=len(labels_cols))\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5\n",
        ")\n",
        "\n",
        "train_acc = torchmetrics.Accuracy()\n",
        "val_acc = torchmetrics.Accuracy()\n",
        "\n",
        "train_hamming_distance = HammingDistance()\n",
        "val_hamming_distance = HammingDistance()\n",
        "\n",
        "loss_fn = F.binary_cross_entropy_with_logits\n",
        "\n",
        "def extract_batch(batch):\n",
        "  x, attention_mask, x_idxs, y = batch\n",
        "  x = x.to(device)\n",
        "  attention_mask = attention_mask.to(device)\n",
        "  y = y.to(device)\n",
        "  return x, y, attention_mask\n",
        "\n",
        "def train_loop(model,dataloader,optimizer,loss_fn):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for batch in tqdm(dataloader):\n",
        "      # print statistics\n",
        "      x, y, attention_mask = extract_batch(batch)\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward + backward + optimize\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits, y.float())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      #train_acc(logits.argmax(1), y)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "def test_loop(model, dataloader, loss_fn, epoch):\n",
        "  model.eval()\n",
        "  validation_loss = 0.0\n",
        "  pbar = tqdm(dataloader)\n",
        "  for batch in pbar:\n",
        "    x, y, attention_mask = extract_batch(batch)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits, y)\n",
        "    validation_loss += loss.item()\n",
        "    val_hamming_distance(logits, y.float())\n",
        "    pbar.set_description(\"[epoch %d] Validation loss: %.3f\" % (epoch + 1, validation_loss))\n",
        "  pbar.set_description(f\"Val hamming score: {val_hamming_distance.compute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6zJHmvxTJ2pN"
      },
      "source": [
        "## 3. Monitor with Galileo\n",
        "\n",
        "After simply logging the orignal dataset with the following columns:\n",
        "```id,text,label```\n",
        "\n",
        "We can hook the dataquality client in our model and dataloaders.\n",
        "\n",
        "```python\n",
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "dq.init(...)\n",
        "dq.log_dataset(...)\n",
        "dq.set_labels_for_run(...)\n",
        "watch(model, [train_dataloader, test_dataloader])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX8KsPk_m6Au"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "\n",
        "# üî≠üåï Initialize the project\n",
        "dq.init('text_multi_label',\n",
        "        \"multi_label_text_classification_pytorch\",\n",
        "        f\"run_{dataset_name}_04\"\n",
        "        )\n",
        "\n",
        "\n",
        "# üî≠üåï Galileo logging\n",
        "dq.set_tasks_for_run(labels_cols, binary=True)\n",
        "\n",
        "\n",
        "# üî≠üåï Galileo logging\n",
        "dq.log_data_samples(\n",
        "    texts=train_df[\"text\"],\n",
        "    task_labels=train_df[labels_cols].apply(lambda row: list(row[row == 1].index.values), axis=1),\n",
        "    ids=train_df[\"id\"],\n",
        "    split=\"train\",\n",
        ")\n",
        "\n",
        "# üî≠üåï Galileo logging\n",
        "dq.log_data_samples(\n",
        "    texts=test_df[\"text\"],\n",
        "    task_labels=test_df[labels_cols].apply(lambda row: list(row[row == 1].index.values), axis=1),\n",
        "    ids=test_df[\"id\"],\n",
        "    split=\"validation\",\n",
        ")\n",
        "\n",
        "# üî≠üåï Monitor the model with Galileo\n",
        "watch(model, [train_dataloader, val_dataloader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV9XnJaSg9j"
      },
      "source": [
        "# 4. Training\n",
        "\n",
        "After hooking into the model we log epochs and split during the training process.\n",
        "```python\n",
        "dq.set_split(\"train\")\n",
        "dq.set_epoch(epoch)\n",
        "```\n",
        "\n",
        "When training is complete\n",
        "```dq.finish()```\n",
        "must be called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    dq.set_epoch(epoch) # üî≠üåï Setting the epoch\n",
        "    dq.set_split(\"training\") # üî≠üåï Setting split to training\n",
        "    train_loop(model,train_dataloader,optimizer,loss_fn)\n",
        "    dq.set_split(\"validation\") # üî≠üåï Setting split to validation\n",
        "    test_loop(model, val_dataloader, loss_fn, epoch)\n",
        "print(\"Finished Training\")\n",
        "\n",
        "dq.finish() # üî≠üåï Finishing the run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUoYnK0oFK_"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9o7ZwZVZIiM"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mqfb_JFuEy"
      },
      "outputs": [],
      "source": [
        "# Get insights to the metrics\n",
        "dq.metrics.get_dataframe(\n",
        "        \"example_project\",\n",
        "        f\"run_{dataset_name}\",\n",
        "        \"train\").head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
