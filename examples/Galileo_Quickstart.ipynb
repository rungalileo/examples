{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Press á… to run me!\n",
        "\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    from IPython.display import Image\n",
        "    display(Image(url='https://galileo-public-tutorial-data.s3.us-west-1.amazonaws.com/use_gpus_colab.gif', width=700))\n",
        "    print(\"GPU not enabled! Please go to Runtime > Change runtime type > Hardware Accelerator > GPU\")\n",
        "    \n",
        "    # Restart the runtime\n",
        "    import os, time\n",
        "    time.sleep(1) # gives the print statements time to flush\n",
        "    os._exit(0) # exits without allowing the next cell to run\n",
        "\n",
        "print(\"Installing `dataquality` and other libraries. This should take ~30 seconds.\")\n",
        "\n",
        "\n",
        "!pip install ipywidgets==7.7.1 traitlets==5.1.1 &> /dev/null\n",
        "# Install dataquality without needing a restart of the kernel\n",
        "!pip install vaex vaex-core vaex-hdf5 wrapt==1.13.3 'responses<0.19' dataquality evaluate datasets transformers tokenizers huggingface-hub --no-deps &> /dev/null\n",
        "!pip install torchmetrics==0.10.0 diskcache==5.2.1 gorilla==0.3.0 resource==0.2.1 types-requests==2.25.2 multiprocess xxhash blake3==0.2.1 aplus 'frozendict!=2.2.0' 'nest-asyncio>=1.3.3' rich jedi cachetools==5.2.0 &> /dev/null\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "\"\"\"\n",
        "============================================================\n",
        "BEGIN MODEL TRAINING CODE [REMOVE ME IN THE FUTURE, and potentially replace pytorch code with HF for a lower amount of code overall needed]\n",
        "============================================================\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from tqdm.notebook import tqdm\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from typing import List\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "\n",
        "file_name = None\n",
        "\n",
        "def train_model():\n",
        "    transformers.logging.disable_progress_bar()\n",
        "    transformers.logging.set_verbosity_error()\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "    \n",
        "    global df\n",
        "\n",
        "    if \"split\" in df:\n",
        "      train_df = df[df[\"split\"]==\"train\"]\n",
        "      test_df = df[df[\"split\"]==\"val\"]\n",
        "    else:\n",
        "      train_df = df.sample(frac = 0.8)\n",
        "      test_df = df.drop(train_df.index)\n",
        "\n",
        "    # ðŸ”­ðŸŒ• Initializing a new run in Galileo. Each run is part of a project.\n",
        "    global file_name\n",
        "    dq.init(task_type=\"text_classification\", \n",
        "            project_name=\"galileo_in_5_minutes\", #TODO: fix this\n",
        "            run_name=f\"{file_name.replace('/', '-').replace('.', '_')}\")\n",
        "\n",
        "\n",
        "    import torch\n",
        "    class TextDataset(torch.utils.data.Dataset):\n",
        "        def __init__(\n",
        "            self, dataset: pd.DataFrame, split: str, list_of_labels: List[str] = None\n",
        "        ):\n",
        "            self.dataset = dataset\n",
        "\n",
        "            # ðŸ”­ðŸŒ• Logging the dataset with Galileo\n",
        "            # Note: this works seamlessly because self.dataset has text, label, and\n",
        "            # id columns. See `help(dq.log_dataset)` for more info\n",
        "            dq.log_dataset(self.dataset, split=split)\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "            self.encodings = tokenizer(\n",
        "                self.dataset[\"text\"].tolist(), truncation=True, padding=True\n",
        "            )\n",
        "\n",
        "            self.list_of_labels = list_of_labels or self.dataset[\"label\"].unique().tolist()\n",
        "\n",
        "            self.labels = np.array(\n",
        "                [self.list_of_labels.index(label) for label in self.dataset[\"label\"]]\n",
        "            )\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            x = torch.tensor(self.encodings[\"input_ids\"][idx])\n",
        "            attention_mask = torch.tensor(self.encodings[\"attention_mask\"][idx])\n",
        "            y = self.labels[idx]\n",
        "            sample_idx = self.dataset.id.iloc[idx]\n",
        "            return sample_idx, x, attention_mask, y\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset)\n",
        "\n",
        "    train_dataset = TextDataset(train_df, split=\"training\")\n",
        "    test_dataset = TextDataset(\n",
        "        test_df, \n",
        "        split=\"validation\",\n",
        "        list_of_labels=train_dataset.list_of_labels,\n",
        "    )\n",
        "\n",
        "    # ðŸ”­ðŸŒ• Registering the list of labels for the run\n",
        "    dq.set_labels_for_run(train_dataset.list_of_labels)\n",
        "\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "    from torch.nn import Linear\n",
        "    from transformers import AutoModel\n",
        "\n",
        "    class TextClassificationModel(torch.nn.Module):\n",
        "        \"\"\"Defines a Pytorch text classification bert based model.\"\"\"\n",
        "\n",
        "        def __init__(self, num_labels: int):\n",
        "            super().__init__()\n",
        "            self.feature_extractor = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "            self.classifier = Linear(self.feature_extractor.config.hidden_size, num_labels)\n",
        "\n",
        "        def forward(self, x, attention_mask, ids):\n",
        "            \"\"\"Model forward function.\"\"\"\n",
        "            encoded_layers = self.feature_extractor(\n",
        "                input_ids=x, attention_mask=attention_mask\n",
        "            ).last_hidden_state\n",
        "            classification_embedding = encoded_layers[:, 0]\n",
        "            logits = self.classifier(classification_embedding)\n",
        "\n",
        "            # ðŸ”­ðŸŒ• Logging the model logits and embeddings\n",
        "            dq.log_model_outputs(\n",
        "                embs=classification_embedding, logits=logits, ids=ids\n",
        "            )\n",
        "\n",
        "            return logits\n",
        "\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    MAX_NUM_EPOCHS = 100\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        shuffle=True,\n",
        "    )\n",
        "    val_dataloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    print(\"Initializing the DistilBERT model.\")\n",
        "\n",
        "    model = TextClassificationModel(num_labels=len(train_dataset.list_of_labels))\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5\n",
        "    )\n",
        "\n",
        "    train_acc = torchmetrics.Accuracy()\n",
        "    val_acc = torchmetrics.Accuracy()\n",
        "\n",
        "    fig = go.FigureWidget(data={\"y\":[0], \"x\":[0]})\n",
        "    # fig = go.FigureWidget()\n",
        "    # fig.add_scatter(x=[0], y=[0], name=\"train loss\")\n",
        "    # fig.add_scatter(x=[0], y=[0], name=\"val loss\")\n",
        "    display(fig)\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            'text': \"Train Loss\"\n",
        "        },\n",
        "        xaxis={\"title\": \"Step\"},\n",
        "        yaxis={\"title\": \"Loss\"}\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_axis = []\n",
        "\n",
        "    for epoch in range(MAX_NUM_EPOCHS):\n",
        "        dq.set_epoch(epoch) # ðŸ”­ðŸŒ• Setting the epoch\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        dq.set_split(\"training\") # ðŸ”­ðŸŒ• Setting split to training\n",
        "\n",
        "        for data in train_dataloader:\n",
        "            x_idxs, x, attention_mask, y = data\n",
        "            x = x.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            y = torch.tensor(y, device=device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            logits = model(x, attention_mask, x_idxs)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss = loss.item()\n",
        "            train_losses.append(train_loss)\n",
        "            # train_loss_fig = fig.data[0]\n",
        "            # train_loss_fig.x, train_loss_fig.y = list(range(len(train_losses))), train_losses\n",
        "            fig.update(data=[{\"y\":train_losses, \"x\": list(range(len(train_losses)))}])\n",
        "            fig.update_layout(\n",
        "                title={\n",
        "                    'text': \"Train Loss\"\n",
        "                },\n",
        "                xaxis={\"title\": \"Step\"},\n",
        "                yaxis={\"title\": \"Loss\"}\n",
        "            )\n",
        "            running_loss += train_loss\n",
        "            train_acc(torch.argmax(logits.to(\"cpu\"), 1), y.to(\"cpu\"))\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            dq.set_split(\"validation\") # ðŸ”­ðŸŒ• Setting split to validation\n",
        "\n",
        "            val_loss = 0.0\n",
        "            for data in val_dataloader:\n",
        "                x_idxs, x, attention_mask, y = data\n",
        "\n",
        "                x = x.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                y = torch.tensor(y, device=device)\n",
        "\n",
        "                logits = model(x, attention_mask, x_idxs)\n",
        "                loss = F.cross_entropy(logits, y)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_acc(torch.argmax(logits.to(\"cpu\"), 1), y.to(\"cpu\"))\n",
        "            \n",
        "\n",
        "\n",
        "            if epoch != 0:\n",
        "                # Check if we should early stop\n",
        "                if val_loss >= val_losses[-1]:\n",
        "                    break\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # val_losses.append(val_loss / ct)\n",
        "            # val_axis.append(len(train_losses))\n",
        "            # val_loss_fig = fig.data[1]\n",
        "            # val_loss_fig.x, val_loss_fig.y = val_axis, val_losses\n",
        "            # print(\"[epoch %d] Validation loss: %.3f\" % (epoch + 1, val_loss))\n",
        "            # print(f\"Val accuracy: {val_acc.compute()}\")\n",
        "\n",
        "    # output.disable_custom_widget_manager()\n",
        "    print(f\"Finished Training. Early Stopped at epoch {epoch}.\")\n",
        "\n",
        "    dq.finish() # ðŸ”­ðŸŒ• Complete the Galileo workflow with a call to dq.finish()\n",
        "\n",
        "\"\"\"\n",
        "============================================================\n",
        "END MODEL TRAINING CODE\n",
        "============================================================\n",
        "\"\"\"\n",
        "\n",
        "import ipywidgets.widgets as widgets\n",
        "from IPython.display import clear_output\n",
        "import io\n",
        "import pandas as pd\n",
        "import dataquality as dq\n",
        "from google.colab import output, files\n",
        "import time\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "from google.colab import data_table\n",
        "\n",
        "\n",
        "GALILEO_HTML_EXAMPLE = \"\"\"<center> <img\n",
        "src=https://galileo-public-tutorial-data.s3.us-west-1.amazonaws.com/galileo_quickstart.svg alt='GalileoRing' width=600> <br> Choose an example dataset to get started.</center>\"\"\"\n",
        "GALILEO_HTML_FILE = \"\"\"<center>or:<br> Upload a .csv file with 2 columns: \"text\" and \"label\"</center>\"\"\"\n",
        "GALILEO_HTML_TOKEN = \"\"\"<center> <img\n",
        "src=https://galileo-public-tutorial-data.s3.us-west-1.amazonaws.com/Logo.svg alt='GalileoLogo' width=200> <br> Now, copy the token from your <a\n",
        "href=\"https://console.cloud.rungalileo.io/get-token\" target=\"_blank\">Galileo Console account</a> and paste it below:</center>\"\"\"\n",
        "GALILEO_HTML_END = \"\"\"<center>\n",
        "<b>Tip: </b>If you don't have an account with Galileo, you can create one <a href=\"https://console.cloud.rungalileo.io/sign-up?utm=galileo-in-5-min\" target=\"_blank\">here</a></center>\"\"\"\n",
        "\n",
        "box_layout = widgets.Layout(\n",
        "    display=\"flex\", flex_flow=\"column\", align_items=\"center\", width=\"85%\"\n",
        ")\n",
        "\n",
        "\n",
        "newsgroups = widgets.Button(description=\"newsgroups\")\n",
        "trec6 = widgets.Button(description=\"trec6\")\n",
        "conv_intent = widgets.Button(description=\"conv_intent\")\n",
        "token_widget = widgets.Password(description=\"\")\n",
        "start_button = widgets.Button(description=\"Start!\", icon=\"fa-rocket\")\n",
        "login_widget = widgets.VBox(\n",
        "    [\n",
        "        widgets.HTML(GALILEO_HTML_TOKEN),\n",
        "        token_widget,\n",
        "        start_button,\n",
        "        widgets.HTML(GALILEO_HTML_END),\n",
        "    ],\n",
        "    layout=box_layout\n",
        ")\n",
        "\n",
        "# upload_widget = widgets.FileUpload(accept='.csv', multiple=True)\n",
        "upload_widget = widgets.Button(description='Upload!')\n",
        "starter = widgets.VBox(\n",
        "    [\n",
        "        widgets.HTML(GALILEO_HTML_EXAMPLE),\n",
        "        widgets.HBox([trec6, conv_intent, newsgroups]),\n",
        "        widgets.HTML(GALILEO_HTML_FILE), \n",
        "        upload_widget\n",
        "     ],\n",
        "        layout=box_layout\n",
        ")\n",
        "\n",
        "def restart(t):\n",
        "    clear_output()\n",
        "    newsgroups.disabled = False\n",
        "    trec6.disabled = False\n",
        "    conv_intent.disabled = False\n",
        "    upload_widget.disabled = True\n",
        "    display(starter)\n",
        "\n",
        "def submit_event(t):\n",
        "    token = token_widget.value\n",
        "    # Erase token\n",
        "    token_widget.value = \"\"\n",
        "    clear_output()\n",
        "    dq.config.token = token\n",
        "    if not dq.clients.api.ApiClient().valid_current_user():\n",
        "      print(\n",
        "          \"It seems as though the token you provided is not valid. Please try again\"\n",
        "      )\n",
        "      display(login_widget)\n",
        "      return\n",
        "\n",
        "    email = dq.clients.api.ApiClient().get_current_user()[\"email\"]\n",
        "    dq.config.current_user = email\n",
        "    dq.login()\n",
        "\n",
        "    import warnings\n",
        "    with warnings.catch_warnings():\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      train_model()\n",
        "    print(\"Start Over?\")\n",
        "    start_over_button = widgets.Button(description=\"Again!\", icon=\"fa-rocket\")\n",
        "    start_over_button.on_click(restart)\n",
        "    display(start_over_button)\n",
        "\n",
        "\n",
        "start_button.on_click(submit_event)\n",
        "\n",
        "display(starter)\n",
        "\n",
        "df = None\n",
        "\n",
        "def example_button_click(button):\n",
        "    newsgroups.disabled = True\n",
        "    trec6.disabled = True\n",
        "    conv_intent.disabled = True   \n",
        "    # upload_widget.disabled = True\n",
        "    datasets.logging.set_verbosity_error()\n",
        "    datasets.logging.disable_progress_bar()\n",
        "    global file_name\n",
        "    file_name = f\"rungalileo_dataset_{button.description}\"\n",
        "    ds = load_dataset(f\"rungalileo/{button.description}\")\n",
        "    train_df = ds[\"train\"].to_pandas()\n",
        "    labels = ds[\"train\"].features[\"label\"].names\n",
        "    train_df[\"label\"] = train_df.label.apply(lambda row: labels[row])\n",
        "    train_df[\"split\"] = [\"train\"]*len(train_df)\n",
        "\n",
        "    val_df = ds[\"validation\"].to_pandas()\n",
        "    val_df[\"label\"] = val_df.label.apply(lambda row: labels[row])\n",
        "    val_df[\"split\"] = [\"val\"]*len(val_df)\n",
        "    global df\n",
        "    df = pd.concat([train_df, val_df])\n",
        "    clear_output()\n",
        "    # display(pd.concat([df[:5], df[-5:]]))\n",
        "    # display(df)\n",
        "    display(data_table.DataTable(df, include_index=False, num_rows_per_page=10))\n",
        "\n",
        "    display(login_widget)\n",
        "\n",
        "# def on_upload(inputs):\n",
        "#     global file_name\n",
        "#     file_name = list(inputs['new'].keys())[-1]\n",
        "#     uploaded_file = list(inputs['new'].values())[-1][\"content\"]\n",
        "#     global df\n",
        "#     df = pd.read_csv(io.BytesIO(uploaded_file))\n",
        "#     df[\"id\"] = list(range(len(df)))\n",
        "def get_file(d):\n",
        "    uploaded = files.upload()\n",
        "    global file_name\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    global df\n",
        "    df = pd.read_csv(file_name)\n",
        "    df[\"id\"] = list(range(len(df)))\n",
        "\n",
        "    if not (\"label\" in df.columns and \"text\" in df.columns):\n",
        "      clear_output()\n",
        "      display(starter)\n",
        "      print(\"CSV must have 'label' and 'text' columns. Please fix and try uploading again!\")\n",
        "      return\n",
        "    clear_output()\n",
        "    \n",
        "    display(file_name)\n",
        "    display(data_table.DataTable(df, include_index=False, num_rows_per_page=10))\n",
        "    display(login_widget)\n",
        "\n",
        "# upload_widget.observe(on_upload, names='value')\n",
        "upload_widget.on_click(get_file)\n",
        "newsgroups.on_click(example_button_click)\n",
        "trec6.on_click(example_button_click)\n",
        "conv_intent.on_click(example_button_click)\n"
      ],
      "metadata": {
        "id": "9eAaVZLrMjxt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " â€‹"
      ],
      "metadata": {
        "id": "UJoUrDQgtyvk"
      }
    }
  ]
}
