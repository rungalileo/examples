{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5JVjGAAJOTn"
      },
      "source": [
        "# Text Classification using PyTorch and ðŸ”­ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UfMcPbg19uz1"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality` with `pip install dataquality`\n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip and install dependencies\n",
        "    !pip install -U pip &> /dev/null\n",
        "    !pip install dataquality transformers datasets torchmetrics==0.10.0 --upgrade 1> /dev/null\n",
        "\n",
        "    print('ðŸ‘‹ Installed necessary libraries.')\n",
        "    print('ðŸ™ Continue with the rest of the notebook or hit \"Run All\" again!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKPfMhYvgeL"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C7B55N_B30kG"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ¤— HuggingFace Dataset\n",
        "#@markdown You can find more datasets [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:text-classification&task_ids=task_ids:multi-class-classification&sort=downloads).\n",
        "\n",
        "dataset_name = \"generalization/newsgroups_Full-p_1\" #@param [\"generalization/newsgroups_Full-p_1\", \"banking77\", \"emotion\", \"rungalileo/conv_intent\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"ðŸ–– Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config)\n",
        "\n",
        "# Check that the dataset has at least train and either of validation/test\n",
        "assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n",
        "f\"ðŸ’¾ The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n",
        "\n",
        "print(f\"\\nðŸ† Dataset {dataset_name} loaded succesfully\")\n",
        "# A small function for minimizing the dataset for testing purposes\n",
        "import os\n",
        "\n",
        "import os\n",
        "\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\"\n",
        "\n",
        "if _minimize_for_ci():\n",
        "  # This is for github testing on cpu.\n",
        "  data[\"train\"] = data[\"train\"].select(range(10))\n",
        "  data[\"test\"] = data[\"test\"].select(range(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "nTrPjUO1FuEu"
      },
      "outputs": [],
      "source": [
        "#@title 2. Preprocess dataset and prepare training\n",
        "#@markdown Most PyTorch training is done with the Dataloader.\n",
        "#@markdown After preprocessing the data (tokenizing) we pass it on to it.\n",
        "#@markdown Galileo relies on PyTorch's DataLoader.\n",
        "#@markdown\n",
        "#@markdown ```python\n",
        "#@markdown train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "#@markdown test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "#@markdown ```\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from transformers import AutoModel\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess(row):\n",
        "  return tokenizer(row[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "encoded_data = data.map(preprocess, batched=True)\n",
        "dataloader_columns = [\"input_ids\",\"attention_mask\",\"label\"]\n",
        "train_ds = encoded_data[\"train\"].with_format(\"torch\", dataloader_columns)\n",
        "test_ds = encoded_data[\"test\"].with_format(\"torch\", dataloader_columns)\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "class TextClassificationModel(torch.nn.Module):\n",
        "    \"\"\"Defines a Pytorch text classification bert based model.\"\"\"\n",
        "\n",
        "    def __init__(self, num_labels: int):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.classifier = Linear(self.feature_extractor.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"Model forward function.\"\"\"\n",
        "        encoded_layers = self.feature_extractor(\n",
        "            input_ids=input_ids, attention_mask=attention_mask\n",
        "        ).last_hidden_state\n",
        "        classification_embedding = encoded_layers[:, 0]\n",
        "        return self.classifier(classification_embedding)\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "model = TextClassificationModel(len(data['train'].features[\"label\"].names))\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5\n",
        ")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "num_classes = len(data['train'].features[\"label\"].names)\n",
        "val_acc = torchmetrics.Accuracy().to(device)\n",
        "train_acc = torchmetrics.Accuracy().to(device)\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def extract_batch(batch):\n",
        "  x = batch.pop(\"input_ids\").to(device)\n",
        "  y = batch.pop(\"label\").to(device)\n",
        "  attention_mask = batch.pop(\"attention_mask\").to(device)\n",
        "  return x, y, attention_mask\n",
        "\n",
        "def train_loop(model,dataloader,optimizer,loss_fn):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for batch in tqdm(dataloader):\n",
        "      # print statistics\n",
        "      x, y, attention_mask = extract_batch(batch)\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward + backward + optimize\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_acc(logits.argmax(1), y)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "def test_loop(model, dataloader, loss_fn, epoch):\n",
        "  model.eval()\n",
        "  validation_loss = 0.0\n",
        "  pbar = tqdm(dataloader)\n",
        "  for batch in pbar:\n",
        "    x, y, attention_mask = extract_batch(batch)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits, y)\n",
        "    val_acc(logits.argmax(1), y)\n",
        "    validation_loss += loss.item()\n",
        "    pbar.set_description(\"[epoch %d] Validation loss: %.3f\" % (epoch + 1, validation_loss))\n",
        "  pbar.set_description(f\"Val accuracy: {val_acc.compute()}\")\n",
        "\n",
        "print(\"Preview of dataset:\")\n",
        "example_df = data[\"train\"].to_pandas().head()\n",
        "example_df[\"label\"] = example_df[\"label\"].map({\n",
        "    i:label\n",
        "    for i,label in enumerate(data[\"train\"].features[\"label\"].names)\n",
        "})\n",
        "example_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6zJHmvxTJ2pN"
      },
      "source": [
        "## 3. Monitor with Galileo\n",
        "\n",
        "After simply logging the orignal dataset with the following columns:\n",
        "```id,text,label```\n",
        "\n",
        "We can hook the dataquality client in our model and dataloaders.\n",
        "\n",
        "```python\n",
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "dq.init(...)\n",
        "dq.log_dataset(...)\n",
        "dq.set_labels_for_run(...)\n",
        "watch(model, [train_dataloader, test_dataloader])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX8KsPk_m6Au"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "# ðŸ”­ðŸŒ• Initialize the project\n",
        "dq.init('text_classification',\n",
        "        \"text-classification-demo\",\n",
        "        f\"run_{dataset_name.replace('/', '-')}\"\n",
        "        )\n",
        "\n",
        "# Add the indices to the dataset\n",
        "# so we can log the following columns: id,text,label\n",
        "train_ds_with_ids = data[\"train\"].map(lambda x,idx: {\"id\": idx}, with_indices=True)\n",
        "test_ds_with_ids = data[\"test\"].map(lambda x,idx: {\"id\": idx}, with_indices=True)\n",
        "\n",
        "# ðŸ”­ðŸŒ• Logging the dataset with Galileo\n",
        "dq.log_dataset(train_ds_with_ids,\n",
        "               split=\"train\")\n",
        "\n",
        "# ðŸ”­ðŸŒ• Logging the dataset with Galileo\n",
        "dq.log_dataset(test_ds_with_ids,\n",
        "               split=\"validation\")\n",
        "\n",
        "# ðŸ”­ðŸŒ• Logging the labels in order for Galileo\n",
        "dq.set_labels_for_run(data[\"train\"].features[\"label\"].names)\n",
        "\n",
        "# ðŸ”­ðŸŒ• Monitor the model with Galileo\n",
        "watch(model, [train_dataloader, test_dataloader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV9XnJaSg9j"
      },
      "source": [
        "# 4. Training\n",
        "\n",
        "After hooking into the model we log epochs and split during the training process.\n",
        "```python\n",
        "dq.set_split(\"train\")\n",
        "dq.set_epoch(epoch)\n",
        "```\n",
        "\n",
        "When training is complete\n",
        "```dq.finish()```\n",
        "must be called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    dq.set_epoch(epoch) # ðŸ”­ðŸŒ• Setting the epoch\n",
        "    dq.set_split(\"training\") # ðŸ”­ðŸŒ• Setting split to training\n",
        "    train_loop(model,train_dataloader,optimizer,loss_fn)\n",
        "    dq.set_split(\"validation\") # ðŸ”­ðŸŒ• Setting split to validation\n",
        "    test_loop(model, test_dataloader, loss_fn, epoch)\n",
        "print(\"Finished Training\")\n",
        "\n",
        "dq.finish() # ðŸ”­ðŸŒ• Finishing the run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUoYnK0oFK_"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9o7ZwZVZIiM"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mqfb_JFuEy"
      },
      "outputs": [],
      "source": [
        "# Get insights to the metrics\n",
        "dq.metrics.get_dataframe(\n",
        "        \"text-classification-demo\",\n",
        "        f\"run_{dataset_name.replace('/', '-')}\",\n",
        "        \"train\").head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Oct 13 2022, 16:12:30) \n[Clang 12.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
