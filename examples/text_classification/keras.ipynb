{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1IQ99tSgicpVpCrkhjTp-Dp_f2ucBhvy3",
          "timestamp": 1659469611341
        }
      ],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification using Keras and üî≠ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with Tensorflow and explore the results in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ],
      "metadata": {
        "id": "j5JVjGAAJOTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfMcPbg19uz1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality`\n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip\n",
        "    !pip install -U pip &> /dev/null\n",
        "\n",
        "    # Install HF datasets for downloading the example datasets\n",
        "    !pip install -U dataquality datasets transformers &> /dev/null\n",
        "    \n",
        "    print('üëã Installed necessary libraries and restarting runtime! This should only need to happen once.')\n",
        "    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n",
        "\n",
        "    # Restart the runtime\n",
        "    import os, time\n",
        "    time.sleep(1) # gives the print statements time to flush\n",
        "    os._exit(0) # exits without allowing the next cell to run"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Login to Galileo"
      ],
      "metadata": {
        "id": "9QyHXYMZJw3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dataquality as dq\n",
        "\n",
        "dq.login()"
      ],
      "metadata": {
        "id": "pO6DdYFob5UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load Data"
      ],
      "metadata": {
        "id": "dOKPfMhYvgeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ü§ó HuggingFace Dataset\n",
        "#@markdown You can find more datasets [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:text-classification&task_ids=task_ids:multi-class-classification&sort=downloads).\n",
        "\n",
        "dataset_name = 'banking77' #@param [\"banking77\", \"emotion\", \"tweet_eval\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config)\n",
        "\n",
        "# Check that the dataset has at least train and either of validation/test\n",
        "assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n",
        "f\"üíæ The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n",
        "\n",
        "print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")"
      ],
      "metadata": {
        "id": "u3yKR4e9d3xa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Convert HF dataset to Pandas dataframes \n",
        "import pandas as pd\n",
        "\n",
        "def load_pandas_df(data):\n",
        "  # Find the name of the ground truth column\n",
        "  good_col_names = [name for name in list(data['train'].features) if \"label\" in name]\n",
        "  if len(good_col_names) == 1:\n",
        "    label_col = good_col_names[0]\n",
        "  else:\n",
        "    col_names = list(data['train'].features)\n",
        "    print(f\"The name of the columns are {col_names}.\")\n",
        "    label_col = input(f\"üèÖ Please enter the name of the column containing the labels: \")\n",
        "    assert label_col in col_names, f\"{label_col} is not an existing column\"\n",
        "\n",
        "  # Load the labels in a dictionary\n",
        "  labels = data['train'].features[label_col].names\n",
        "  labels = {v:k for v, k in enumerate(labels)}\n",
        "\n",
        "  # Load the train data into a frame\n",
        "  train_data = data[\"train\"]\n",
        "  train_df = pd.DataFrame.from_dict(train_data)\n",
        "  train_df['label'] = train_df[label_col].map(labels)\n",
        "  train_df['id'] = train_df.index\n",
        "\n",
        "  # Load the test data into a frame\n",
        "  test_split_name = \"validation\" if \"validation\" in data else \"test\"\n",
        "  test_data = data[test_split_name]\n",
        "  test_df = pd.DataFrame.from_dict(test_data)\n",
        "  test_df['label'] = test_df[label_col].map(labels)\n",
        "  test_df['id'] = test_df.index\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_pandas_df(data)\n",
        "labels = train_df.label.unique().tolist()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4VrA5uAaRvJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Initialize Galileo"
      ],
      "metadata": {
        "id": "YaoKpP9YR0UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üî≠üåï Galileo logging\n",
        "dq.init(task_type=\"text_classification\", \n",
        "        project_name=\"text_classification_keras\", \n",
        "        run_name=f\"example_run_{dataset_name.replace('/','-')}\")"
      ],
      "metadata": {
        "id": "yT7HC1xdRnqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Log input data with Galileo\n",
        "Input data can be logged via `log_data_samples` (or `log_dataset` for logging iterables). This step will log input samples, gold labels, data split, and list of all labels. You can achieve this by adding 1 line of code to the standard PyTorch Dataset Class."
      ],
      "metadata": {
        "id": "6zJHmvxTJ2pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# üî≠üåï Galileo logging\n",
        "dq.log_dataset(train_df, split=\"training\")\n",
        "dq.log_dataset(test_df, split=\"test\")\n",
        "dq.set_labels_for_run(labels)\n",
        "\n",
        "# Tokenize inputs and get attention mask\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_df, test_df = train_df.dropna(), test_df.dropna()\n",
        "\n",
        "# Train and test datasets\n",
        "datasets = []\n",
        "for df in [train_df, test_df]:\n",
        "  inputs = tokenizer(df.text.to_list(), truncation=True, padding=True)\n",
        "  # Get our label index for model training\n",
        "  inputs[\"label\"] = [labels.index(label) for label in df.label]\n",
        "  inputs[\"uuid\"] = df[\"id\"]\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(dict(inputs)).batch(BATCH_SIZE)\n",
        "  datasets.append(dataset)\n",
        "\n",
        "train_ds, test_ds = datasets"
      ],
      "metadata": {
        "id": "FX8KsPk_m6Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Log Keras model data with Galileo\n",
        "\n",
        "Model data can be logged via a `DataQualityLoggingLayer`. This step will log the model probabilitiy and embeddings. You can achieve this by adding the following line of code to Keras sequential nodel. "
      ],
      "metadata": {
        "id": "nMV9XnJaSg9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataquality.integrations.keras import DataQualityLoggingLayer\n",
        "from transformers import TFBertModel\n",
        "from tensorflow import keras\n",
        "\n",
        "def build_model(max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attn_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
        "    DataQualityLoggingLayer(\"ids\"), # üåïüî≠ Galileo - right after the input layer, separates ids from your regular inputs\n",
        "    bert = TFBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    bert_outputs = bert([input_word_ids, attn_mask])\n",
        "    last_hidden_states = bert_outputs.last_hidden_state\n",
        "    clf_output = last_hidden_states[:, 0, :] # CLS token \n",
        "    DataQualityLoggingLayer(\"embs\") # üåïüî≠ Galileo - place this after the layer you want embeddings to be logged from\n",
        "    outputs = tf.keras.layers.Dense(len(labels), activation='softmax')(clf_output)\n",
        "    DataQualityLoggingLayer(\"probs\") # üåïüî≠ Galileo - place this after the layer you want probabilities to be logged from\n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, attn_mask], outputs=)\n",
        "    model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=\"adam\", \n",
        "              metrics=[\"accuracy\"], \n",
        "              run_eagerly=True) # üåïüî≠ Galileo - set run_eagerly to True!!\n",
        "    return model "
      ],
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Putting into Action: Training a Model\n",
        "\n",
        "We complete the training pipeline by using a standard PyTorch training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."
      ],
      "metadata": {
        "id": "PnEeJN1IyMl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataquality.integrations.keras import DataQualityCallback\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "model.fit(train_ds,\n",
        "          batch_size=batch_size, \n",
        "          epochs=epochs, \n",
        "          validation_data=[test_ds], \n",
        "          callbacks=[DataQualityCallback()]) # üåïüî≠ Galileo\n",
        "\n",
        "dq.finish() # üåïüî≠ Galileo"
      ],
      "metadata": {
        "id": "-lRIh9U7oL2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ],
      "metadata": {
        "id": "QkUoYnK0oFK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ],
      "metadata": {
        "id": "V9o7ZwZVZIiM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
