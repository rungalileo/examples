{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fccf6556",
   "metadata": {
    "id": "fccf6556"
   },
   "source": [
    "# Image Classification using PyTorch and 🔭 Galileo\n",
    "\n",
    "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
    "\n",
    "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vthATGXfFAup",
   "metadata": {
    "cellView": "form",
    "id": "vthATGXfFAup"
   },
   "outputs": [],
   "source": [
    "#@title Install `dataquality`\n",
    "# Upgrade pip\n",
    "!pip install -U pip &> /dev/null\n",
    "\n",
    "# Install all dependecies\n",
    "!pip install -U dataquality matplotlib==3.1.3 torch torchmetrics==0.10.0 datasets &> /dev/null\n",
    "\n",
    "print('👋 Installed necessary libraries!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zBGmxapS_S5w",
   "metadata": {
    "cellView": "form",
    "id": "zBGmxapS_S5w"
   },
   "outputs": [],
   "source": [
    "#@markdown Check that a GPU is available\n",
    "\n",
    "import torch\n",
    "# Check Cuda.\n",
    "if torch.cuda.is_available():\n",
    "  print(\"⚡ You are connected to a GPU!\")\n",
    "else:\n",
    "  print(\"❗You are NOT connected to a GPU ❗It is recommended to connect to a GPU before training\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "Skd8uU7HFKwi",
   "metadata": {
    "id": "Skd8uU7HFKwi"
   },
   "source": [
    "# 1. Login to Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XNuCGL_eFFJl",
   "metadata": {
    "id": "XNuCGL_eFFJl"
   },
   "outputs": [],
   "source": [
    "import dataquality as dq\n",
    "\n",
    "dq.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5yL5xeoZFPk_",
   "metadata": {
    "id": "5yL5xeoZFPk_"
   },
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pTOMOlv4FFGB",
   "metadata": {
    "cellView": "form",
    "id": "pTOMOlv4FFGB"
   },
   "outputs": [],
   "source": [
    "#@title Load a 🤗 HuggingFace Dataset\n",
    "#@markdown You can find more datasets [here](https://huggingface.co/datasets?task_categories=task_categories:image-classification&sort=downloads).\n",
    "\n",
    "dataset_name = \"CVdatasets/food27\" #@param [\"mnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\", \"Maysee/tiny-imagenet\", \"frgfm/imagenette\"] {allow-input: true}\n",
    "print(f\"You selected the {dataset_name} dataset\")\n",
    "\n",
    "from IPython.utils import io\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "\n",
    "# Try to load the data. If a config (subset) is needed, pick one\n",
    "try:\n",
    "  with io.capture_output() as captured:\n",
    "    data = load_dataset(dataset_name, trust_remote_code=True)\n",
    "except ValueError as e:\n",
    "  if \"Config name is missing\" not in repr(e):\n",
    "    raise e\n",
    "\n",
    "  configs = get_dataset_config_names(dataset_name)\n",
    "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
    "  config = input(f\"🖖 Enter the name of the subset to pick (or leave blank for any): \")\n",
    "  if config:\n",
    "    assert config in configs, f\"{config} is not a valid subset\"\n",
    "  else:\n",
    "    config = configs[0]\n",
    "  with io.capture_output() as captured:\n",
    "    data = load_dataset(dataset_name, name=config, trust_remote_code=True)\n",
    "\n",
    "# Check that the dataset has at least train and either of validation/test\n",
    "assert \"train\" in data and len({\"validation\", \"valid\", \"test\"}.intersection(data)), \\\n",
    "f\"💾 The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n",
    "test_split_name = list({\"validation\", \"valid\", \"test\"}.intersection(data))[0]\n",
    "if test_split_name == 'valid':\n",
    "  data['validation'] = data['valid']\n",
    "  test_split_name = 'validation'\n",
    "\n",
    "print(f\"\\n🏆 Dataset {dataset_name} loaded succesfully\")\n",
    "\n",
    "# Select a small portion of the dataset for CI.\n",
    "import os\n",
    "def _minimize_for_ci() -> bool:\n",
    "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\"\n",
    "\n",
    "if _minimize_for_ci():\n",
    "  from datasets.features.features import ClassLabel\n",
    "\n",
    "  # Find the name of the ground truth column\n",
    "  good_col_names = [name for name in list(data['train'].features) if \"label\" in name]\n",
    "  if len(good_col_names) == 1:\n",
    "      label_col = good_col_names[0]\n",
    "  else:\n",
    "    col_names = list(data['train'].features)\n",
    "    print(f\"The name of the columns are {col_names}.\")\n",
    "    label_col = input(f\"🏅 Please enter the name of the column containing the labels: \")\n",
    "    assert label_col in col_names, f\"{label_col} is not an existing column\"\n",
    "\n",
    "  # Create a tiny dataset with only label 0 as ground truth and 10 samples.\n",
    "  data['train'] = data['train'].filter(lambda example: example[label_col] == 0).select(range(100))\n",
    "  data['train'].features[label_col] = ClassLabel(names = [data['train'].features[label_col].names[0]])\n",
    "\n",
    "  data[test_split_name] = data[test_split_name].filter(lambda example: example[label_col] == 0).select(range(100))\n",
    "  data[test_split_name].features[label_col] = ClassLabel(names = [data[test_split_name].features[label_col].names[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "Al89xQQ_F9rv",
   "metadata": {
    "id": "Al89xQQ_F9rv"
   },
   "source": [
    "# 3. Initialize Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oXua2j7QFFBJ",
   "metadata": {
    "id": "oXua2j7QFFBJ"
   },
   "outputs": [],
   "source": [
    "# 🔭🌕 Initializing a new run in Galileo. Each run is part of a project.\n",
    "dq.init(task_type=\"image_classification\", \n",
    "        project_name=\"image-classification-demo\", \n",
    "        run_name=f\"example_run_{dataset_name.replace('/', '-')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "etbtUHJyGwyD",
   "metadata": {
    "id": "etbtUHJyGwyD"
   },
   "source": [
    "# 4. Create Dataset and Log Input Data with Galileo\n",
    "\n",
    "Input data can be logged via `log_image_dataset`. This step will log the images, gold labels, data split, and list of all labels. You can achieve this adding 1 line of code to the standard PyTorch Dataset Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DOEsmOshGHxD",
   "metadata": {
    "cellView": "form",
    "id": "DOEsmOshGHxD"
   },
   "outputs": [],
   "source": [
    "#@markdown Fix a random Seed and load helper methods.\n",
    "from typing import Optional, List\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Fix a random seed.\n",
    "def seed_all(seed: int) -> None:\n",
    "    \"\"\"Set all relevant seed for training a Pytorch Model.\n",
    "\n",
    "    Based on the following post:\n",
    "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def seed_worker(worker_id: int) -> None:\n",
    "    \"\"\"Set seed for dataloader worker.\n",
    "\n",
    "    Based on the following post:\n",
    "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Methods for converting between bytes, numpy, image.\n",
    "def _bytes_to_img(b: bytes) -> Image:\n",
    "    return Image.open(BytesIO(b))\n",
    "\n",
    "def _bytes_to_np(b: bytes, dtype: str = \"float\") -> np.ndarray:    \n",
    "    array = np.frombuffer(b, dtype=np.uint8)\n",
    "    if dtype == \"uint8\":\n",
    "      return array\n",
    "    elif dtype == \"float\":\n",
    "      return array / 255\n",
    "\n",
    "# Methods for loading the df into a dataset.\n",
    "def find_label_col_name(col_names: List[str]) -> Optional[str]:\n",
    "    for col_name in col_names:\n",
    "        if \"label\" in col_name:\n",
    "            return col_name\n",
    "    return None\n",
    "\n",
    "def find_image_col_name(col_names: List[str]) -> Optional[str]:\n",
    "    for col_name in col_names:\n",
    "        if \"img\" in col_name or \"image\" in col_name:\n",
    "            return col_name\n",
    "    return None\n",
    "\n",
    "def find_imgs_location_col_name(col_names: List[str]) -> Optional[str]:\n",
    "    for col_name in col_names:\n",
    "        if \"path\" in col_name:\n",
    "            return col_name\n",
    "    return None\n",
    "\n",
    "def find_raw_image_col_name(col_names: List[str]) -> Optional[str]:\n",
    "  for name in col_names:\n",
    "    if any([ n in name for n in [\"img\", \"image\"]]):\n",
    "      return name\n",
    "  return None\n",
    "\n",
    "def _write_to_disk(x, path) -> str:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    Image.open(BytesIO(x[\"bytes\"])).save(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eViQAp_AGD6j",
   "metadata": {
    "cellView": "form",
    "id": "eViQAp_AGD6j"
   },
   "outputs": [],
   "source": [
    "#@markdown 🔭🌕 Galileo -- Log Input Data\n",
    "from uuid import uuid4\n",
    "\n",
    "from datasets import Image as datasetsImage\n",
    "from datasets import DatasetDict\n",
    "from datasets.features.features import ClassLabel\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "STANDARD_DATA_COLUMNS_CV = [\"id\", \"text\", \"label_idx\", \"path\"]\n",
    "\n",
    "class ImageDataset(TorchDataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hf: DatasetDict,\n",
    "        split: str,\n",
    "        transform: transforms.Compose = None, \n",
    "        class_labels: ClassLabel = None\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hf: a HuggingFace dataset\n",
    "          split: the split for the hf dataset\n",
    "          transform [Optional]: a transform to apply to the images dynamically \n",
    "            before training\n",
    "          class_labels [Optional]: the ClassLabel object containing the list of \n",
    "            labels and the method to convert between label (string) and \n",
    "            label_idx (int). To insure consistency pass the class_labels of the\n",
    "            training dataset to the test/val datasets.\n",
    "        \"\"\"\n",
    "        hf = hf[split] \n",
    "        self.imgs_dir = os.path.dirname(hf.cache_files[0][\"filename\"])\n",
    "        self.transform = transform\n",
    "\n",
    "        # Find the column containing the images' paths. If not specify, save them to disk.\n",
    "        self.raw_img_location_colname = find_raw_image_col_name(hf.column_names)\n",
    "        if self.raw_img_location_colname is None:\n",
    "          raise ValueError(\"Could not find the images location column in the dataframe\")\n",
    "        STANDARD_DATA_COLUMNS_CV.append(self.raw_img_location_colname)\n",
    "\n",
    "        # Convert to pandas df.\n",
    "        hf = hf.cast_column(self.raw_img_location_colname, datasetsImage(decode=False))\n",
    "        self.ds = hf.to_pandas()\n",
    "        self.ds[\"text\"] = self.ds[self.raw_img_location_colname].apply(lambda x: _write_to_disk(x, f\"{self.imgs_dir}/{uuid4()}.png\"))\n",
    "\n",
    "        # Find the label column name: could be label, labels, coarse_label, etc.\n",
    "        self.label_col_name = find_label_col_name(self.ds.columns)\n",
    "        if self.label_col_name is None:\n",
    "            raise ValueError(f\"Could not find the label column in the dataframe\")\n",
    "        STANDARD_DATA_COLUMNS_CV.append(self.label_col_name)\n",
    "\n",
    "        # Set the list of labels for this split.\n",
    "        self.class_labels = class_labels\n",
    "        if self.class_labels is None:\n",
    "            self.class_labels = hf.features[self.label_col_name]\n",
    "        self.list_of_labels = self.class_labels.names\n",
    "        if split == \"train\":\n",
    "          dq.set_labels_for_run(self.list_of_labels)\n",
    "\n",
    "        # Add column with labels as string (for dq).\n",
    "        self.ds[\"label_idx\"] = self.ds[self.label_col_name]\n",
    "        labels_int2str = self.class_labels.int2str\n",
    "        self.ds[self.label_col_name] = self.ds[\"label_idx\"].map(labels_int2str)\n",
    "\n",
    "        # Find the id column, or create it if it doesn't exist.\n",
    "        if \"id\" not in self.ds.columns:\n",
    "            self.ds = self.ds.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "        # Get the metadata columns.\n",
    "        meta_data_cols = [\n",
    "            column\n",
    "            for column in self.ds.columns\n",
    "            if column not in STANDARD_DATA_COLUMNS_CV\n",
    "        ]\n",
    "\n",
    "        # 🔭🌕 Galileo logging -- Log Input Data\n",
    "        dq.log_image_dataset(\n",
    "            dataset=self.ds,\n",
    "            label=self.label_col_name,\n",
    "            split=split,\n",
    "            meta=meta_data_cols,\n",
    "            imgs_local_colname=\"text\",\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.ds.loc[idx]\n",
    "        image = Image.open(row[\"text\"]).convert('RGB')\n",
    "        label, id = row[\"label_idx\"], row[\"id\"]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"label\": label, \"id\": id}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ADbbMqzxnGjt",
   "metadata": {
    "cellView": "form",
    "id": "ADbbMqzxnGjt"
   },
   "outputs": [],
   "source": [
    "#@markdown Create the Dataset and DataLoader\n",
    "\n",
    "# Create the Datasets.\n",
    "image_crop_size = (224, 224)\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_crop_size[0], image_crop_size[1])),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "train_transforms = transforms.Compose(val_transforms.transforms + [transforms.RandomHorizontalFlip()])\n",
    "\n",
    "TRAIN_SPLIT_NAME = \"train\"\n",
    "train_dataset = ImageDataset(data, split=TRAIN_SPLIT_NAME, transform=train_transforms)\n",
    "VAL_SPLIT_NAME = test_split_name # this var is needed in dq.set_split down below\n",
    "test_dataset = ImageDataset(data, split=VAL_SPLIT_NAME, transform=val_transforms, class_labels=train_dataset.class_labels)\n",
    "\n",
    "print(f\"Loaded {TRAIN_SPLIT_NAME} dataset with {len(train_dataset.ds)} samples and {len(train_dataset.list_of_labels)} labels\")\n",
    "print(f\"Loaded {VAL_SPLIT_NAME} dataset with {len(test_dataset.ds)} samples and  {len(test_dataset.list_of_labels)} labels\")\n",
    "\n",
    "\n",
    "# Create the DataLoaders.\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "SEED_WORKER = 42\n",
    "\n",
    "seed_all(SEED_WORKER)\n",
    "\n",
    "train_dataloader = TorchDataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    worker_init_fn=seed_worker,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_dataloader = TorchDataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    worker_init_fn=seed_worker,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7-Xf3dvhm-HX",
   "metadata": {
    "cellView": "form",
    "id": "7-Xf3dvhm-HX"
   },
   "outputs": [],
   "source": [
    "#@markdown Visualize the Data.\n",
    "# Visualizing a few images of the dataset (post-processing/augmentation)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "idxs = [random.randint(0, len(train_dataset) -1) for _ in range(20)]\n",
    "grid_img = make_grid([train_dataset[idx][\"image\"] for idx in idxs], nrow=5)\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "of4EhRuHOF3m",
   "metadata": {
    "id": "of4EhRuHOF3m"
   },
   "source": [
    "# 6. Log model data with Galileo\n",
    "\n",
    "Model data is logged by wrapping the model with `watch` function. This step will log the model logits and embeddings. You can achieve this by adding 1 line of code to the standard pytorch model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dq-8r96TIduv",
   "metadata": {
    "id": "dq-8r96TIduv"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, resnet50\n",
    "\n",
    "EPOCHS = 3\n",
    "if _minimize_for_ci():\n",
    "    EPOCHS = 1\n",
    "\n",
    "# Load model and replace last layer.\n",
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(train_dataset.list_of_labels))\n",
    "torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set optimizer and loss.\n",
    "params_1x = [  # get the original weights, they'll be updated with a lower learning rate\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if \"fc\" not in str(name)\n",
    "]\n",
    "lr, weight_decay = 1e-5, 5e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": params_1x, \"lr\": lr},\n",
    "        {\"params\": model.fc.parameters(), \"lr\": lr * 10},\n",
    "    ],\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "from dataquality.integrations.torch import watch\n",
    "\n",
    "# 🔭🌕 Galileo logging -- Log Embeddings\n",
    "watch(\n",
    "    model=model,\n",
    "    classifier_layer=model.fc,\n",
    "    dataloaders=[train_dataloader, test_dataloader]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "knpdaq7YBmmo",
   "metadata": {
    "id": "knpdaq7YBmmo"
   },
   "source": [
    "# 7. Putting into Action: Training a Model\n",
    "\n",
    "We complete the training pipeline by using a standard PyTorch training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2kT2ulkjFArn",
   "metadata": {
    "id": "2kT2ulkjFArn"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep, time\n",
    "\n",
    "# Train !\n",
    "start = time()\n",
    "print(f\"Training for {EPOCHS} epochs on {device}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    dq.set_epoch(epoch)  # 🔭🌕 Galileo -- Set split\n",
    "\n",
    "    model.train()\n",
    "    train_loss = torch.tensor(0.0, device=device)\n",
    "    train_correct = torch.tensor(0, device=device)\n",
    "    \n",
    "    dq.set_split(TRAIN_SPLIT_NAME)\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as train_minibatchs:\n",
    "      for train_minibatch in train_minibatchs:\n",
    "          train_minibatchs.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "          images = train_minibatch[\"image\"].to(device)\n",
    "          labels = train_minibatch[\"label\"].to(device)\n",
    "\n",
    "          preds = model(images)\n",
    "          loss = criterion(preds, labels)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          with torch.no_grad():\n",
    "            train_loss += loss\n",
    "            train_batch_correct = (torch.argmax(preds, dim=1) == labels).sum()\n",
    "            train_correct += train_batch_correct\n",
    "\n",
    "          train_minibatchs.set_postfix(batch_loss=loss.item(), batch_accuracy=float(train_batch_correct) / BATCH_SIZE)\n",
    "          sleep(0.01)\n",
    "\n",
    "    print(f\"Training loss: {train_loss:.2f}\")\n",
    "    print(f\"Training accuracy: {100 * float(train_correct) / len(train_dataloader.dataset):.2f}\")\n",
    "    \n",
    "    dq.set_split(VAL_SPLIT_NAME)  # 🔭🌕 Galileo -- Set split\n",
    "    if test_dataloader is not None:\n",
    "        model.eval()\n",
    "        val_loss = torch.tensor(0.0, device=device)\n",
    "        val_correct = torch.tensor(0, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_minibatch in tqdm(test_dataloader):\n",
    "                images = val_minibatch[\"image\"].to(device)\n",
    "                labels = val_minibatch[\"label\"].to(device)\n",
    "                \n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, labels)\n",
    "\n",
    "                val_loss += loss\n",
    "                val_correct += (torch.argmax(preds, dim=1) == labels).sum()\n",
    "\n",
    "        print(f\"{VAL_SPLIT_NAME} loss: {val_loss:.2f}\")\n",
    "        print(f\"{VAL_SPLIT_NAME} accuracy: {100*val_correct/len(test_dataloader.dataset):.2f}\")\n",
    "\n",
    "end = time()\n",
    "print(f\"Total training time: {end-start:.1f} seconds\")\n",
    "dq.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "xRUq_rJ3Aiow",
   "metadata": {
    "id": "xRUq_rJ3Aiow"
   },
   "source": [
    "# General Help and Docs\n",
    "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
    "- To see more general data and model logging docs, run `dq.docs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cp76O3rIAiBE",
   "metadata": {
    "id": "cp76O3rIAiBE"
   },
   "outputs": [],
   "source": [
    "dq.get_data_logger().doc()\n",
    "help(dq.log_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
