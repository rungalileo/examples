{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccf6556",
   "metadata": {},
   "source": [
    "# Image Classification on Galileo 🔭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae56fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dataquality as dq\n",
    "except ImportError:\n",
    "    # Upgrade pip\n",
    "    !pip install -U pip &> /dev/null\n",
    "\n",
    "    # Install all dependecies\n",
    "    !pip install -U dataquality torch torchvision datasets &> /dev/null\n",
    "\n",
    "    print('👋 Installed necessary libraries and restarting runtime! This should only need to happen once.')\n",
    "    print('🙏 Continue with the rest of the notebook or hit \"Run All\" again!')\n",
    "\n",
    "    # Restart the runtime\n",
    "    import os, time\n",
    "    time.sleep(1) # gives the print statements time to flush\n",
    "    os._exit(0) # exits without allowing the next cell to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18243d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, List\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Random Seeds.\n",
    "def seed_all(seed: int) -> None:\n",
    "    \"\"\"Set all relevant seed for training a Pytorch Model.\n",
    "\n",
    "    Based on the following post:\n",
    "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def seed_worker(worker_id: int) -> None:\n",
    "    \"\"\"Set seed for dataloader worker.\n",
    "\n",
    "    Based on the following post:\n",
    "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Check Cuda.\n",
    "print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# A small function for minimizing the dataset for testing purposes\n",
    "import os\n",
    "\n",
    "def _minimize_for_ci() -> bool:\n",
    "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de208d5",
   "metadata": {},
   "source": [
    "# Connect to Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526adb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataquality version is v0.8.13\n",
      "\n",
      "✨ Initializing existing public project 'Image Classification Example'\n",
      "🏃‍♂️ Fetching existing run 'BEANS'\n",
      "🛰 Connected to existing project 'Image Classification Example', and existing run 'BEANS'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthcor/examples/.venv/lib/python3.9/site-packages/dataquality/core/init.py:159: UserWarning: Run: Image Classification Example/BEANS already exists! The existing run will get overwritten on call to finish()!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import dataquality as dq\n",
    "\n",
    "dq.init(\n",
    "      task_type=\"image_classification\",\n",
    "      project_name=\"Image Classification Example\",\n",
    "      run_name=\"BEANS\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63dee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create your dataset\n",
    "#\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "\n",
    "def find_label_col_name(col_names: List[str]) -> Optional[str]:\n",
    "    for col_name in col_names:\n",
    "        if \"label\" in col_name:\n",
    "            return col_name\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_imgs_location_col_name(col_names: List[str]) -> Optional[str]:\n",
    "    for col_name in col_names:\n",
    "        if \"path\" in col_name:\n",
    "            return col_name\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_imgs_dir() -> str:\n",
    "    d = f\"{os.environ['HOME']}/.cache/huggingface/datasets/beans/default/0.0.0\"\n",
    "    for p in list(os.walk(d)):\n",
    "        if p[0] != d and os.path.isdir(p[0]):\n",
    "            return (p[0])\n",
    "    raise Exception(\"Images directory not found. Did the dataset download correctly?\")\n",
    "\n",
    "\n",
    "STANDARD_DATA_COLUMNS_CV = [\"id\", \"text\", \"label_idx\"]\n",
    "\n",
    "\n",
    "class ImageDatasetFromCSV(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        imgs_dir: str,\n",
    "        split: str,\n",
    "        transform: Optional[Compose] = None,\n",
    "    ):\n",
    "        self.ds = df\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Find the id column, or create it if it doesn't exist.\n",
    "        if \"id\" not in self.ds.columns:\n",
    "            self.ds = self.ds.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "        # Find the label column name: could be label, labels, coarse_label, etc.\n",
    "        self.label_col_name = find_label_col_name(self.ds.columns)\n",
    "        if self.label_col_name is None:\n",
    "            raise ValueError(\"Could not find the label column in the dataframe\")\n",
    "        STANDARD_DATA_COLUMNS_CV.append(self.label_col_name)\n",
    "        self.list_of_labels = list(self.ds[self.label_col_name].unique())\n",
    "\n",
    "        # Convert string labels to indexes, store them in the column label_idx.\n",
    "        str_to_int = {\n",
    "            label: index\n",
    "            for index, label in enumerate(self.ds[self.label_col_name].unique())\n",
    "        }\n",
    "        self.ds[\"label_idx\"] = self.ds[self.label_col_name].map(str_to_int)\n",
    "\n",
    "        # Find the images paths column name: could be path, rel_path, imgs_path, etc.\n",
    "        self.imgs_location_colname = find_imgs_location_col_name(self.ds.columns)\n",
    "        if self.imgs_location_colname is None:\n",
    "            raise ValueError(\n",
    "                \"Could not find the images location column in the dataframe\"\n",
    "            )\n",
    "        STANDARD_DATA_COLUMNS_CV.append(self.imgs_location_colname)\n",
    "\n",
    "        # Get the metadata columns.\n",
    "        meta_data_cols = [\n",
    "            column\n",
    "            for column in self.ds.columns\n",
    "            if column not in STANDARD_DATA_COLUMNS_CV\n",
    "        ]\n",
    "\n",
    "        # 🔭🌕 Galileo logging -- Input Data\n",
    "        dq.log_image_dataset(\n",
    "            dataset=self.ds,\n",
    "            label=\"labels\",\n",
    "            split=split,\n",
    "            meta=meta_data_cols,\n",
    "            imgs_dir=self.imgs_dir,\n",
    "            imgs_location_colname=self.imgs_location_colname,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        img_path = os.path.join(\n",
    "            self.imgs_dir, self.ds.loc[idx, self.imgs_location_colname]\n",
    "        )\n",
    "        image = Image.open(img_path)\n",
    "        id = self.ds.loc[idx, \"id\"]\n",
    "        label = self.ds.loc[idx, \"label_idx\"]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"label\": label, \"id\": id}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6421f",
   "metadata": {},
   "source": [
    "# Load Data and Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380920af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Image as datasetsImage\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dataset_train = load_dataset(\"beans\", split=\"train\").cast_column(\"image\", datasetsImage(decode=False))\n",
    "dataset_test = load_dataset(\"beans\", split=\"test\").cast_column(\"image\", datasetsImage(decode=False))\n",
    "\n",
    "train_labels = dataset_train.features[\"labels\"].names\n",
    "train_df = dataset_train.to_pandas()\n",
    "train_df[\"labels\"] = train_df[\"labels\"].map(lambda x: train_labels[x])\n",
    "\n",
    "test_labels = dataset_test.features[\"labels\"].names\n",
    "test_df = dataset_test.to_pandas()\n",
    "test_df[\"labels\"] = test_df[\"labels\"].map(lambda x: test_labels[x])\n",
    "\n",
    "train_df = train_df.reset_index()\n",
    "train_df[\"index\"] = np.arange(len(train_df))\n",
    "test_df = test_df.reset_index()\n",
    "test_df[\"index\"] = np.arange(len(test_df))\n",
    "\n",
    "if _minimize_for_ci():\n",
    "    train_df = train_df[:10]\n",
    "    test_df = test_df[test_df.labels.isin(train_df.labels.unique())][:10]\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "imgs_dir = _get_imgs_dir()\n",
    "\n",
    "train_dataset = ImageDatasetFromCSV(\n",
    "    df=train_df, imgs_dir=imgs_dir, split=\"train\", transform=data_transform\n",
    ")\n",
    "test_dataset = ImageDatasetFromCSV(\n",
    "    df=test_df, imgs_dir=imgs_dir, split=\"test\", transform=data_transform\n",
    ")\n",
    "\n",
    "print(f\"Loaded train dataset with {len(train_dataset.ds)} samples and {len(train_dataset.list_of_labels)} labels\")\n",
    "print(f\"Loaded val dataset with {len(test_dataset.ds)} samples and {len(test_dataset.list_of_labels)} labels\")\n",
    "\n",
    "dq.set_labels_for_run(train_dataset.list_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9da24",
   "metadata": {},
   "source": [
    "# Create the dataloaders and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71611b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching dataquality to model and dataloaders\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "from dataquality.integrations.torch import watch\n",
    "\n",
    "# Some global HP.\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "EPOCHS = 4\n",
    "if _minimize_for_ci():\n",
    "    EPOCHS = 1\n",
    "\n",
    "# Create data loaders.\n",
    "NUM_WORKERS = 0\n",
    "SEED_WORKER = 42\n",
    "\n",
    "seed_all(SEED_WORKER)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    worker_init_fn=seed_worker,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    worker_init_fn=seed_worker,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Load model and replace last layer.\n",
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(train_dataset.list_of_labels))\n",
    "torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set optimizer and loss.\n",
    "params_1x = [  # get the original weights, they'll be updated with a lower learning rate\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if \"fc\" not in str(name)\n",
    "]\n",
    "lr, weight_decay = 1e-5, 5e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": params_1x, \"lr\": lr},\n",
    "        {\"params\": model.fc.parameters(), \"lr\": lr * 10},\n",
    "    ],\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "watch(\n",
    "    model=model,\n",
    "    classifier_layer=model.fc,\n",
    "    dataloaders=[train_dataloader, test_dataloader],\n",
    "    unpatch_on_start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43819862",
   "metadata": {},
   "source": [
    "# Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0589f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1 epochs on cpu\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [04:28<00:00, 29.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.71\n",
      "Training accuracy: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.85\n",
      "Validation accuracy: 64.06\n",
      "Total training time: 277.0 seconds\n",
      "☁️ Uploading Data\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04870891571044922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "training",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785abc43574b498abf4d02769fe47ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015207290649414062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Processing data for upload",
       "rate": null,
       "total": 9,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data for upload:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01806807518005371,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "training (epoch=1)",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training (epoch=1):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02844381332397461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Uploading data to Galileo",
       "rate": null,
       "total": 8489576,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/8.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019263029098510742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Uploading data to Galileo",
       "rate": null,
       "total": 49028,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/47.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0368189811706543,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Uploading data to Galileo",
       "rate": null,
       "total": 191730122,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/183M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020376920700073242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "test",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a039d934564de3acae5b4d4580680a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026905059814453125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Processing data for upload",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data for upload:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015494108200073242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "test (epoch=1)",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test (epoch=1):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021044015884399414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Uploading data to Galileo",
       "rate": null,
       "total": 1061400,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/1.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02660369873046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Uploading data to Galileo",
       "rate": null,
       "total": 18332,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/17.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02019476890563965,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 65,
       "postfix": null,
       "prefix": "Uploading data to Galileo",
       "rate": null,
       "total": 23623744,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/22.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job default successfully submitted. Results will be available soon at https://console.cloud.rungalileo.io/insights?projectId=736c3b50-458c-4b11-8232-f771e27ed0d4&runId=b906bc90-d7aa-4d3c-a1a9-a4ec22b7777b&split=training&metric=f1&depHigh=1&depLow=0&taskType=3\n",
      "Waiting for job...\n",
      "\tFound embs. Analyzing dimensions\n",
      "\tApplying dimensionality reduction to embs\n",
      "\tLooking for data embeddings\n",
      "\tNo data embs found, skipping processing\n",
      "\tSaving processed training data\n",
      "\tCalculating test data error potential\n",
      "\tSaving processed test data\n",
      "Done! Job finished with status completed\n",
      "Click here to see your run! https://console.cloud.rungalileo.io/insights?projectId=736c3b50-458c-4b11-8232-f771e27ed0d4&runId=b906bc90-d7aa-4d3c-a1a9-a4ec22b7777b&split=training&metric=f1&depHigh=1&depLow=0&taskType=3\n",
      "🧹 Cleaning up\n",
      "🧹 Cleaning up\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep, time\n",
    "\n",
    "# Train !\n",
    "start = time()\n",
    "print(f\"Training for {EPOCHS} epochs on {device}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    dq.set_epoch(epoch)\n",
    "\n",
    "    model.train()\n",
    "    train_loss = torch.tensor(0.0, device=device)\n",
    "    train_correct = torch.tensor(0, device=device)\n",
    "    \n",
    "    dq.set_split(\"train\")\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as train_minibatchs:\n",
    "        for train_minibatch in train_minibatchs:\n",
    "            train_minibatchs.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            images = train_minibatch[\"image\"].to(device)\n",
    "            labels = train_minibatch[\"label\"].to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "            train_batch_correct = (torch.argmax(preds, dim=1) == labels).sum()\n",
    "            train_correct += train_batch_correct\n",
    "\n",
    "        train_minibatchs.set_postfix(batch_loss=loss.item(), batch_accuracy=float(train_batch_correct) / BATCH_SIZE)\n",
    "        sleep(0.01)\n",
    "\n",
    "    print(f\"Training loss: {train_loss:.2f}\")\n",
    "    print(f\"Training accuracy: {100 * float(train_correct) / len(train_dataloader.dataset):.2f}\")\n",
    "    \n",
    "    dq.set_split(\"test\")\n",
    "    if test_dataloader is not None:\n",
    "        model.eval()\n",
    "        val_loss = torch.tensor(0.0, device=device)\n",
    "        val_correct = torch.tensor(0, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_minibatch in tqdm(test_dataloader):\n",
    "                images = val_minibatch[\"image\"].to(device)\n",
    "                labels = val_minibatch[\"label\"].to(device)\n",
    "                \n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, labels)\n",
    "\n",
    "                val_loss += loss\n",
    "                val_correct += (torch.argmax(preds, dim=1) == labels).sum()\n",
    "\n",
    "        print(f\"Validation loss: {val_loss:.2f}\")\n",
    "        print(f\"Validation accuracy: {100*val_correct/len(test_dataloader.dataset):.2f}\")\n",
    "\n",
    "end = time()\n",
    "print(f\"Total training time: {end-start:.1f} seconds\")\n",
    "dq.finish()\n",
    "print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
