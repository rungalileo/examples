{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fccf6556",
      "metadata": {
        "id": "fccf6556"
      },
      "source": [
        "# Image Classification using PyTorch and ðŸ”­ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
        "\n",
        "This notebook pulls data from S3 and is the suggested way for working with images in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "NkLn5MRHbowm",
      "metadata": {
        "id": "NkLn5MRHbowm"
      },
      "source": [
        "# 1. Install Prerequisites and Login Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vthATGXfFAup",
      "metadata": {
        "cellView": "form",
        "id": "vthATGXfFAup"
      },
      "outputs": [],
      "source": [
        "#@markdown Install `dataquality`\n",
        "# Upgrade pip\n",
        "!pip install -U pip &> /dev/null\n",
        "\n",
        "# Install all dependecies\n",
        "!pip install -U dataquality torch &> /dev/null\n",
        "\n",
        "print('ðŸ‘‹ Installed necessary libraries!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zBGmxapS_S5w",
      "metadata": {
        "cellView": "form",
        "id": "zBGmxapS_S5w"
      },
      "outputs": [],
      "source": [
        "#@markdown Check that a GPU is available\n",
        "\n",
        "import torch\n",
        "# Check Cuda.\n",
        "if torch.cuda.is_available():\n",
        "  print(\"âš¡ You are connected to a GPU!\")\n",
        "else:\n",
        "  print(\"â—You are NOT connected to a GPU â—It is recommended to connect to a GPU before training\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bi3jMDTScRIB",
      "metadata": {
        "id": "bi3jMDTScRIB"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "\n",
        "dq.login()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Ty4TmPT7TYTO",
      "metadata": {
        "id": "Ty4TmPT7TYTO"
      },
      "source": [
        "# 2. Download the data in the notebook\n",
        "\n",
        "Pull the data from GCS or S3. \n",
        "\n",
        "For enterprise customers, your cluster will have to have permissions to request data from S3 and GCS for AWS and GCP clusters. Cross account transfer is not currently supported.\n",
        "\n",
        "For free users, the data has to be publicly available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8qH3-LXTh9Z",
      "metadata": {
        "id": "f8qH3-LXTh9Z"
      },
      "outputs": [],
      "source": [
        "CLOUD_ZIP_PATH = f\"https://storage.googleapis.com/galileo-public-data/CV_datasets/ImageNet10_animals_train_val.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WShoKNhsTh5Z",
      "metadata": {
        "cellView": "form",
        "id": "WShoKNhsTh5Z"
      },
      "outputs": [],
      "source": [
        "#@markdown Download the images\n",
        "\n",
        "LOCAL_DATA_DIR = \"tmp/content\"\n",
        "dataset_dir_name = CLOUD_ZIP_PATH.split('/')[-1].split('.zip')[0]\n",
        "\n",
        "cmd = f\"\"\"\n",
        "mkdir -p {LOCAL_DATA_DIR}\n",
        "if [ ! -d {LOCAL_DATA_DIR}/{dataset_dir_name} ]\n",
        "then\n",
        "  echo \"Downloading data\"\n",
        "  curl {CLOUD_ZIP_PATH} -o {LOCAL_DATA_DIR}/{dataset_dir_name}.zip\n",
        "  unzip {LOCAL_DATA_DIR}/{dataset_dir_name}.zip -d {LOCAL_DATA_DIR}\n",
        "else\n",
        "  echo \"Data already exists locally. Moving on.\"\n",
        "fi\n",
        "\"\"\"\n",
        "with open('download_images.sh', 'w') as file:\n",
        "  file.write(cmd)\n",
        "\n",
        "!bash download_images.sh\n",
        "\n",
        "# Select a small portion of the dataset for CI.\n",
        "import os\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "vQI6GwGGcFIE",
      "metadata": {
        "id": "vQI6GwGGcFIE"
      },
      "source": [
        "# 3. Initialize Galileo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddyS8Hw_cAQI",
      "metadata": {
        "id": "ddyS8Hw_cAQI"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"ImageNet10_animals_train_val\" # ðŸ”­ðŸŒ• used for creating a run name in Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oXua2j7QFFBJ",
      "metadata": {
        "id": "oXua2j7QFFBJ"
      },
      "outputs": [],
      "source": [
        "# ðŸ”­ðŸŒ• Initializing a new run in Galileo. Each run is part of a project.\n",
        "dq.init(task_type=\"image_classification\", \n",
        "        project_name=\"image_classification_pytorch\", \n",
        "        run_name=f\"example_run_{DATASET_NAME.replace('/', '-')}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "etbtUHJyGwyD",
      "metadata": {
        "id": "etbtUHJyGwyD"
      },
      "source": [
        "# 4. Create Dataset and Log Input Data with Galileo\n",
        "\n",
        "Input data is logged via `log_image_dataset`. This step will log the images, gold labels, data split, and list of all labels. You can achieve this adding 1 line of code to the standard PyTorch Dataset Class.\n",
        "\n",
        "To skip uploading the images, provide their location in a (unzipped) folder in the cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dVbcMePMhxEm",
      "metadata": {
        "id": "dVbcMePMhxEm"
      },
      "outputs": [],
      "source": [
        "CLOUD_DATA_DIR = f\"https://storage.googleapis.com/galileo-public-data/CV_datasets/ImageNet10_animals_train_val\" # ðŸ”­ðŸŒ•  Set to None if data not available unzipped in the cloud (which would require uploading)\n",
        "train_csv_relpath = \"train.csv\"\n",
        "val_csv_relpath = \"val.csv\"\n",
        "# Fix Labels: The labels of ImageNet are hashes. Convert to human readable labels\n",
        "CLASSES_DICT = {'n02124075' : 'Egyptian_cat', 'n02107574': 'Greater_Swiss_Mountain_dog', 'n02114367': 'Timber_wolf', 'n02085620': 'Chihuahua', 'n02114548': 'White_wolf', 'n02117135': 'Hyena', 'n02108915': 'French_bulldog', 'n02123159': 'Tiger_cat', 'n02114855': 'Coyote', 'n02106550': 'Rottweiler'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DOEsmOshGHxD",
      "metadata": {
        "cellView": "form",
        "id": "DOEsmOshGHxD"
      },
      "outputs": [],
      "source": [
        "#@markdown Fix a random Seed and load helper methods.\n",
        "from typing import Optional, List\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Fix a random seed.\n",
        "def seed_all(seed: int) -> None:\n",
        "    \"\"\"Set all relevant seed for training a Pytorch Model.\n",
        "\n",
        "    Based on the following post:\n",
        "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def seed_worker(worker_id: int) -> None:\n",
        "    \"\"\"Set seed for dataloader worker.\n",
        "\n",
        "    Based on the following post:\n",
        "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
        "    \"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# Methods for loading the df into a dataset.\n",
        "def find_label_col_name(col_names: List[str]) -> Optional[str]:\n",
        "    for col_name in col_names:\n",
        "        if \"label\" in col_name:\n",
        "            return col_name\n",
        "    return None\n",
        "\n",
        "def find_imgs_loc_col_name(col_names: List[str]) -> Optional[str]:\n",
        "    for col_name in col_names:\n",
        "        if \"path\" in col_name:\n",
        "            return col_name\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eViQAp_AGD6j",
      "metadata": {
        "cellView": "form",
        "id": "eViQAp_AGD6j"
      },
      "outputs": [],
      "source": [
        "#@markdown Create the Dataset class\n",
        "from uuid import uuid4\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "STANDARD_DATA_COLUMNS_CV = [\"id\", \"text\", \"label_idx\", \"path\"]\n",
        "\n",
        "class ImageDatasetFromLocal(TorchDataset):\n",
        "    def __init__(\n",
        "        self, \n",
        "        split: str,\n",
        "        imgs_dir: str,\n",
        "        csv_relpath: str,\n",
        "        cloud_imgs_dir: str = None,\n",
        "        transform: transforms.Compose = None, \n",
        "        list_of_labels: List[str] = None\n",
        "    ):  \n",
        "        \"\"\"\n",
        "        Args:\n",
        "          split: the split for the hf dataset\n",
        "          imgs_dir: location of images on this machine (for training)\n",
        "          csv_relpath: relative path to the csv on this machine (for training)\n",
        "          cloud_imgs_dir: location of images in the cloud (for reference in console)\n",
        "          transform [Optional]: a transform to apply to the images dynamically \n",
        "            before training\n",
        "          list_of_labels [Optional]: the list of labels used to convert between \n",
        "            label (string) and label_idx (int). To insure consistency pass the \n",
        "            list_of_labels of the training dataset to the test/val datasets.\n",
        "        \"\"\"\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.cloud_imgs_dir = cloud_imgs_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        \n",
        "        self.ds = pd.read_csv(f\"{imgs_dir}/{csv_relpath}\")\n",
        "        \n",
        "        # Find the label column name: could be label, labels, coarse_label, etc.\n",
        "        self.label_col_name = find_label_col_name(self.ds.columns)\n",
        "        if self.label_col_name is None:\n",
        "            raise ValueError(f\"Could not find the label column in the dataframe\")\n",
        "        STANDARD_DATA_COLUMNS_CV.append(self.label_col_name)\n",
        "\n",
        "        if _minimize_for_ci():\n",
        "          if self.label_col_name is None:\n",
        "            self.ds = self.ds[:10].reset_index(drop=True)\n",
        "          else:\n",
        "            self.ds = self.ds.groupby(self.label_col_name, group_keys=False).apply(lambda x: x.sample(10)).reset_index(drop=True)\n",
        "\n",
        "        # Fix Labels: The labels of ImageNet are hashes. Convert to human readable labels\n",
        "        self.ds[self.label_col_name] = self.ds[self.label_col_name].map(CLASSES_DICT)\n",
        "\n",
        "        # Set the list of labels for this split.\n",
        "        self.list_of_labels = list_of_labels\n",
        "        if self.list_of_labels is None:\n",
        "          self.list_of_labels = list(self.ds[self.label_col_name].unique())\n",
        "\n",
        "        # Add column with labels as string (for dq).\n",
        "        label_to_labelidx = {label:i for i, label in enumerate(self.list_of_labels)}\n",
        "        self.ds[\"label_idx\"] =  self.ds[self.label_col_name].map(label_to_labelidx)\n",
        "\n",
        "        # Find the path column name: could be path, relpath, etc (or none).\n",
        "        self.imgs_location_colname = find_imgs_loc_col_name(self.ds.columns)\n",
        "        STANDARD_DATA_COLUMNS_CV.append(self.imgs_location_colname)\n",
        "\n",
        "        # Get the metadata columns.\n",
        "        self.meta_data_cols = [\n",
        "            column\n",
        "            for column in self.ds.columns\n",
        "            if column not in STANDARD_DATA_COLUMNS_CV\n",
        "        ]\n",
        "\n",
        "        # Set the images local paths in the \"text\" column (for training and smart features)\n",
        "        self.ds[\"text\"] = self.ds[self.imgs_location_colname].apply(lambda x: f\"{self.imgs_dir}/{x}\")\n",
        "        # If a remote location is given, set the remote paths (to skip uploading)\n",
        "        if cloud_imgs_dir is not None:\n",
        "            self.ds[\"imgs_remote_paths\"] = self.ds[self.imgs_location_colname].apply(lambda x: f\"{self.cloud_imgs_dir}/{x}\")\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.ds.loc[idx]\n",
        "        img_path = os.path.join(self.imgs_dir, row[self.imgs_location_colname])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label, id = row[\"label_idx\"], row[\"id\"]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return {\"image\": image, \"label\": label, \"id\": id}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ADbbMqzxnGjt",
      "metadata": {
        "cellView": "form",
        "id": "ADbbMqzxnGjt"
      },
      "outputs": [],
      "source": [
        "# Create the Dataset and Dataloader + Log input to ðŸ”­ðŸŒ• Galileo\n",
        "\n",
        "# Create the Datasets.\n",
        "image_crop_size = (224, 224)\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_crop_size[0], image_crop_size[1])),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "train_transforms = transforms.Compose(val_transforms.transforms + [transforms.RandomHorizontalFlip()])\n",
        "\n",
        "TRAIN_SPLIT_NAME = \"train\"\n",
        "train_dataset = ImageDatasetFromLocal(\n",
        "    imgs_dir=f\"./{LOCAL_DATA_DIR}/{dataset_dir_name}\", \n",
        "    cloud_imgs_dir=CLOUD_DATA_DIR, \n",
        "    csv_relpath=train_csv_relpath,\n",
        "    split=TRAIN_SPLIT_NAME, \n",
        "    transform=train_transforms)\n",
        "\n",
        "VAL_SPLIT_NAME = \"validation\" # this var is needed in dq.set_split down below\n",
        "val_dataset = ImageDatasetFromLocal(\n",
        "    imgs_dir=f\"./{LOCAL_DATA_DIR}/{dataset_dir_name}\",\n",
        "    cloud_imgs_dir=CLOUD_DATA_DIR, \n",
        "    csv_relpath=val_csv_relpath,\n",
        "    split=VAL_SPLIT_NAME, \n",
        "    transform=val_transforms,\n",
        "    list_of_labels=train_dataset.list_of_labels)\n",
        "\n",
        "print(f\"Loaded {TRAIN_SPLIT_NAME} dataset with {len(train_dataset.ds)} samples and {len(train_dataset.list_of_labels)} labels\")\n",
        "print(f\"Loaded {VAL_SPLIT_NAME} dataset with {len(val_dataset.ds)} samples and  {len(val_dataset.list_of_labels)} labels\")\n",
        "\n",
        "# ðŸ”­ðŸŒ• Galileo log: Set labels\n",
        "dq.set_labels_for_run(train_dataset.list_of_labels)\n",
        "\n",
        "# ðŸ”­ðŸŒ• Galileo log: Log dataset\n",
        "dq.log_image_dataset(\n",
        "    dataset = train_dataset.ds,\n",
        "    label = train_dataset.label_col_name,\n",
        "    split = train_dataset.split,\n",
        "    meta = train_dataset.meta_data_cols,\n",
        "    imgs_local_colname = \"text\",\n",
        "    imgs_remote=\"imgs_remote_paths\" if CLOUD_DATA_DIR is not None else None\n",
        ")\n",
        "dq.log_image_dataset(\n",
        "    dataset = val_dataset.ds,\n",
        "    label = val_dataset.label_col_name,\n",
        "    split = val_dataset.split,\n",
        "    meta = val_dataset.meta_data_cols,\n",
        "    imgs_local_colname = \"text\",\n",
        "    imgs_remote=\"imgs_remote_paths\" if CLOUD_DATA_DIR is not None else None\n",
        ")\n",
        "\n",
        "# Create the DataLoaders.\n",
        "from torch.utils.data import DataLoader as TorchDataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "NUM_WORKERS = 0 \n",
        "SEED_WORKER = 42\n",
        "\n",
        "seed_all(SEED_WORKER)\n",
        "\n",
        "train_dataloader = TorchDataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    worker_init_fn=seed_worker,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_dataloader = TorchDataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    worker_init_fn=seed_worker,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7-Xf3dvhm-HX",
      "metadata": {
        "cellView": "form",
        "id": "7-Xf3dvhm-HX"
      },
      "outputs": [],
      "source": [
        "#@markdown Visualize the Data.\n",
        "# Visualizing a few images of the dataset (post-processing/augmentation)\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "idxs = [random.randint(0, len(train_dataset) -1) for _ in range(20)]\n",
        "grid_img = make_grid([train_dataset[idx][\"image\"] for idx in idxs], nrow=5)\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "of4EhRuHOF3m",
      "metadata": {
        "id": "of4EhRuHOF3m"
      },
      "source": [
        "# 6. Log model data with Galileo\n",
        "\n",
        "Model data is logged by wrapping the model with `watch` function. This step will log the model logits and embeddings. You can achieve this by adding 1 line of code to the standard pytorch model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dq-8r96TIduv",
      "metadata": {
        "id": "dq-8r96TIduv"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "# Load model and replace last layer.\n",
        "model = resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(train_dataset.list_of_labels))\n",
        "torch.nn.init.xavier_uniform_(model.fc.weight)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Set optimizer and loss.\n",
        "params_1x = [  # get the original weights, they'll be updated with a lower learning rate\n",
        "    param\n",
        "    for name, param in model.named_parameters()\n",
        "    if \"fc\" not in str(name)\n",
        "]\n",
        "lr, weight_decay = 1e-5, 5e-4\n",
        "optimizer = torch.optim.Adam(\n",
        "    [\n",
        "        {\"params\": params_1x, \"lr\": lr},\n",
        "        {\"params\": model.fc.parameters(), \"lr\": lr * 10},\n",
        "    ],\n",
        "    weight_decay=weight_decay,\n",
        ")\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "from dataquality.integrations.torch import watch, unwatch\n",
        "\n",
        "# ðŸ”­ðŸŒ• Galileo logging -- Watch model\n",
        "watch(\n",
        "    model=model,\n",
        "    classifier_layer=model.fc,\n",
        "    dataloaders=[train_dataloader, val_dataloader],\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "knpdaq7YBmmo",
      "metadata": {
        "id": "knpdaq7YBmmo"
      },
      "source": [
        "# 7. Putting into Action: Training a Model\n",
        "\n",
        "We complete the training pipeline by using a standard PyTorch training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2kT2ulkjFArn",
      "metadata": {
        "id": "2kT2ulkjFArn"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from time import sleep, time\n",
        "\n",
        "# Train !\n",
        "start = time()\n",
        "print(f\"Training for {EPOCHS} epochs on {device}\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
        "    dq.set_epoch(epoch)  # ðŸ”­ðŸŒ• Galileo -- Set split\n",
        "\n",
        "    model.train()\n",
        "    train_loss = torch.tensor(0.0, device=device)\n",
        "    train_correct = torch.tensor(0, device=device)\n",
        "    \n",
        "    dq.set_split(TRAIN_SPLIT_NAME)\n",
        "    with tqdm(train_dataloader, unit=\"batch\") as train_minibatchs:\n",
        "        for train_minibatch in train_minibatchs:\n",
        "            train_minibatchs.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "            images = train_minibatch[\"image\"].to(device)\n",
        "            labels = train_minibatch[\"label\"].to(device)\n",
        "\n",
        "            preds = model(images)\n",
        "            loss = criterion(preds, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                train_loss += loss\n",
        "                train_batch_correct = (torch.argmax(preds, dim=1) == labels).sum()\n",
        "                train_correct += train_batch_correct\n",
        "\n",
        "            train_minibatchs.set_postfix(batch_loss=loss.item(), batch_accuracy=float(train_batch_correct) / BATCH_SIZE)\n",
        "            sleep(0.01)\n",
        "\n",
        "    print(f\"Training loss: {train_loss:.2f}\")\n",
        "    print(f\"Training accuracy: {100 * float(train_correct) / len(train_dataloader.dataset):.2f}\")\n",
        "    \n",
        "    dq.set_split(VAL_SPLIT_NAME)  # ðŸ”­ðŸŒ• Galileo -- Set split\n",
        "    if val_dataloader is not None:\n",
        "        model.eval()\n",
        "        val_loss = torch.tensor(0.0, device=device)\n",
        "        val_correct = torch.tensor(0, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_minibatch in tqdm(val_dataloader):\n",
        "                images = val_minibatch[\"image\"].to(device)\n",
        "                labels = val_minibatch[\"label\"].to(device)\n",
        "                \n",
        "                preds = model(images)\n",
        "                loss = criterion(preds, labels)\n",
        "\n",
        "                val_loss += loss\n",
        "                val_correct += (torch.argmax(preds, dim=1) == labels).sum()\n",
        "\n",
        "        print(f\"{VAL_SPLIT_NAME} loss: {val_loss:.2f}\")\n",
        "        print(f\"{VAL_SPLIT_NAME} accuracy: {100*val_correct/len(val_dataloader.dataset):.2f}\")\n",
        "\n",
        "end = time()\n",
        "print(f\"Total training time: {end-start:.1f} seconds\")\n",
        "dq.finish()\n",
        "\n",
        "unwatch(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2adc4b0c",
      "metadata": {},
      "source": [
        "# 8. Monitoring: Inference on Production data\n",
        "\n",
        "After training, continue monitoring the model's performance by logging predictions on production (unlabeled) data. The integration of the `dataquality` client is very similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba20d93b",
      "metadata": {},
      "outputs": [],
      "source": [
        "INF_SPLIT_NAME = \"inference\"\n",
        "INF_NAME = \"inference_run1\"\n",
        "\n",
        "inf_dataset = ImageDatasetFromLocal(\n",
        "    imgs_dir=f\"./{LOCAL_DATA_DIR}/{dataset_dir_name}\", \n",
        "    cloud_imgs_dir=CLOUD_DATA_DIR, \n",
        "    csv_relpath=\"inf.csv\",\n",
        "    split=INF_SPLIT_NAME, \n",
        "    transform=val_transforms)\n",
        "\n",
        "print(f\"Loaded {INF_SPLIT_NAME} dataset with {len(inf_dataset.ds)} samples and {len(train_dataset.list_of_labels)} labels\")\n",
        "\n",
        "# ðŸ”­ðŸŒ• Galileo log: Set labels\n",
        "dq.set_labels_for_run(train_dataset.list_of_labels)\n",
        "\n",
        "# ðŸ”­ðŸŒ• Galileo log: Log dataset\n",
        "dq.log_image_dataset(\n",
        "    dataset = inf_dataset.ds,\n",
        "    split = INF_SPLIT_NAME,\n",
        "    inference_name = INF_NAME,\n",
        "    imgs_local_colname = \"text\",\n",
        "    imgs_remote=\"imgs_remote_paths\" if CLOUD_DATA_DIR is not None else None\n",
        ")\n",
        "\n",
        "inf_dataloader = TorchDataLoader(\n",
        "    inf_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    worker_init_fn=seed_worker,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# ðŸ”­ðŸŒ• Galileo logging -- Watch model\n",
        "watch(\n",
        "    model=model,\n",
        "    classifier_layer=model.fc,\n",
        "    dataloaders=[inf_dataloader],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa5596c",
      "metadata": {},
      "outputs": [],
      "source": [
        "dq.set_split(INF_SPLIT_NAME, inference_name = INF_NAME)  # ðŸ”­ðŸŒ• Galileo -- Set split\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inf_minibatch in tqdm(inf_dataloader):\n",
        "        images = inf_minibatch[\"image\"].to(device)\n",
        "        preds = model(images)\n",
        "\n",
        "dq.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "xRUq_rJ3Aiow",
      "metadata": {
        "id": "xRUq_rJ3Aiow"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cp76O3rIAiBE",
      "metadata": {
        "id": "cp76O3rIAiBE"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
