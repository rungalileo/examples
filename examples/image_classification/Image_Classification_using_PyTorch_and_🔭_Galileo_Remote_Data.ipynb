{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccf6556",
   "metadata": {
    "id": "fccf6556"
   },
   "source": [
    "# Image Classification using PyTorch and 🔭 Galileo\n",
    "\n",
    "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
    "\n",
    "This notebook pulls data from S3 and is the suggested way for working with images in Galileo.\n",
    "\n",
    "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vthATGXfFAup",
   "metadata": {
    "cellView": "form",
    "id": "vthATGXfFAup"
   },
   "outputs": [],
   "source": [
    "#@title Install `dataquality`\n",
    "# Upgrade pip\n",
    "!pip install -U pip &> /dev/null\n",
    "\n",
    "# Install all dependecies\n",
    "!pip install -U dataquality matplotlib==3.1.3 torch torchmetrics==0.10.0 datasets &> /dev/null\n",
    "\n",
    "print('👋 Installed necessary libraries!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zBGmxapS_S5w",
   "metadata": {
    "cellView": "form",
    "id": "zBGmxapS_S5w"
   },
   "outputs": [],
   "source": [
    "#@markdown Check that a GPU is available\n",
    "\n",
    "import torch\n",
    "# Check Cuda.\n",
    "if torch.cuda.is_available():\n",
    "  print(\"⚡ You are connected to a GPU!\")\n",
    "else:\n",
    "  print(\"❗You are NOT connected to a GPU ❗It is recommended to connect to a GPU before training\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Skd8uU7HFKwi",
   "metadata": {
    "id": "Skd8uU7HFKwi"
   },
   "source": [
    "# 1. Login to Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XNuCGL_eFFJl",
   "metadata": {
    "id": "XNuCGL_eFFJl"
   },
   "outputs": [],
   "source": [
    "import dataquality as dq\n",
    "\n",
    "dq.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5yL5xeoZFPk_",
   "metadata": {
    "id": "5yL5xeoZFPk_"
   },
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pTOMOlv4FFGB",
   "metadata": {
    "cellView": "form",
    "id": "pTOMOlv4FFGB"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"CV_datasets/ImageNet15_animals\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Al89xQQ_F9rv",
   "metadata": {
    "id": "Al89xQQ_F9rv"
   },
   "source": [
    "# 3. Initialize Galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oXua2j7QFFBJ",
   "metadata": {
    "id": "oXua2j7QFFBJ"
   },
   "outputs": [],
   "source": [
    "# 🔭🌕 Initializing a new run in Galileo. Each run is part of a project.\n",
    "dq.init(task_type=\"image_classification\", \n",
    "        project_name=\"image_classification_pytorch\", \n",
    "        run_name=f\"example_run_{dataset_name.replace('/', '-')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etbtUHJyGwyD",
   "metadata": {
    "id": "etbtUHJyGwyD"
   },
   "source": [
    "# 4. Create Dataset and Log Input Data with Galileo\n",
    "\n",
    "Input data can be logged via `log_image_dataset`. This step will log the images, gold labels, data split, and list of all labels. You can achieve this adding 1 line of code to the standard PyTorch Dataset Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DOEsmOshGHxD",
   "metadata": {
    "cellView": "form",
    "id": "DOEsmOshGHxD"
   },
   "outputs": [],
   "source": [
    "#@markdown Fix a random Seed and load helper methods.\n",
    "from typing import Optional, List\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Fix a random seed.\n",
    "def seed_all(seed: int) -> None:\n",
    "    \"\"\"Set all relevant seed for training a Pytorch Model.\n",
    "\n",
    "    Based on the following post:\n",
    "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def seed_worker(worker_id: int) -> None:\n",
    "    \"\"\"Set seed for dataloader worker.\n",
    "\n",
    "    Based on the following post:\n",
    "    https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Methods for loading the df into a dataset.\n",
    "def find_label_col_name(col_names: List[str]) -> Optional[str]:\n",
    "    for col_name in col_names:\n",
    "        if \"label\" in col_name:\n",
    "            return col_name\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff71623",
   "metadata": {},
   "source": [
    "# Pull the data to the notebook\n",
    "\n",
    "This could be done from S3 too.\n",
    "\n",
    "For enterprise customers, note that your cluster will have to have permissions to request data from S3 and GCS for AWS and GCP clusters. Cross account transfer is not currently supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a54c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"\n",
    "mkdir -p tmp/content\n",
    "if [ ! -d tmp/content/{dataset_name.split('/')[-1]} ]\n",
    "then\n",
    "  echo \"Downloading data\"\n",
    "  curl https://storage.googleapis.com/galileo-public-data/CV_datasets/{dataset_name.split('/')[-1]}.zip -o tmp/content/{dataset_name.split('/')[-1]}.zip\n",
    "  unzip tmp/content/{dataset_name.split('/')[-1]}.zip -d tmp/content\n",
    "else\n",
    "  echo \"Data exists. Moving on.\"\n",
    "fi\n",
    "\"\"\"\n",
    "with open('download_images.sh', 'w') as file:\n",
    "  file.write(cmd)\n",
    "\n",
    "!bash download_images.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eViQAp_AGD6j",
   "metadata": {
    "cellView": "form",
    "id": "eViQAp_AGD6j"
   },
   "outputs": [],
   "source": [
    "#@markdown 🔭🌕 Galileo -- Log Input Data\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Image as datasetsImage\n",
    "from datasets import DatasetDict\n",
    "from datasets.features.features import ClassLabel\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "STANDARD_DATA_COLUMNS_CV = [\"id\", \"text\", \"label_idx\", \"path\"]\n",
    "\n",
    "class ImageDatasetFromLocal(TorchDataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        split: str,\n",
    "        imgs_dir: str,\n",
    "        transform: transforms.Compose = None, \n",
    "        class_labels: ClassLabel = None\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hf: a HuggingFace dataset\n",
    "          split: the split for the hf dataset\n",
    "          transform [Optional]: a transform to apply to the images dynamically \n",
    "            before training\n",
    "          class_labels [Optional]: the ClassLabel object containing the list of \n",
    "            labels and the method to convert between label (string) and \n",
    "            label_idx (int). To insure consistency pass the class_labels of the\n",
    "            training dataset to the test/val datasets.\n",
    "        \"\"\"\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.ds = pd.read_csv(f\"{self.imgs_dir}/{split}.csv\")\n",
    "\n",
    "        # Find the label column name: could be label, labels, coarse_label, etc.\n",
    "        self.label_col_name = find_label_col_name(self.ds.columns)\n",
    "        if self.label_col_name is None:\n",
    "            raise ValueError(f\"Could not find the label column in the dataframe\")\n",
    "        STANDARD_DATA_COLUMNS_CV.append(self.label_col_name)\n",
    "\n",
    "        # Set the list of labels for this split.\n",
    "        self.class_labels = class_labels\n",
    "        if self.class_labels is None:\n",
    "            self.class_labels = list(self.ds[self.label_col_name].unique())\n",
    "        self.list_of_labels = self.class_labels\n",
    "        if split == \"train\":\n",
    "            dq.set_labels_for_run(self.list_of_labels)\n",
    "\n",
    "        # Add column with labels as string (for dq).\n",
    "        label_to_labelidx = {label:i for i, label in enumerate(self.list_of_labels)}\n",
    "        self.ds[\"label_idx\"] =  self.ds[self.label_col_name].map(label_to_labelidx)\n",
    "\n",
    "        # Find the id column, or create it if it doesn't exist.\n",
    "        if \"id\" not in self.ds.columns:\n",
    "            self.ds = self.ds.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "        # Get the metadata columns.\n",
    "        meta_data_cols = [\n",
    "            column\n",
    "            for column in self.ds.columns\n",
    "            if column not in STANDARD_DATA_COLUMNS_CV\n",
    "        ]\n",
    "\n",
    "#         bucket_prefix = \"gs://galileo-public-data/CV_datasets/ImageNet15_animals/\"\n",
    "        # 🔭🌕 Galileo logging -- Log Input Data\n",
    "        self.ds[\"text\"] = self.ds[\"path\"].apply(lambda x: f\"{self.imgs_dir}/{x}\")\n",
    "        dq.log_image_dataset(\n",
    "            dataset=self.ds,\n",
    "            label=self.label_col_name,\n",
    "            split=split,\n",
    "            meta=meta_data_cols,\n",
    "            imgs_location_colname=\"text\",\n",
    "            imgs_dir=self.imgs_dir,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.ds.loc[idx]\n",
    "        img_path = os.path.join(self.imgs_dir, row[\"path\"])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label, id = row[\"label_idx\"], row[\"id\"]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return {\"image\": image, \"label\": label, \"id\": id}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ADbbMqzxnGjt",
   "metadata": {
    "cellView": "form",
    "id": "ADbbMqzxnGjt"
   },
   "outputs": [],
   "source": [
    "#@markdown Create the Dataset and DataLoader\n",
    "\n",
    "# Create the Datasets.\n",
    "image_crop_size = (224, 224)\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_crop_size[0], image_crop_size[1])),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "train_transforms = transforms.Compose(val_transforms.transforms + [transforms.RandomHorizontalFlip()])\n",
    "\n",
    "TRAIN_SPLIT_NAME = \"train\"\n",
    "train_dataset = ImageDatasetFromLocal(imgs_dir=f\"./tmp/content/{dataset_name.split('/')[-1]}\", split=TRAIN_SPLIT_NAME, transform=train_transforms)\n",
    "VAL_SPLIT_NAME = \"validation\" # this var is needed in dq.set_split down below\n",
    "# test_dataset = ImageDatasetFromHF(data, split=VAL_SPLIT_NAME, transform=val_transforms, class_labels=train_dataset.class_labels)\n",
    "\n",
    "print(f\"Loaded {TRAIN_SPLIT_NAME} dataset with {len(train_dataset.ds)} samples and {len(train_dataset.list_of_labels)} labels\")\n",
    "# print(f\"Loaded {VAL_SPLIT_NAME} dataset with {len(test_dataset.ds)} samples and  {len(test_dataset.list_of_labels)} labels\")\n",
    "\n",
    "\n",
    "# Create the DataLoaders.\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_WORKERS = 0 \n",
    "SEED_WORKER = 42\n",
    "\n",
    "seed_all(SEED_WORKER)\n",
    "\n",
    "train_dataloader = TorchDataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    worker_init_fn=seed_worker,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# We're not doing test in this example.\n",
    "test_dataloader=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e65634",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7-Xf3dvhm-HX",
   "metadata": {
    "cellView": "form",
    "id": "7-Xf3dvhm-HX"
   },
   "outputs": [],
   "source": [
    "#@markdown Visualize the Data.\n",
    "# Visualizing a few images of the dataset (post-processing/augmentation)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "idxs = [random.randint(0, len(train_dataset) -1) for _ in range(20)]\n",
    "grid_img = make_grid([train_dataset[idx][\"image\"] for idx in idxs], nrow=5)\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "of4EhRuHOF3m",
   "metadata": {
    "id": "of4EhRuHOF3m"
   },
   "source": [
    "# 6. Log model data with Galileo\n",
    "\n",
    "Model data is logged by wrapping the model with `watch` function. This step will log the model logits and embeddings. You can achieve this by adding 1 line of code to the standard pytorch model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dq-8r96TIduv",
   "metadata": {
    "id": "dq-8r96TIduv"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, resnet50, resnet18\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "# Load model and replace last layer.\n",
    "model = resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(train_dataset.list_of_labels))\n",
    "torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set optimizer and loss.\n",
    "params_1x = [  # get the original weights, they'll be updated with a lower learning rate\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if \"fc\" not in str(name)\n",
    "]\n",
    "lr, weight_decay = 1e-5, 5e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": params_1x, \"lr\": lr},\n",
    "        {\"params\": model.fc.parameters(), \"lr\": lr * 10},\n",
    "    ],\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "from dataquality.integrations.torch import watch\n",
    "\n",
    "# 🔭🌕 Galileo logging -- Log Embeddings\n",
    "watch(\n",
    "    model=model,\n",
    "    classifier_layer=model.fc,\n",
    "    dataloaders=[train_dataloader],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knpdaq7YBmmo",
   "metadata": {
    "id": "knpdaq7YBmmo"
   },
   "source": [
    "# 7. Putting into Action: Training a Model\n",
    "\n",
    "We complete the training pipeline by using a standard PyTorch training setup. While training, we log the current `epoch` and `split`. To complete logging, we call `dq.finish()` after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2kT2ulkjFArn",
   "metadata": {
    "id": "2kT2ulkjFArn"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep, time\n",
    "\n",
    "# Train !\n",
    "start = time()\n",
    "print(f\"Training for {EPOCHS} epochs on {device}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    dq.set_epoch(epoch)  # 🔭🌕 Galileo -- Set split\n",
    "\n",
    "    model.train()\n",
    "    train_loss = torch.tensor(0.0, device=device)\n",
    "    train_correct = torch.tensor(0, device=device)\n",
    "    \n",
    "    dq.set_split(TRAIN_SPLIT_NAME)\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as train_minibatchs:\n",
    "        for train_minibatch in train_minibatchs:\n",
    "            train_minibatchs.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            images = train_minibatch[\"image\"].to(device)\n",
    "            labels = train_minibatch[\"label\"].to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                train_loss += loss\n",
    "                train_batch_correct = (torch.argmax(preds, dim=1) == labels).sum()\n",
    "                train_correct += train_batch_correct\n",
    "\n",
    "            train_minibatchs.set_postfix(batch_loss=loss.item(), batch_accuracy=float(train_batch_correct) / BATCH_SIZE)\n",
    "            sleep(0.01)\n",
    "\n",
    "    print(f\"Training loss: {train_loss:.2f}\")\n",
    "    print(f\"Training accuracy: {100 * float(train_correct) / len(train_dataloader.dataset):.2f}\")\n",
    "    \n",
    "    dq.set_split(VAL_SPLIT_NAME)  # 🔭🌕 Galileo -- Set split\n",
    "    if test_dataloader is not None:\n",
    "        model.eval()\n",
    "        val_loss = torch.tensor(0.0, device=device)\n",
    "        val_correct = torch.tensor(0, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_minibatch in tqdm(test_dataloader):\n",
    "                images = val_minibatch[\"image\"].to(device)\n",
    "                labels = val_minibatch[\"label\"].to(device)\n",
    "                \n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, labels)\n",
    "\n",
    "                val_loss += loss\n",
    "                val_correct += (torch.argmax(preds, dim=1) == labels).sum()\n",
    "\n",
    "        print(f\"{VAL_SPLIT_NAME} loss: {val_loss:.2f}\")\n",
    "        print(f\"{VAL_SPLIT_NAME} accuracy: {100*val_correct/len(test_dataloader.dataset):.2f}\")\n",
    "\n",
    "end = time()\n",
    "print(f\"Total training time: {end-start:.1f} seconds\")\n",
    "dq.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xRUq_rJ3Aiow",
   "metadata": {
    "id": "xRUq_rJ3Aiow"
   },
   "source": [
    "# General Help and Docs\n",
    "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
    "- To see more general data and model logging docs, run `dq.docs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cp76O3rIAiBE",
   "metadata": {
    "id": "cp76O3rIAiBE"
   },
   "outputs": [],
   "source": [
    "dq.get_data_logger().doc()\n",
    "help(dq.log_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
