{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4k3q4W0CmVp"
      },
      "source": [
        "# LLM Fine-Tuning using HuggingFace's ðŸ¤— Encoder-Decoder models and ðŸ”­ Galileo\n",
        "\n",
        "In this tutorial, we will fine-tune an Encoder-Decoder model from HuggingFace ðŸ¤— for instruction completion and explore the results in Galileo.\n",
        "\n",
        "We use the well known Alpaca intruction-tuning dataset, from the [Stanford Alpaca project](https://github.com/tatsu-lab/stanford_alpaca). In doing so, we help highlight several known data errors and limitations of this dataset!\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6DPqgz0c8pW"
      },
      "source": [
        "# Install Dependancies [Including Setting up DQ] + Add Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z1hro7XtGD-j"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality`\n",
        "\n",
        "# Upgrade pip\n",
        "!pip install -U pip &> /dev/null\n",
        "\n",
        "# Install all dependecies\n",
        "!pip install 'dataquality[cuda]' --extra-index-url=https://pypi.nvidia.com/\n",
        "print('ðŸ‘‹ Installed necessary libraries.')\n",
        "\n",
        "# Select a small portion of the dataset for CI/QA.\n",
        "import os\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEaEd_zAHpDm"
      },
      "source": [
        "# 1. Initialize Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anwDG2zLHrjf"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "# ðŸ”­ðŸŒ• Galileo log-in\n",
        "\n",
        "dq.init(task_type=\"seq2seq\",\n",
        "        project_name=\"galileo-finetune\",\n",
        "        run_name=f\"example_run_galileo-finetune_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXFUYBuSJRTJ"
      },
      "source": [
        "# 2. Load Data\n",
        "Load the data from Hugging Face and format it for fine-tuning an Encoder-Decoder model. Additionally, the original Alpaca dataset does not specify a val/test split so we randomly sample to get train/val/test with the ratios (0.85, 0.1, 0.05).\n",
        "\n",
        "NOTE: We are working with LLMs (emphasis on Large) and Alpaca is a decently sized dataset with 52,000 data samples. Therefore, training times can be large. To speed up training during this tutorial, consider setting the flag `use_small_ds = True`. This will downsample the Alpaca dataset to 2500 samples before splitting into train/val/test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jLkoam2p-ZV"
      },
      "outputs": [],
      "source": [
        "use_small_ds = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6F2mUW-wJW4G"
      },
      "outputs": [],
      "source": [
        "#@title Load ðŸ¤— HuggingFace Alpaca Dataset\n",
        "\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "ds = load_dataset(\"tatsu-lab/alpaca\", trust_remote_code=True)\n",
        "\n",
        "if use_small_ds or _minimize_for_ci():\n",
        "    total_n_samples = 50 if _minimize_for_ci() else 10_000\n",
        "    ds = DatasetDict({'train': Dataset.from_dict(ds['train'][:total_n_samples])})\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fZzfgVm-J_AR"
      },
      "outputs": [],
      "source": [
        "#@title Format the Dataset For Encoder-Decoder Fine-Tuning\n",
        "#@markdown We use the following data format to combine the `Instruction` and `Input` columns: ```Human: {instruction} Input: {input}```\n",
        "\n",
        "# FORMAT ALPACA\n",
        "def format_alpaca(example, idx):\n",
        "  return {\"formatted_input\": f\"Human: {example['instruction']}\" + f\" Context: {example['input']}\"*bool(example['input']),\n",
        "          \"id\": idx}\n",
        "\n",
        "if \"formatted_input\" not in ds['train'].features:\n",
        "  ds = ds.map(format_alpaca, with_indices=True, remove_columns=['text', 'instruction', 'input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zczZJEtfVG0W"
      },
      "outputs": [],
      "source": [
        "#@title Split the data into train/val/test splits as (0.85/0.1/0.05)\n",
        "#@markdown The original Alpaca dataset does not come with a designated val/test split so we randomly sample to create these.\n",
        "\n",
        "if 'val' not in ds and 'valid' not in ds and 'validation' not in ds:\n",
        "  ds = ds.shuffle(seed=8)\n",
        "\n",
        "  num_samples = len(ds['train'])\n",
        "  train_size = int(num_samples * 0.85)\n",
        "  val_size = int(num_samples * 0.1)\n",
        "\n",
        "  ds_train = Dataset.from_dict(ds['train'][:train_size])\n",
        "  ds_val = Dataset.from_dict(ds['train'][train_size:train_size + val_size])\n",
        "  ds_test = Dataset.from_dict(ds['train'][train_size + val_size:])\n",
        "\n",
        "  ds = DatasetDict({\n",
        "      'train': ds_train,\n",
        "      'val': ds_val,\n",
        "      'test': ds_test\n",
        "  })\n",
        "\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K6C-PReO66q"
      },
      "source": [
        "# 3. Setup Logging with Galileo\n",
        "Galileo \"watches\" (i.e. uses) your `model`, `tokenizer`, and `GenerationConfig` to aid in logging + computing token level statistics and to power generation after training.\n",
        "\n",
        "In this tutorial, we use the Encoder-Decoder model [`google/flan-t5-small`](https://huggingface.co/google/flan-t5-small) and leverage a simple greedy decoding strategy.\n",
        "\n",
        "To speed up training and reduce memory, we limit the `max_output_tokens` (for the decoder block) to `128`, while leaving `max_input_tokens` (for the encoder block) as the default 512. Feel free to change this to reduce the samples with truncation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDF9O9lAO59P"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, GenerationConfig, T5ForConditionalGeneration\n",
        "from dataquality.integrations.seq2seq.core import watch\n",
        "from dataquality.schemas.seq2seq import Seq2SeqModelType\n",
        "\n",
        "# Load model and tokenizer\n",
        "MODEL = \"google/flan-t5-small\"\n",
        "MAX_INPUT_TOKENS = 512\n",
        "MAX_TARGET_TOKENS = 128\n",
        "\n",
        "# Generation\n",
        "MAX_NEW_TOKENS = 128\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True, model_max_length=MAX_INPUT_TOKENS)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    do_sample=False,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "watch(\n",
        "    model_type=Seq2SeqModelType.encoder_decoder.value,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        "    # generation_splits=[\"test\"],  # ðŸ”­ðŸŒ• Galileo generates on Test set only by default\n",
        "    max_input_tokens=MAX_INPUT_TOKENS,\n",
        "    max_target_tokens=MAX_TARGET_TOKENS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1umfIXJT2YE"
      },
      "source": [
        "# 4. Log input data with Galileo\n",
        "Input data can be logged via `log_dataset` for logging iterables such as `HuggingFace` or `Pytorch Dataset` classes. This step logs the `input` (referenced as the `text`) and `target` (referenced as `label`) data columns for each split.\n",
        "\n",
        "In our setting we have the following data columns:\n",
        "- `text`/`input` = `formatted_input`\n",
        "- `label`/`target` = `output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwsmIRciSwZf"
      },
      "outputs": [],
      "source": [
        "# Log datasets with dq\n",
        "from functools import partial\n",
        "\n",
        "def _log_dataset(_ds, split, input_col, target_col):\n",
        "    dq.log_dataset(\n",
        "        _ds,\n",
        "        text=input_col,\n",
        "        label=target_col,\n",
        "        split=split\n",
        "    )\n",
        "\n",
        "log_dataset = partial(_log_dataset, input_col=\"formatted_input\", target_col=\"output\")\n",
        "\n",
        "\n",
        "# Log just for training\n",
        "log_dataset(ds['train'], split=\"training\")\n",
        "log_dataset(ds['val'], split='validation')\n",
        "log_dataset(ds['test'], split='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E64UocQbawCo"
      },
      "source": [
        "# 5. Tokenize and Format Data for Training\n",
        "\n",
        "Here we tokenize the data and create per-split DataLoaders.\n",
        "\n",
        "One important parameter to control is `BATCH_SIZE` and the `ACCUMULATION_STEPS`. To alleviate GPU memory limitations we use by default a `BATCH_SIZE=8` and `ACCUMULATION_STEPS=4` to have an `EFFECTIVE_BATCH_SIZE=32`.\n",
        "\n",
        "These parameter setting require around `4 GB` of GPU RAM. If you run into RAM issues or if you are hungry to use more RAM feel free to change these setting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pad_nnaac-eN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "ACCUMULATION_STEPS = 4  # Effective batch size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kWtZwOeVa4q_"
      },
      "outputs": [],
      "source": [
        "#@title Tokenize Inputs and Targets\n",
        "def tokenize(row, input_col, target_col, max_input_length=512, max_target_length=512):\n",
        "  \"\"\"Tokenize the inputs and targets\n",
        "\n",
        "  Creates the following columns:\n",
        "    - input_ids\n",
        "    - attention_mask\n",
        "    - labels\n",
        "\n",
        "  Note: We keep the id column for use in Galileo logging\n",
        "  \"\"\"\n",
        "  model_inputs = tokenizer(\n",
        "        row[input_col],\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "  labels = tokenizer(\n",
        "        row[target_col],\n",
        "        truncation=True,\n",
        "        max_length=max_target_length,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    ).input_ids\n",
        "\n",
        "  model_inputs['labels'] = labels\n",
        "  model_inputs['id'] = row['id']\n",
        "  return model_inputs\n",
        "\n",
        "\n",
        "ds_tokenized = ds.map(lambda x: tokenize(x, input_col=\"formatted_input\", target_col=\"output\", max_input_length=MAX_INPUT_TOKENS, max_target_length=MAX_TARGET_TOKENS),\n",
        "                      remove_columns=ds['train'].column_names,\n",
        "                      batched=True,\n",
        "                      desc=\"Tokenizing Datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tTSb6kkvcONf"
      },
      "outputs": [],
      "source": [
        "#@title Setup the dataloaders\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "train_dataset = ds_tokenized[\"train\"]\n",
        "eval_dataset = ds_tokenized['val']\n",
        "test_dataset = ds_tokenized['test']\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, shuffle=False, collate_fn=data_collator, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, collate_fn=data_collator, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "evaluation_dataloaders = {\n",
        "    'validation': eval_dataloader,\n",
        "    'test': test_dataloader\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTiIJBbeZgp0"
      },
      "source": [
        "# 6. Putting it all together - Training a Model!\n",
        "\n",
        "Now we put it all together and train our model while logging to Galileo. This can be achieved with only 2 key additional lines of code:\n",
        "1.  `dq.log_model_outputs`: Model \"data\" (i.e. logits) can be logged via `log_model_outputs` during the training and evaluation process.\n",
        "2. `dq.set_epoch_and_split(split=<split>, epoch=<epoch>)`: Before logging model data for a given split we must indicate to Galileo which split and epoch we are logging for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16yzukX8ev5a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "LR = 3e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Ta2e9LOeNW4"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to activate Tensorboard\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=/content/runs --load_fast=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM2hApN5emI5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import Adafactor\n",
        "from tqdm import tqdm\n",
        "\n",
        "writer = SummaryWriter()\n",
        "writer.add_custom_scalars({\n",
        "    \"Losses\": {\n",
        "        \"loss\": [\"Multiline\", [\"loss/train\", \"loss/validation\", \"loss/test\"]]\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# training and evaluation\n",
        "model = model.to(device)\n",
        "optimizer = Adafactor(model.parameters(), lr=LR, scale_parameter=False, relative_step=False)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # ðŸ”­ðŸŒ• Galileo set epoch and split\n",
        "    dq.set_epoch_and_split(split=\"training\", epoch=epoch)\n",
        "    model.train()\n",
        "\n",
        "    train_epoch_loss = 0.\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "      ids = batch['id']\n",
        "      batch = {k: v.to(device) for k, v in batch.items() if k != 'id'}\n",
        "\n",
        "      outputs = model(**batch)\n",
        "\n",
        "      # ðŸ”­ðŸŒ• Galileo logging\n",
        "      logits = outputs.logits  # Shape - [bs, bs_seq_ln, vocab]\n",
        "      dq.log_model_outputs(\n",
        "        logits = logits.cpu().numpy(),\n",
        "        ids = ids\n",
        "      )\n",
        "\n",
        "      loss = outputs.loss / ACCUMULATION_STEPS\n",
        "\n",
        "      loss.backward()\n",
        "      # Grad Accumulation\n",
        "      if ((step + 1) % ACCUMULATION_STEPS == 0) \\\n",
        "          or ((step + 1) == len(train_dataloader)):\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      step_loss = loss.detach().cpu().item()\n",
        "      train_epoch_loss += step_loss\n",
        "      writer.add_scalar(\"Loss/train\", step_loss, global_step=epoch*len(train_dataloader) + step) # Per step loss tracking\n",
        "\n",
        "    train_epoch_loss = train_epoch_loss / len(train_dataloader) * ACCUMULATION_STEPS # Correct for the constant factor\n",
        "    print(f\"{epoch=}: {train_epoch_loss=}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for eval_split, dataloader in evaluation_dataloaders.items():\n",
        "        eval_epoch_loss = 0\n",
        "        dq.set_epoch_and_split(split=eval_split, epoch=epoch)\n",
        "        for step, batch in enumerate(tqdm(dataloader, desc=f\"Evaluation on split: {eval_split}\")):\n",
        "            ids = batch['id']\n",
        "            batch = {k: v.to(device) for k, v in batch.items() if k != 'id'}\n",
        "\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            # ðŸ”­ðŸŒ• Galileo logging\n",
        "            logits = outputs.logits  # Shape - [bs, bs_seq_ln, vocab]\n",
        "            dq.log_model_outputs(\n",
        "              logits = logits.cpu().numpy(),\n",
        "              ids = ids\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            eval_step_loss = loss.cpu().item()\n",
        "            eval_epoch_loss += eval_step_loss\n",
        "\n",
        "      # Look just at the loss in aggregate!\n",
        "      eval_epoch_loss = eval_epoch_loss / len(eval_dataloader)\n",
        "      writer.add_scalar(f\"Loss/{eval_split}\", eval_epoch_loss, global_step=epoch)\n",
        "\n",
        "      print(f\"{eval_split}: {eval_epoch_loss=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cEeeT5aRBGP"
      },
      "source": [
        " # 6. Wrapping up - Pushing Data to Galileo\n",
        "\n",
        "Now that we have finished training, the final step is to call `dq.finish()`, which kicks off data processing and uploads results to Galileo!\n",
        "\n",
        "\n",
        "Some special things to note:\n",
        "1. As a reminder, internally, Galileo leverage's ðŸ¤— HuggingFace's standardized `generation` workflow to generate model completions over specified data splits. This is why we have you log your `GenerationConfig`!\n",
        "2. `dq.finish()` requires some additional GPU RAM (about `0.5-1` GB from `generation` and some other processes). Therefore, it is very helpful to clear out any unused GPU RAM to avoid memory issues. In general, deleting the `optimizer` should suffice (though you can delete all unused GPU data), which we demonstrate below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIaBiLyUtwMV"
      },
      "outputs": [],
      "source": [
        "del optimizer\n",
        "del batch\n",
        "del outputs\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWOQQblbt8wD"
      },
      "outputs": [],
      "source": [
        "dq.finish(data_embs_col=\"input\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "x6DPqgz0c8pW",
        "MXFUYBuSJRTJ",
        "T1umfIXJT2YE"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
