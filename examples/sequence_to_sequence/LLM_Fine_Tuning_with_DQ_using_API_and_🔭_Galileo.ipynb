{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6R5O5djSCqi"
      },
      "outputs": [],
      "source": [
        "#@title Install dependancies such as `dataquality`\n",
        "\n",
        "# Upgrade pip\n",
        "!pip install -U pip &> /dev/null\n",
        "\n",
        "# Install all dependecies\n",
        "!pip install tokenizers cohere\n",
        "!pip install 'dataquality[cuda]' --extra-index-url=https://pypi.nvidia.com/\n",
        "\n",
        "print('ðŸ‘‹ Installed necessary libraries.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQIK2m_ya9wb"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "from typing import Any, Dict, List, Optional\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import torch\n",
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from datasets import Dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "import cohere\n",
        "from cohere import AsyncClient\n",
        "\n",
        "import dataquality as dq\n",
        "from dataquality.integrations.seq2seq.core import watch\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R0hyp8YGmyN"
      },
      "source": [
        "## Loading and Expanding Chat Data\n",
        "\n",
        "Below we include helper functions for loading your Chat data and \"expanding\" out the conversational turns into individual chat completion samples. The \"expanded\" dataset has one line per `User` + `Chatbot` pair, stored in columns `input` and `target`.\n",
        "\n",
        "**Note** to handle chat history, we include historical turns in the `input` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSVKHO-lEmt4"
      },
      "outputs": [],
      "source": [
        "#@title Formatting Helpers\n",
        "\n",
        "def format_sample(\n",
        "    sample: Dict[str, Any],\n",
        "    user_role: str = \"User\",\n",
        "    chatbot_role: str = \"Chatbot\",\n",
        "    idx: Optional[int] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Formats a chat dataset for seq2seq\n",
        "\n",
        "    Takes in a sample with \"turns\" column and explodes it to have one row\n",
        "    per turn.\n",
        "\n",
        "    Example:\n",
        "        >>> sample = {\n",
        "        ...     \"turns\": [\n",
        "        ...         {\"role\": \"User\", \"content\": \"Hello\"},\n",
        "        ...         {\"role\": \"Chatbot\", \"content\": \"Hi\"},\n",
        "        ...         {\"role\": \"User\", \"content\": \"How are you?\"},\n",
        "        ...         {\"role\": \"Chatbot\", \"content\": \"I'm good, how are you?\"},\n",
        "        ...     ],\n",
        "        ...     \"metadata\": {\"unique_id\": 1234, \"dataset\": \"test\"},\n",
        "        ...     \"score\": 0.5,\n",
        "        ... }\n",
        "        >>> ChatFormatter().format_sample(sample, 5)\n",
        "        {\n",
        "            \"chat_id\": [5, 5],\n",
        "            \"turn_id\": [1, 2],\n",
        "            \"input\": [\"Hello\", \"How are you?\"],\n",
        "            \"target\": [\"Hi\", \"I'm good, how are you?\"],\n",
        "            \"unique_id\": [1234, 1234],\n",
        "            \"dataset\": [\"test\", \"test\"],\n",
        "        }\n",
        "    \"\"\"\n",
        "    unraveled_turns: Dict[str, Any] = defaultdict(list)\n",
        "    valid_meta_types = (str, int, float, bool)\n",
        "    turns: List[Dict[str, Any]] = sample[\"turns\"]\n",
        "\n",
        "    # # Add metadata and sample level cols to each turn\n",
        "    metadata: Dict[str, Any] = sample.get(\"metadata\", {})\n",
        "    for k, v in sample.items():\n",
        "        if k not in [\"metadata\", \"turns\", \"id\"]:\n",
        "            metadata[k] = v\n",
        "\n",
        "    chat_history = \"\"\n",
        "    turn_data: Dict[str, Any] = {\"chat_id\": None, \"turn_id\": None}\n",
        "    turn_id = 1\n",
        "    turn_default_cols = [\"role\", \"content\"]\n",
        "    for turn in turns:\n",
        "        role = turn[\"role\"]\n",
        "        content = turn[\"content\"]\n",
        "\n",
        "        # Add metadata to each turn\n",
        "        turn_meta = {\n",
        "            f\"{role}_{col}\": turn[col]\n",
        "            for col in turn.keys()\n",
        "            if col not in turn_default_cols\n",
        "            and isinstance(turn[col], valid_meta_types)\n",
        "        }\n",
        "        turn_data.update(turn_meta)\n",
        "\n",
        "        if role == user_role:\n",
        "            # Add in the history\n",
        "            new_line = \"\\n\"\n",
        "            if chat_history == \"\":\n",
        "                new_line = \"\"\n",
        "            turn_data[\"input\"] = f\"\"\"{chat_history}{new_line}{user_role}: {content}\"\"\"\n",
        "        elif role == chatbot_role:\n",
        "            turn_data[\"target\"] = content\n",
        "            turn_data[\"turn_id\"] = turn_id\n",
        "            turn_data[\"chat_id\"] = idx\n",
        "\n",
        "            turn_data.update(metadata)\n",
        "            for k, v in turn_data.items():\n",
        "                unraveled_turns[k].append(v)\n",
        "\n",
        "            # Update chat history - Note that the formatted input contains the history!\n",
        "            chat_history = f\"\"\"{turn_data['input']}\\{chatbot_role}: {content}\"\"\"\n",
        "\n",
        "            # Reset turn data\n",
        "            turn_data = {}\n",
        "            turn_id += 1\n",
        "        else:\n",
        "            raise ValueError(f\"Role {role} not recognized\")\n",
        "\n",
        "    return unraveled_turns\n",
        "\n",
        "@dataclass\n",
        "class BatchData:\n",
        "    batch: Dict[str, Any]\n",
        "\n",
        "    def sample_from_idx(self, batch_idx: int) -> Dict[str, Any]:\n",
        "        \"\"\"Gets a subset of the batch\"\"\"\n",
        "        sample = {}\n",
        "        for k, v in self.batch.items():\n",
        "            sample[k] = v[batch_idx]\n",
        "        return sample\n",
        "\n",
        "def expand_batch(batch: Dict[str, List], idxs: List[int]) -> Dict[str, List]:\n",
        "    \"\"\"Formats a batch of chat data for seq2seq\"\"\"\n",
        "    result: Dict[str, List] = defaultdict(list)\n",
        "    batch_data = BatchData(batch)\n",
        "    batch_sz = len(idxs)\n",
        "    for idx in idxs:\n",
        "        batch_idx = idx % batch_sz\n",
        "        formatted_sample = format_sample(\n",
        "            batch_data.sample_from_idx(batch_idx),\n",
        "            idx=idx,\n",
        "        )\n",
        "        # formatted_sample returns one or more samples per idx, we add to result\n",
        "        for k, v in formatted_sample.items():\n",
        "            result[k] += v\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaE-uQixEsne"
      },
      "outputs": [],
      "source": [
        "train_ds = pd.read_json('chat.jsonl', lines=True)\n",
        "train_ds = Dataset.from_pandas(train_ds)\n",
        "train_ds_expanded = train_ds.map(expand_batch, batched=True, remove_columns=train_ds.column_names, with_indices=True)\n",
        "train_ds_expanded\n",
        "\n",
        "# TODO: Add if available\n",
        "# val_ds = pd.read_json('...', lines=True)\n",
        "# val_ds = Dataset.from_pandas(val_ds)\n",
        "# val_ds_expanded = val_ds.map(expand_batch, batched=True, remove_columns=val_ds.column_names, with_indices=True)\n",
        "\n",
        "# test_ds = pd.read_json('...', lines=True)\n",
        "# test_ds = Dataset.from_pandas(test_ds)\n",
        "# test_ds_expanded = test_ds.map(expand_batch, batched=True, remove_columns=test_ds.column_names, with_indices=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w9yYMhuTAJb"
      },
      "source": [
        "## Formatting Chat Data Samples\n",
        "\n",
        "Next we format each Chat Sample based on the *Input* and *Target* columns generated from above. We call this the **formatted_prompt** and ensure that it follows the expected input format for your generation API\n",
        "\n",
        "This format is as follows:\n",
        "```\n",
        "User: q_1\n",
        "Chatbot: a_1\n",
        "...\n",
        "User: q_n\n",
        "Chatbot:<EOP_TOKEN> a_n\n",
        "```\n",
        "Essentially we combine the `input` and `target` columns using the following rule:\n",
        "```\n",
        "{input}\\nChatbot:<EOP_TOKEN> {target}\n",
        "```\n",
        "\n",
        "**Note**: One important variable that you will see in the `Prompt Formatting Function` section below is `response_template`. This provides Galileo the `seperator` between the model input and desired completion, so during processing we can just focus on the model's completion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIvmFcf7U4RI"
      },
      "outputs": [],
      "source": [
        "#@title Prompt Formatting Function\n",
        "\n",
        "response_template = \"Chatbot:<EOP_TOKEN>\"\n",
        "def create_formatted_prompt_chat(\n",
        "    row: Dict,\n",
        "    idx: int\n",
        ") -> Dict:\n",
        "    # Assuming completion data\n",
        "    formatted_prompt = f\"\"\"{row['input']}\\nChatbot:<EOP_TOKEN> {row['target']}\"\"\"\n",
        "    return {\"formatted_prompt\": formatted_prompt, \"id\": idx}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6x0-gZAUrkX"
      },
      "outputs": [],
      "source": [
        "train_ds_formatted = train_ds_expanded.map(create_formatted_prompt_chat, with_indices=True)\n",
        "train_ds_formatted\n",
        "# NOTE: If including val / test data, uncomment the below lines\n",
        "# val_ds_formatted = val_ds_expanded.map(create_formatted_prompt_chat, with_indices=True)\n",
        "# test_ds_formatted = test_ds_expanded.map(create_formatted_prompt_chat, with_indices=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhDMrbNXV2cQ"
      },
      "source": [
        "## API Helper Functions\n",
        "\n",
        "Below are a number of helper functions for calling the Cohere async APIs (using raw_prompting) and processing the returned output. The primary thing that we are interested in are the `token likelihoods` and the `sample embeddings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0zCn0ezE_7r"
      },
      "outputs": [],
      "source": [
        "#@title `AsyncClient.generate` with `raw_prompting`\n",
        "\n",
        "a_co = AsyncClient('') # TODO: add key\n",
        "\n",
        "async def co_generate(\n",
        "    prompt: Optional[str] = None,\n",
        "    prompt_vars: object = {},\n",
        "    model: Optional[str] = None,\n",
        "    preset: Optional[str] = None,\n",
        "    num_generations: Optional[int] = None,\n",
        "    max_tokens: Optional[int] = None,\n",
        "    temperature: Optional[float] = None,\n",
        "    k: Optional[int] = None,\n",
        "    p: Optional[float] = None,\n",
        "    frequency_penalty: Optional[float] = None,\n",
        "    presence_penalty: Optional[float] = None,\n",
        "    end_sequences: Optional[List[str]] = None,\n",
        "    stop_sequences: Optional[List[str]] = None,\n",
        "    return_likelihoods: Optional[str] = None,\n",
        "    truncate: Optional[str] = None,\n",
        "    logit_bias: Dict[int, float] = {},\n",
        "    raw_prompting: bool = False,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Overwrites `AsyncClient.generate` to we can use the internal `raw_prompting` argument.\n",
        "    \"\"\"\n",
        "    json_body = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"prompt_vars\": prompt_vars,\n",
        "        \"preset\": preset,\n",
        "        \"num_generations\": num_generations,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"k\": k,\n",
        "        \"p\": p,\n",
        "        \"frequency_penalty\": frequency_penalty,\n",
        "        \"presence_penalty\": presence_penalty,\n",
        "        \"end_sequences\": end_sequences,\n",
        "        \"stop_sequences\": stop_sequences,\n",
        "        \"return_likelihoods\": return_likelihoods,\n",
        "        \"truncate\": truncate,\n",
        "        \"logit_bias\": logit_bias,\n",
        "        \"stream\": False,\n",
        "        \"raw_prompting\": raw_prompting,\n",
        "    }\n",
        "    response = await a_co._request(cohere.GENERATE_URL, json=json_body, stream=False)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JO9508IFCZg"
      },
      "outputs": [],
      "source": [
        "#@title API Response Processors\n",
        "\n",
        "async def a_query_batch(prompts: List[str]) -> torch.Tensor:\n",
        "    response_jobs = []\n",
        "    for prompt in prompts:\n",
        "        # For now append <BOS_TOKEN> and <EOS_TOKEN> - NOTE DQ TOKENIZER THINKS IT IS EOP\n",
        "        prompt = f\"\"\"<BOS_TOKEN>{prompt}<EOS_TOKEN>\"\"\"\n",
        "        response_job = co_generate(\n",
        "            prompt = prompt,\n",
        "            return_likelihoods = \"ALL\",\n",
        "            raw_prompting = True,\n",
        "            max_tokens = 0\n",
        "        )\n",
        "        response_jobs.append(response_job)\n",
        "\n",
        "    responses = await asyncio.gather(*response_jobs)\n",
        "    logprob_responses = []\n",
        "    for response in responses:\n",
        "        logprobs = [token['likelihood'] for token in response['generations'][0]['token_likelihoods']]\n",
        "        logprob_responses.append(torch.Tensor(logprobs))\n",
        "\n",
        "    # Pad to the max sequence length in the batch\n",
        "    logprob_responses = torch.nn.utils.rnn.pad_sequence(logprob_responses, batch_first=True)\n",
        "    return logprob_responses\n",
        "\n",
        "async def batch_embedd(prompts: List[str]) -> torch.Tensor:\n",
        "    \"\"\"Embedd the batch of FULL prompts using Cohere API\n",
        "\n",
        "    For each sample we generate the embedding using the combination of the\n",
        "    (prompt, completion).\n",
        "\n",
        "    Using the embeddings API we set input type to `clustering`.\n",
        "\n",
        "    Note that we must use a batch size < 96? per the API docs\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_response = await a_co.embed(\n",
        "      texts=prompts,\n",
        "      model='embed-english-v3.0',\n",
        "      input_type='clustering'\n",
        "    )\n",
        "    embeddings = torch.Tensor(embedding_response.embeddings)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-efT_DNXUyG"
      },
      "source": [
        "## Galileo Logging!\n",
        "\n",
        "Now we can get to logging with Galileo. This will looks similar to the `dq.auto` flow with a bit of extra added logic. Specifically to:\n",
        "1. Log your formatted dataset\n",
        "2. Log (from your API models) model likelihoods and embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8PXC-fkYkGu"
      },
      "source": [
        "### Logging In and Creating a Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBYNixlFFE_3"
      },
      "outputs": [],
      "source": [
        "# TODO: Update the below\n",
        "os.environ['GALILEO_CONSOLE_URL']=\"\"\n",
        "os.environ[\"GALILEO_USERNAME\"]=\"\"\n",
        "os.environ[\"GALILEO_PASSWORD\"]=\"\"\n",
        "dq.configure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyFq_wp3FOoz"
      },
      "outputs": [],
      "source": [
        "dq.init(\"seq2seq\", project_name=\"Seq2Seq-API-Integration\", run_name=\"My Run\")\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"Cohere/command-nightly\")\n",
        "\n",
        "# Tokenizer the response_template indicating the <Seperator> between\n",
        "# model inputs and target outputs.\n",
        "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False).ids\n",
        "print(f\"Separator template is: {response_template}\")\n",
        "print(f\"This maps to tokenized ids {response_template_ids}\")\n",
        "\n",
        "watch(\n",
        "    tokenizer=tokenizer,\n",
        "    model_type=\"decoder_only\",\n",
        "    generation_splits=[],\n",
        "    max_input_tokens=4096,\n",
        "    response_template=response_template_ids\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AerabnLuYpTB"
      },
      "source": [
        "### Logging Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1MjR3NHFSHw"
      },
      "outputs": [],
      "source": [
        "default_columns = {\n",
        "    \"id\",\n",
        "    \"input\",\n",
        "    \"target\",\n",
        "    \"formatted_prompt\",\n",
        "}\n",
        "\n",
        "\n",
        "def log_dataset(\n",
        "    ds: Dataset,\n",
        "    split: str,\n",
        "    input_col: str = \"input\",\n",
        "    target_col: str = \"target\",\n",
        "    formatted_prompt: str = \"formatted_prompt\"\n",
        ") -> None:\n",
        "    meta = [col for col in ds.features if col not in default_columns]\n",
        "    dq.log_dataset(\n",
        "        ds,\n",
        "        text=input_col,\n",
        "        label=target_col,\n",
        "        formatted_prompt=formatted_prompt,\n",
        "        split=split,\n",
        "        meta=meta\n",
        "    )\n",
        "\n",
        "log_dataset(train_ds_formatted, \"training\")\n",
        "# NOTE: Uncomment the below to include val / test data\n",
        "# log_dataset(val_ds_formatted, \"validation\")\n",
        "# log_dataset(test_ds_formatted, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2hYHfttZhmf"
      },
      "source": [
        "### Logging Model Outputs\n",
        "\n",
        "Logging model outputs involves:\n",
        "1. Iterating over your dataset(s) in batches - controled by the `batch_size` param\n",
        "2. Retrieve likelihoods and embeddings from the respective Cohere API endpoints.\n",
        "3. Logging results to Galileo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INEPFQIsFcDK"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "\n",
        "async def log_model_outputs(ds: Dataset, split) -> None:\n",
        "  \"\"\"Log model outputs in batches\n",
        "\n",
        "  Given the provided dataset split, query in batches\n",
        "  the Cohere `generate` and `embeddings` APIs to log\n",
        "  sample likelihoods and embeddings.\n",
        "  \"\"\"\n",
        "  dq.set_epoch_and_split(epoch=0, split=split)\n",
        "  for i in range(0, len(ds), batch_size):\n",
        "      print (f\"Processing batch {i // batch_size}\")\n",
        "      batch = ds[i: i + batch_size]\n",
        "      batch_ids = batch['id']\n",
        "      batch_model_inputs = batch['formatted_prompt']\n",
        "\n",
        "      print (\"Querying API...\")\n",
        "      logprobs = await a_query_batch(batch_model_inputs)\n",
        "      embeddings = await batch_embedd(batch_model_inputs)\n",
        "      print(\"Done querying! \\n\")\n",
        "\n",
        "      dq.log_model_outputs(\n",
        "          probs = logprobs,\n",
        "          embs = embeddings,\n",
        "          ids = batch_ids,\n",
        "      )\n",
        "\n",
        "response = await log_model_outputs(train_ds_formatted, split=\"training\")\n",
        "# NOTE: Uncomment the below to include val / test data\n",
        "# response = await log_model_outputs(val_ds_formatted, split=\"validation\")\n",
        "# response = await log_model_outputs(test_ds_formatted, split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHhybfopFjls"
      },
      "outputs": [],
      "source": [
        "dq.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
