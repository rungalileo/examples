{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4k3q4W0CmVp"
      },
      "source": [
        "# LLM Fine-Tuning using ðŸ”­ Galileo's auto\n",
        "\n",
        "In this tutorial, we will fine-tune an Encoder-Decoder model from HuggingFace ðŸ¤— for instruction completion and explore the results in Galileo.\n",
        "\n",
        "We use the well known Alpaca intruction-tuning dataset, from the [Stanford Alpaca project](https://github.com/tatsu-lab/stanford_alpaca). In doing so, we help highlight several known data errors and limitations of this dataset!\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6DPqgz0c8pW"
      },
      "source": [
        "# Install Dependancies [Including Setting up DQ] + Add Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z1hro7XtGD-j"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality`\n",
        "\n",
        "# Upgrade pip\n",
        "!pip install -U pip &> /dev/null\n",
        "\n",
        "# Install all dependecies\n",
        "!pip install 'dataquality[cuda]' --extra-index-url=https://pypi.nvidia.com/\n",
        "print('ðŸ‘‹ Installed necessary libraries.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEaEd_zAHpDm"
      },
      "source": [
        "# 1. Initialize Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anwDG2zLHrjf"
      },
      "outputs": [],
      "source": [
        "# ðŸ”­ðŸŒ• Galileo log-in\n",
        "import os\n",
        "\n",
        "# Update these so that you can log in to Galileo\n",
        "# without having to enter your credentials every time\n",
        "os.environ['GALILEO_CONSOLE_URL']=\"\"\n",
        "os.environ[\"GALILEO_USERNAME\"]=\"\"\n",
        "os.environ[\"GALILEO_PASSWORD\"]=\"\"\n",
        "\n",
        "import dataquality as dq\n",
        "dq.configure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXFUYBuSJRTJ"
      },
      "source": [
        "# 2. Set Data\n",
        "We load the data from Hugging Face for fine-tuning an Encoder-Decoder model. Additionally, the original Alpaca dataset does not specify a val/test split, so in auto we randomly sample to train/val with the ratios (0.83, 0.17). Use the auto docs to learn more about how to configure your own training/val/test sets. \n",
        "\n",
        "NOTE: We are working with LLMs (emphasis on Large) and Alpaca is a decently sized dataset with 52,000 data samples. Therefore, training times can be large. To speed up training during this tutorial we default the training set size to be 1000 samples (and thus 200 for val). Consider changing the `max_train_size` parameter to fit your data needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6F2mUW-wJW4G"
      },
      "outputs": [],
      "source": [
        "#@title Load ðŸ¤— HuggingFace Alpaca Dataset\n",
        "max_train_size = 1000\n",
        "dataset = \"tatsu-lab/alpaca\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K6C-PReO66q"
      },
      "source": [
        "# 3. Setup configuration with Galileo\n",
        "Galileo auto uses 3 classes to set configuration for the Dataset, Training parameters, and Generation config. While they all have defaults that work out of the box, we also allow granular control over these settings, see the [docs](https://docs.rungalileo.io/galileo/llm-studio/llm-debugger/getting-started) for more info.\n",
        "\n",
        "In this tutorial, we use the Encoder-Decoder model [`google/flan-t5-small`](https://huggingface.co/google/flan-t5-small) and leverage a simple greedy decoding strategy.\n",
        "\n",
        "To speed up training and reduce memory, we limit the `max_target_tokens` (for the decoder block) to `128`, while leaving `max_input_tokens` (for the encoder block) as the default 512. Feel free to change this to reduce the samples with truncation.\n",
        "\n",
        "The dataset can be passed in as a path to a local file, a HuggingFace dataset, or the string name of a remote HF dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDF9O9lAO59P"
      },
      "outputs": [],
      "source": [
        "from dataquality.integrations.seq2seq.schema import (\n",
        "    Seq2SeqDatasetConfig, Seq2SeqGenerationConfig, Seq2SeqTrainingConfig\n",
        ")\n",
        "\n",
        "# For huggingface datasets, use `train_data`\n",
        "# For local files, use `train_train`\n",
        "dataset_config = Seq2SeqDatasetConfig(\n",
        "    hf_data=dataset,\n",
        "    input_col=\"prompt\",\n",
        "    target_col=\"completion\",\n",
        ")\n",
        "# Generation takes about 1 second per sample on a V100 GPU\n",
        "# So we limit to only the test set\n",
        "# Update here to include \"training\" or omit \"validation\" / \"test\"\n",
        "gen_config = Seq2SeqGenerationConfig(\n",
        "    generation_splits=[\"validation\", \"test\"]\n",
        ")\n",
        "tr_config = Seq2SeqTrainingConfig(\n",
        "    epochs=3,\n",
        "    max_target_tokens=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1umfIXJT2YE"
      },
      "source": [
        "# 4. Log input data with Galileo auto\n",
        "\n",
        "Testing `auto` for Seq2Seq tasks is as simple as importing and calling `auto()`. However, we set a few basic parameters in this tutorial such as project/run name, the config settings, and a max dataset size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwsmIRciSwZf"
      },
      "outputs": [],
      "source": [
        "from dataquality.integrations.seq2seq.auto import auto\n",
        "\n",
        "auto(\n",
        "    project_name=\"galileo-finetune\",  # TODO, update project name\n",
        "    run_name=\"example_run_galileo-finetune_with_auto\",  # TODO, update with unique run name\n",
        "    dataset_config=dataset_config,\n",
        "    generation_config=gen_config,\n",
        "    training_config=tr_config,\n",
        "    max_train_size=max_train_size,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "x6DPqgz0c8pW",
        "MXFUYBuSJRTJ",
        "T1umfIXJT2YE"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
