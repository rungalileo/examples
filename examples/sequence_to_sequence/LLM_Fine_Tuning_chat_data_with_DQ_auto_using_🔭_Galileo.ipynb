{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4k3q4W0CmVp"
      },
      "source": [
        "# LLM Fine-Tuning chat data using ðŸ”­ Galileo's auto\n",
        "\n",
        "In this tutorial we will upload chat data to Galileo's console.\n",
        "\n",
        "We use a small sample chat dataset with `jsonl` data, but users can provide data via Pandas DataFrame, Huggingface datasets, or a local path to the dataset stored as a `.csv`, `.json`, or `.jsonl` file. \n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6DPqgz0c8pW"
      },
      "source": [
        "# Install Dependancies [Including Setting up DQ] + Add Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z1hro7XtGD-j"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality`\n",
        "\n",
        "# Upgrade pip\n",
        "!pip install -U pip &> /dev/null\n",
        "\n",
        "# Install all dependecies\n",
        "!pip install 'dataquality[cuda]' --extra-index-url=https://pypi.nvidia.com/\n",
        "print('ðŸ‘‹ Installed necessary libraries.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEaEd_zAHpDm"
      },
      "source": [
        "# 1. Initialize Galileo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anwDG2zLHrjf"
      },
      "outputs": [],
      "source": [
        "# ðŸ”­ðŸŒ• Galileo log-in\n",
        "import os\n",
        "\n",
        "# Update these so that you can log in to Galileo\n",
        "# without having to enter your credentials every time\n",
        "os.environ['GALILEO_CONSOLE_URL']=\"\"\n",
        "os.environ[\"GALILEO_USERNAME\"]=\"\"\n",
        "os.environ[\"GALILEO_PASSWORD\"]=\"\"\n",
        "\n",
        "import dataquality as dq\n",
        "dq.configure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXFUYBuSJRTJ"
      },
      "source": [
        "# 2. Set Data\n",
        "We load the data from Hugging Face for fine-tuning an Encoder-Decoder model. Additionally, the original Alpaca dataset does not specify a val/test split, so in auto we randomly sample to train/val with the ratios (0.8, 0.2). Use the auto docs to learn more about how to configure your own training/val/test sets. \n",
        "\n",
        "NOTE: We are working with LLMs (emphasis on Large) and Alpaca is a decently sized dataset with 52,000 data samples. Therefore, training times can be large. To speed up training during this tutorial we default the training set size to be 1000 samples (and thus 200 for val). Consider changing the `max_train_size` parameter to fit your data needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6F2mUW-wJW4G"
      },
      "outputs": [],
      "source": [
        "#@title Load mock data\n",
        "sample = {\n",
        "    \"turns\": [\n",
        "        {\n",
        "            \"role\": \"User\",\n",
        "            \"content\": \"What is the meaning of life?\",\n",
        "            \"my_metric\": 1.618,  # These fields will show up as metadata\n",
        "            \"other_metric\": 5,  # These fields will show up as metadata\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"Assistant\",\n",
        "            \"content\": \"I cannot answer that with certainty, but I hear it is 42.\",\n",
        "            \"my_metric\": 2.718,\n",
        "            \"other_metric\": 3,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"User\",\n",
        "            \"content\": \"Hmm, what does that mean?\",\n",
        "            \"my_metric\": 0.001,\n",
        "            \"other_metric\": 4,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"Assistant\",\n",
        "            \"content\": \"To me it means that you should always be nice to others.\",\n",
        "            \"my_metric\": 1.234,\n",
        "            \"other_metric\": 4,\n",
        "        },\n",
        "    ],\n",
        "    \"score\": 3.14, # This field will also be logged as metadata\n",
        "    \"metadata\": {\n",
        "        \"sample_id\": \"1234\",\n",
        "        \"annotator\": \"Bugs Bunny\",\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "n_samples = 10\n",
        "dataset = pd.DataFrame([sample] * n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K6C-PReO66q"
      },
      "source": [
        "# 3. Setup configuration with Galileo\n",
        "Galileo auto uses 3 classes to set configuration for the Dataset, Training parameters, and Generation config. While they all have defaults that work out of the box, we also allow granular control over these settings, see the [docs](https://docs.rungalileo.io/galileo/llm-studio/llm-debugger/getting-started) for more info.\n",
        "\n",
        "For chat we must include a data formatter `ChatHistoryFormatter`. The ChatHistoryFormatter assumes that each sample has a column (default name is `turns`) that contains a list of turn information. Update the below fields to match your dataset column names. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDF9O9lAO59P"
      },
      "outputs": [],
      "source": [
        "from dataquality.integrations.seq2seq.formatters.chat import ChatHistoryFormatter\n",
        "from dataquality.integrations.seq2seq.schema import (\n",
        "    Seq2SeqDatasetConfig,\n",
        "    Seq2SeqGenerationConfig,\n",
        "    Seq2SeqTrainingConfig\n",
        ")\n",
        "\n",
        "chat_history_formatter = ChatHistoryFormatter(\n",
        "    hf_tokenizer=\"google/flan-t5-base\",  # This is the default tokenizer\n",
        "    turns_col=\"turns\",\n",
        "    metadata_col=\"metadata\",\n",
        "    content_col=\"content\",\n",
        "    role_col=\"role\",\n",
        "    user=\"User\",\n",
        "    assistant=\"Assistant\",\n",
        ")\n",
        "\n",
        "\n",
        "# For huggingface datasets, use `train_data`\n",
        "# For local files, use `train_train`\n",
        "dataset_config = Seq2SeqDatasetConfig(\n",
        "    train_data=dataset,\n",
        "    formatter=chat_history_formatter,\n",
        ")\n",
        "# For chat data with pre-trained models we can skip generation\n",
        "gen_config = Seq2SeqGenerationConfig(\n",
        "    generation_splits=[]\n",
        ")\n",
        "# Since we are focused on data insights rather than model specific data insights \n",
        "# we can only train for 1 epoch to speed up the process\n",
        "tr_config = Seq2SeqTrainingConfig(\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1umfIXJT2YE"
      },
      "source": [
        "# 4. Log input data with Galileo auto\n",
        "\n",
        "Testing `auto` for Seq2Seq tasks is as simple as importing and calling `auto()`. However, we set a few basic parameters in this tutorial such as project/run name and config settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwsmIRciSwZf"
      },
      "outputs": [],
      "source": [
        "from dataquality.integrations.seq2seq.auto import auto\n",
        "\n",
        "auto(\n",
        "    project_name=\"galileo-llm-auto\",  # TODO, update project name\n",
        "    run_name=\"example_run_galileo-chat_with_auto\",  # TODO, update with unique run name\n",
        "    dataset_config=dataset_config,\n",
        "    generation_config=gen_config,\n",
        "    training_config=tr_config,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "x6DPqgz0c8pW",
        "MXFUYBuSJRTJ",
        "T1umfIXJT2YE"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
