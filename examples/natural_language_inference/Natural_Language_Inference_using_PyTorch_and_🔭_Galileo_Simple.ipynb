{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5JVjGAAJOTn"
      },
      "source": [
        "# Natural Language Inference using Pytorch and üî≠ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UfMcPbg19uz1"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality` with `pip install dataquality`\n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip and install dependencies\n",
        "    !pip install -U pip &> /dev/null\n",
        "    !pip install dataquality transformers datasets torchmetrics==0.10.0 --upgrade 1> /dev/null\n",
        "\n",
        "    print('üëã Installed necessary libraries and restarting runtime! This should only need to happen once.')\n",
        "    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKPfMhYvgeL"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C7B55N_B30kG"
      },
      "outputs": [],
      "source": [
        "#@title ü§ó HuggingFace Dataset\n",
        "#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:token-classification&task_ids=task_ids:named-entity-recognition&sort=downloads) which contains train/test splits and an `ner_tags` column.\n",
        "\n",
        "dataset_name = 'conllpp' #@param [\"wnut_17\", \"conllpp\", \"wikiann\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config)\n",
        "\n",
        "# A small function for minimizing the dataset for testing\n",
        "import os\n",
        "\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\"\n",
        "\n",
        "if _minimize_for_ci():\n",
        "  data[\"train\"] = data[\"train\"].select(range(10))\n",
        "  data[\"test\"] = data[\"test\"].select(range(10))\n",
        "\n",
        "# Check that the dataset has at least train and test splits\n",
        "assert {\"train\", \"test\"}.issubset(data), \\\n",
        "f\"üíæ The dataset {dataset_name} does no have train/test splits, please pick another one.\"\n",
        "\n",
        "print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "nTrPjUO1FuEu"
      },
      "outputs": [],
      "source": [
        "#@markdown Convert HF dataset to Pandas dataframes \n",
        "import pandas as pd\n",
        "\n",
        "def load_pandas_df(data):\n",
        "  # Find the name of the ground truth column\n",
        "  good_col_names = [name for name in list(data['train'].features) if \"label\" in name]\n",
        "  if len(good_col_names) == 1:\n",
        "    label_col = good_col_names[0]\n",
        "  else:\n",
        "    col_names = list(data['train'].features)\n",
        "    print(f\"The name of the columns are {col_names}.\")\n",
        "    label_col = input(f\"üèÖ Please enter the name of the column containing the labels: \")\n",
        "    assert label_col in col_names, f\"{label_col} is not an existing column\"\n",
        "\n",
        "  # Load the labels in a dictionary\n",
        "  labels = data['train'].features[label_col].names\n",
        "  labels = {v:k for v, k in enumerate(labels)}\n",
        "\n",
        "  # Load the train data into a frame\n",
        "  train_data = data[\"train\"]\n",
        "  train_df = pd.DataFrame.from_dict(train_data)\n",
        "  train_df['label'] = train_df[label_col].map(labels)\n",
        "  train_df['id'] = train_df.index\n",
        "  train_df['text'] = train_df['premise'] + \"  <>  \" + train_df['hypothesis']\n",
        "\n",
        "  # Load the test data into a frame\n",
        "  test_split_name = \"validation\" if \"validation\" in data else \"test\"\n",
        "  test_data = data[test_split_name]\n",
        "  test_df = pd.DataFrame.from_dict(test_data)\n",
        "  test_df['label'] = test_df[label_col].map(labels)\n",
        "  test_df['id'] = test_df.index\n",
        "  test_df['text'] = test_df['premise'] + \"  <>  \" + test_df['hypothesis']\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_pandas_df(data)\n",
        "labels = train_df.label.unique().tolist()\n",
        "\n",
        "if _minimize_for_ci():\n",
        "  train_df, test_df = train_df[:10], test_df[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 2. Preprocess dataset and prepare training\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AutoModel\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "class TokenClassificationModel(torch.nn.Module):\n",
        "  \"\"\"Defines a Pytorch text classification bert based model.\"\"\"\n",
        "\n",
        "  def __init__(self, num_labels: int):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    self.classifier = Linear(self.feature_extractor.config.hidden_size, num_labels)\n",
        "\n",
        "  def forward(self, x, attention_mask):\n",
        "    \"\"\"Model forward function.\"\"\"\n",
        "    encoded_layers = self.feature_extractor(\n",
        "        input_ids=x, attention_mask=attention_mask\n",
        "    ).last_hidden_state\n",
        "    logits = self.classifier(encoded_layers)\n",
        "    return logits\n",
        "\n",
        "num_classes = len(data['train'].features['ner_tags'].feature.names)\n",
        "\n",
        "model = TokenClassificationModel(num_classes)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5\n",
        ")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "val_acc = torchmetrics.Accuracy().to(device)\n",
        "train_acc = torchmetrics.Accuracy().to(device)\n",
        "loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def extract_batch(batch):\n",
        "  x = batch.pop(\"input_ids\").to(device)\n",
        "  y = batch.pop(\"labels\").to(device)\n",
        "  attention_mask = batch.pop(\"attention_mask\").to(device)\n",
        "  return x, y, attention_mask\n",
        "\n",
        "def train_loop(model,dataloader,optimizer,loss_fn):\n",
        "  model.train()\n",
        "  for batch in tqdm(dataloader):\n",
        "      # print statistics\n",
        "      x, y, attention_mask = extract_batch(batch)\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward + backward + optimize\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits.transpose(1, 2), y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "   \n",
        "def test_loop(model, dataloader, loss_fn, epoch):\n",
        "  model.eval()\n",
        "  test_loss, num_test_batches = 0.0, 0\n",
        "  pbar = tqdm(dataloader)\n",
        "  for batch in pbar:\n",
        "    x, y, attention_mask = extract_batch(batch)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits.transpose(1, 2), y)\n",
        "    test_loss += loss.item()\n",
        "    num_test_batches += 1\n",
        "    pbar.set_description(\"[epoch %d] Validation loss: %.3f\" % (epoch + 1, test_loss / num_test_batches))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6zJHmvxTJ2pN"
      },
      "source": [
        "## 3. Monitor with Galileo\n",
        "\n",
        "After logging the orignal dataset, we can hook the dataquality client in our model and dataloaders.\n",
        "\n",
        "```python\n",
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "dq.init(...)\n",
        "dq.log_dataset(...)\n",
        "dq.set_labels_for_run(...)\n",
        "watch(model, [train_dataloader, test_dataloader])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX8KsPk_m6Au"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "# üî≠üåï Initialize the project\n",
        "dq.init('text_classification',\n",
        "        \"example_project\",\n",
        "        f\"run_{dataset_name}\"\n",
        "        )\n",
        "\n",
        "# üî≠üåï Logging the dataset with Galileo\n",
        "dq.log_dataset(train_df,\n",
        "               split=\"train\")\n",
        "\n",
        "# üî≠üåï Logging the dataset with Galileo\n",
        "dq.log_dataset(test_df,\n",
        "               split=\"validation\")\n",
        "\n",
        "# üî≠üåï Logging the labels in order for Galileo\n",
        "dq.set_labels_for_run(train_dataset.list_of_labels)\n",
        "\n",
        "# üî≠üåï Monitor the model with Galileo\n",
        "watch(model, [train_dataloader, test_dataloader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV9XnJaSg9j"
      },
      "source": [
        "# 4. Training\n",
        "\n",
        "After hooking into the model we log epochs and split during the training process.\n",
        "```python\n",
        "dq.set_split(\"train\")\n",
        "dq.set_epoch(epoch)\n",
        "```\n",
        "\n",
        "When training is complete\n",
        "```dq.finish()```\n",
        "must be called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    dq.set_epoch(epoch) # üî≠üåï Setting the epoch\n",
        "    dq.set_split(\"training\") # üî≠üåï Setting split to training\n",
        "    train_loop(model,train_dataloader,optimizer,loss_fn)\n",
        "    dq.set_split(\"validation\") # üî≠üåï Setting split to validation\n",
        "    test_loop(model, test_dataloader, loss_fn, epoch)\n",
        "print(\"Finished Training\")\n",
        "\n",
        "dq.finish() # üî≠üåï Finishing the run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUoYnK0oFK_"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9o7ZwZVZIiM"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mqfb_JFuEy"
      },
      "outputs": [],
      "source": [
        "# Get insights to the metrics\n",
        "dq.metrics.get_dataframe(\n",
        "        \"example_project\",\n",
        "        f\"run_{dataset_name}\",\n",
        "        \"train\").head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
