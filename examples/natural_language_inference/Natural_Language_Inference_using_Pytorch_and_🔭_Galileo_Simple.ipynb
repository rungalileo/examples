{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j5JVjGAAJOTn"
      },
      "source": [
        "# Natural Language Inference using Pytorch and üî≠ Galileo\n",
        "\n",
        "In this tutorial, we'll train a model with PyTorch and explore the results in Galileo.\n",
        "\n",
        "**Make sure to select GPU in your Runtime! (Runtime -> Change Runtime type)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UfMcPbg19uz1"
      },
      "outputs": [],
      "source": [
        "#@title Install `dataquality` with `pip install dataquality`\n",
        "try:\n",
        "    import dataquality as dq\n",
        "except ImportError:\n",
        "    # Upgrade pip and install dependencies\n",
        "    !pip install -U pip &> /dev/null\n",
        "    !pip install dataquality transformers datasets torchmetrics==0.10.0 --upgrade 1> /dev/null\n",
        "\n",
        "    print('üëã Installed necessary libraries.')\n",
        "    print('üôè Continue with the rest of the notebook or hit \"Run All\" again!')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKPfMhYvgeL"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C7B55N_B30kG"
      },
      "outputs": [],
      "source": [
        "#@title ü§ó HuggingFace Dataset\n",
        "#@markdown You can select any dataset from [here](https://huggingface.co/datasets?language=language:en&task_categories=task_categories:text-classification&task_ids=task_ids:natural-language-inference&sort=downloads) which contains the columns `premise` and `hypothesis`.\n",
        "\n",
        "dataset_name = 'hans' #@param [\"snli\", \"sem_eval_2014_task_1\", \"hans\"] {allow-input: true}\n",
        "print(f\"You selected the {dataset_name} dataset\")\n",
        "\n",
        "from IPython.utils import io\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "\n",
        "# Try to load the data. If a config (subset) is needed, pick one\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "  if \"Config name is missing\" not in repr(e):\n",
        "    raise e\n",
        "\n",
        "  configs = get_dataset_config_names(dataset_name)\n",
        "  print(f\"The dataset {dataset_name} has multiple subsets {configs}.\")\n",
        "  config = input(f\"üññ Enter the name of the subset to pick (or leave blank for any): \")\n",
        "  if config:\n",
        "    assert config in configs, f\"{config} is not a valid subset\"\n",
        "  else:\n",
        "    config = configs[0]\n",
        "  with io.capture_output() as captured:\n",
        "    data = load_dataset(dataset_name, name=config)\n",
        "\n",
        "# Check that the dataset has at least train and either of validation/test\n",
        "assert \"train\" in data and {\"validation\", \"test\"}.intersection(data), \\\n",
        "f\"üíæ The dataset {dataset_name} has either no train, or no validation or test splits, select another one.\"\n",
        "\n",
        "print(f\"\\nüèÜ Dataset {dataset_name} loaded succesfully\")\n",
        "\n",
        "# A small function for minimizing the dataset for testing\n",
        "import os\n",
        "\n",
        "def _minimize_for_ci() -> bool:\n",
        "    return os.getenv(\"MINIMIZE_FOR_CI\", \"false\") == \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "nTrPjUO1FuEu"
      },
      "outputs": [],
      "source": [
        "#@markdown Convert HF dataset to Pandas dataframes \n",
        "import pandas as pd\n",
        "\n",
        "def load_pandas_df(data):\n",
        "  # Find the name of the ground truth column\n",
        "  good_col_names = [name for name in list(data['train'].features) if \"label\" in name]\n",
        "  if len(good_col_names) == 1:\n",
        "    label_col = good_col_names[0]\n",
        "  else:\n",
        "    col_names = list(data['train'].features)\n",
        "    print(f\"The name of the columns are {col_names}.\")\n",
        "    label_col = input(f\"üèÖ Please enter the name of the column containing the labels: \")\n",
        "    assert label_col in col_names, f\"{label_col} is not an existing column\"\n",
        "\n",
        "  # Load the labels in a dictionary\n",
        "  labels = data['train'].features[label_col].names\n",
        "  labels = {v:k for v, k in enumerate(labels)}\n",
        "\n",
        "  # Load the train data into a frame\n",
        "  train_data = data[\"train\"]\n",
        "  train_df = pd.DataFrame.from_dict(train_data)\n",
        "  train_df['label'] = train_df[label_col].map(labels)\n",
        "  train_df['id'] = train_df.index\n",
        "  train_df['text'] = train_df['premise'] + \"  <>  \" + train_df['hypothesis']\n",
        "\n",
        "  # Load the test data into a frame\n",
        "  test_split_name = \"validation\" if \"validation\" in data else \"test\"\n",
        "  test_data = data[test_split_name]\n",
        "  test_df = pd.DataFrame.from_dict(test_data)\n",
        "  test_df['label'] = test_df[label_col].map(labels)\n",
        "  test_df['id'] = test_df.index\n",
        "  test_df['text'] = test_df['premise'] + \"  <>  \" + test_df['hypothesis']\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_pandas_df(data)\n",
        "\n",
        "if _minimize_for_ci():\n",
        "  train_df = train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(10))\n",
        "  test_df = test_df.groupby('label', group_keys=False).apply(lambda x: x.sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 2. Preprocess dataset and prepare training\n",
        "#@markdown Most PyTorch training is done with the Dataloader.\n",
        "#@markdown After preprocessing the data (tokenizing) we pass it on to it.\n",
        "#@markdown Galileo relies on PyTorch's DataLoader.\n",
        "#@markdown\n",
        "#@markdown ```python\n",
        "#@markdown train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "#@markdown test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "#@markdown ```\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from transformers import AutoModel\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import List\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, dataset: pd.DataFrame, list_of_labels: List[str] = None\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.encodings = tokenizer(\n",
        "            self.dataset[\"text\"].tolist(), truncation=True, padding=True\n",
        "        )\n",
        "        self.list_of_labels = list_of_labels or self.dataset[\"label\"].unique().tolist()\n",
        "        self.labels = np.array(\n",
        "            [self.list_of_labels.index(label) for label in self.dataset[\"label\"]]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.encodings[\"input_ids\"][idx])\n",
        "        attention_mask = torch.tensor(self.encodings[\"attention_mask\"][idx])\n",
        "        y = self.labels[idx]\n",
        "        return {\"input_ids\": x, \"attention_mask\": attention_mask, \"label\": y}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "train_dataset = TextDataset(train_df)\n",
        "test_dataset = TextDataset(test_df, list_of_labels=train_dataset.list_of_labels)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "class TextClassificationModel(torch.nn.Module):\n",
        "    \"\"\"Defines a Pytorch text classification bert based model.\"\"\"\n",
        "\n",
        "    def __init__(self, num_labels: int):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.classifier = Linear(self.feature_extractor.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"Model forward function.\"\"\"\n",
        "        encoded_layers = self.feature_extractor(\n",
        "            input_ids=input_ids, attention_mask=attention_mask\n",
        "        ).last_hidden_state\n",
        "        classification_embedding = encoded_layers[:, 0]\n",
        "        return self.classifier(classification_embedding)\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "model = TextClassificationModel(num_labels=len(train_dataset.list_of_labels))\n",
        "\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "train_acc = torchmetrics.Accuracy().to(device)\n",
        "val_acc = torchmetrics.Accuracy().to(device)\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def extract_batch(batch):\n",
        "  x = batch.pop(\"input_ids\").to(device)\n",
        "  attention_mask = batch.pop(\"attention_mask\").to(device)\n",
        "  y = batch.pop(\"label\").to(device)\n",
        "  return x, y, attention_mask\n",
        "\n",
        "def train_loop(model,dataloader,optimizer,loss_fn):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for batch in tqdm(dataloader):\n",
        "      # print statistics\n",
        "      x, y, attention_mask = extract_batch(batch)\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward + backward + optimize\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_acc(logits.argmax(1), y)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "def test_loop(model, dataloader, loss_fn, epoch):\n",
        "  model.eval()\n",
        "  validation_loss = 0.0\n",
        "  pbar = tqdm(dataloader)\n",
        "  for batch in pbar:\n",
        "    x, y, attention_mask = extract_batch(batch)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x, attention_mask)\n",
        "      loss = loss_fn(logits, y)\n",
        "    val_acc(logits.argmax(1), y)\n",
        "    validation_loss += loss.item()\n",
        "    pbar.set_description(\"[epoch %d] Validation loss: %.3f\" % (epoch + 1, validation_loss))\n",
        "  pbar.set_description(f\"Val accuracy: {val_acc.compute()}\")\n",
        "\n",
        "print(\"Preview of dataset:\")\n",
        "example_df = data[\"train\"].to_pandas().head()\n",
        "example_df[\"label\"] = example_df[\"label\"].map({\n",
        "    i:label\n",
        "    for i,label in enumerate(data[\"train\"].features[\"label\"].names)\n",
        "})\n",
        "example_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6zJHmvxTJ2pN"
      },
      "source": [
        "## 3. Monitor with Galileo\n",
        "\n",
        "After logging the orignal dataset, we can hook the dataquality client in our model and dataloaders.\n",
        "\n",
        "```python\n",
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "dq.init(...)\n",
        "dq.log_dataset(...)\n",
        "dq.set_labels_for_run(...)\n",
        "watch(model, [train_dataloader, test_dataloader])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX8KsPk_m6Au"
      },
      "outputs": [],
      "source": [
        "import dataquality as dq\n",
        "from dataquality.integrations.torch import watch\n",
        "\n",
        "# üî≠üåï Initialize the project\n",
        "dq.init('text_classification',\n",
        "        \"natural-language-inference-demo\",\n",
        "        f\"run_{dataset_name}\")\n",
        "\n",
        "# üî≠üåï Logging the labels in order for Galileo\n",
        "dq.set_labels_for_run(train_df.label.unique().tolist())\n",
        "\n",
        "# üî≠üåï Logging the dataset with Galileo\n",
        "dq.log_dataset(train_df, split=\"train\")\n",
        "\n",
        "# üî≠üåï Logging the dataset with Galileo\n",
        "dq.log_dataset(test_df, split=\"validation\")\n",
        "\n",
        "# üî≠üåï Monitor the model with Galileo\n",
        "watch(model, [train_dataloader, test_dataloader])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV9XnJaSg9j"
      },
      "source": [
        "# 4. Training\n",
        "\n",
        "After hooking into the model we log epochs and split during the training process.\n",
        "```python\n",
        "dq.set_split(\"train\")\n",
        "dq.set_epoch(epoch)\n",
        "```\n",
        "\n",
        "When training is complete\n",
        "```dq.finish()```\n",
        "must be called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvfEg4cnzTZ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    dq.set_epoch(epoch) # üî≠üåï Setting the epoch\n",
        "    dq.set_split(\"training\") # üî≠üåï Setting split to training\n",
        "    train_loop(model, train_dataloader, optimizer, loss_fn)\n",
        "    dq.set_split(\"validation\") # üî≠üåï Setting split to validation\n",
        "    test_loop(model, test_dataloader, loss_fn, epoch)\n",
        "print(\"Finished Training\")\n",
        "\n",
        "dq.finish() # üî≠üåï Finishing the run"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUoYnK0oFK_"
      },
      "source": [
        "# General Help and Docs\n",
        "- To get help with your task's requirements, call `dq.get_data_logger().doc()`\n",
        "- To see more general data and model logging docs, run `dq.docs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9o7ZwZVZIiM"
      },
      "outputs": [],
      "source": [
        "dq.get_data_logger().doc()\n",
        "help(dq.log_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mqfb_JFuEy"
      },
      "outputs": [],
      "source": [
        "# Get insights to the metrics\n",
        "dq.metrics.get_dataframe(\n",
        "        \"natural-language-inference-demo\",\n",
        "        f\"run_{dataset_name}\",\n",
        "        \"train\").head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
